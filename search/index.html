<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en><head><meta charset=utf-8><meta name=author content="Arnab Kumar Shil"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=theme-color content="#000000"><meta property="fb:app_id" content="668971647169067"><meta name=yandex-verification content="3ccce49727df78e2"><link rel=copyright href=https://ruddra.com/search/#copyright><link rel=article:author href=https://ruddra.com/about><meta name=robots content="index,follow"><meta name=referrer content="unsafe-url"><meta property="og:url" content="https://ruddra.com/search/"><meta name=keywords content="Blog"><title>Search</title><meta name=description content><link rel=preload as=font type=font/woff2 href=/fonts/merriweather-v21-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/pt-sans-v11-latin-regular.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/pt-sans-v11-latin-700.woff2 crossorigin><link rel=preload as=font type=font/woff2 href=/fonts/merriweather-v21-latin-700.woff2 crossorigin><style>@font-face{font-family:merriweather;font-style:normal;font-weight:400;font-display:swap;src:url(/fonts/merriweather-v21-latin-regular.eot);src:local("Merriweather Regular"),local("Merriweather-Regular"),url(/fonts/merriweather-v21-latin-regular.eot?#iefix)format("embedded-opentype"),url(/fonts/merriweather-v21-latin-regular.woff2)format("woff2"),url(/fonts/merriweather-v21-latin-regular.woff)format("woff"),url(/fonts/merriweather-v21-latin-regular.ttf)format("truetype"),url(/fonts/merriweather-v21-latin-regular.svg#Merriweather)format("svg")}@font-face{font-family:merriweather;font-style:normal;font-weight:700;font-display:swap;src:url(/fonts/merriweather-v21-latin-700.eot);src:local("Merriweather Bold"),local("Merriweather-Bold"),url(/fonts/merriweather-v21-latin-700.eot?#iefix)format("embedded-opentype"),url(/fonts/merriweather-v21-latin-700.woff2)format("woff2"),url(/fonts/merriweather-v21-latin-700.woff)format("woff"),url(/fonts/merriweather-v21-latin-700.ttf)format("truetype"),url(/fonts/merriweather-v21-latin-700.svg#Merriweather)format("svg")}@font-face{font-family:pt sans;font-style:normal;font-weight:400;font-display:swap;src:url(/fonts/pt-sans-v11-latin-regular.eot);src:local("PT Sans"),local("PTSans-Regular"),url(/fonts/pt-sans-v11-latin-regular.eot?#iefix)format("embedded-opentype"),url(/fonts/pt-sans-v11-latin-regular.woff2)format("woff2"),url(/fonts/pt-sans-v11-latin-regular.woff)format("woff"),url(/fonts/pt-sans-v11-latin-regular.ttf)format("truetype"),url(/fonts/pt-sans-v11-latin-regular.svg#PTSans)format("svg")}@font-face{font-family:pt sans;font-style:normal;font-weight:700;font-display:swap;src:url(/fonts/pt-sans-v11-latin-700.eot);src:local("PT Sans Bold"),local("PTSans-Bold"),url(/fonts/pt-sans-v11-latin-700.eot?#iefix)format("embedded-opentype"),url(/fonts/pt-sans-v11-latin-700.woff2)format("woff2"),url(/fonts/pt-sans-v11-latin-700.woff)format("woff"),url(/fonts/pt-sans-v11-latin-700.ttf)format("truetype"),url(/fonts/pt-sans-v11-latin-700.svg#PTSans)format("svg")}*{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}html,body{margin:0;padding:0}html{font-family:Merriweather,Georgia,times new roman,serif;font-weight:400;font-size:16px;line-height:1.8}body{color:#222;background-color:#fff;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%}a{color:#268bd2;text-decoration:none}a strong{color:inherit}a:hover,a:focus{text-decoration:underline}h1,h2,h3,h4,h5,h6{margin-bottom:.5rem;font-weight:700;line-height:1.25;color:#313131}h1{font-size:2rem}h2{margin-top:1rem;font-size:1.5rem}h3{margin-top:1.5rem;font-size:1.25rem}h4,h5,h6{margin-top:1rem;font-size:1rem}p{margin-top:0;margin-bottom:1rem}strong{color:#303030}ul,ol,dl{margin-top:0;margin-bottom:1rem}dt{font-weight:700}dd{margin-bottom:.5rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee;border-bottom:1px solid #fff}abbr{font-size:85%;font-weight:700;color:#555;text-transform:uppercase}abbr[title]{cursor:help;border-bottom:1px dotted #e5e5e5}code,pre{font-family:Menlo,Monaco,courier new,monospace}code{padding:.25em .5em;font-size:85%;color:#bf616a;background-color:#f9f9f9;border-radius:3px}pre{display:block;margin-top:0;margin-bottom:1rem;padding:1rem;font-size:.8rem;line-height:1.4;white-space:pre;white-space:pre-wrap;word-break:break-all;word-wrap:break-word;background-color:#f9f9f9}pre code{padding:0;font-size:100%;color:inherit;background-color:transparent}.highlight{margin-bottom:1rem;border-radius:4px}.highlight pre{margin-bottom:0}.gist .gist-file{font-family:Menlo,Monaco,courier new,monospace}.gist .markdown-body{padding:15px}.gist pre{padding:0;background-color:transparent}.gist .gist-file .gist-data{font-size:.8rem;line-height:1.4}.gist code{padding:0;color:inherit;background-color:transparent;border-radius:0}blockquote{padding:.5rem 1rem;margin:.8rem 0;color:#7a7a7a;border-left:.25rem solid #e5e5e5}blockquote p:last-child{margin-bottom:0}@media(min-width:30em){blockquote{padding-right:5rem;padding-left:1.25rem}}img{display:block;max-width:100%;margin:0 0 1rem;border-radius:5px}table{margin-bottom:1rem;width:100%;border:1px solid #e5e5e5;border-collapse:collapse}td,th{padding:.25rem .5rem;border:1px solid #e5e5e5}tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th{background-color:#f9f9f9}.lead{font-size:1.25rem;font-weight:300}.message{margin-bottom:1rem;padding:1rem;color:#717171;background-color:#f9f9f9}.container{max-width:38rem;padding-left:1rem;padding-right:1rem;margin-left:auto;margin-right:auto}.masthead{padding-top:1rem;padding-bottom:1rem;margin-bottom:3rem}.masthead-title{margin-top:0;margin-bottom:0;color:#505050}.masthead-title a{color:#505050}.masthead-title small{font-size:75%;font-weight:400;color:silver;letter-spacing:0}.page,.post{margin-bottom:4em}.page-title,.post-title,.post-title a{color:#303030}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-.5rem;margin-bottom:.5rem;color:#9a9a9a}.related{padding-top:2rem;padding-bottom:2rem;border-top:1px solid #eee}.related-posts{padding-left:0;list-style:none}.related-posts h3{margin-top:0}.related-posts li small{font-size:75%;color:#999}.related-posts li a:hover{color:#268bd2;text-decoration:none}.related-posts li a:hover small{color:inherit}.pagination{overflow:hidden;margin-left:-1rem;margin-right:-1rem;color:#ccc;text-align:center}.pagination-item{display:block;padding:1rem;border:1px solid #eee}.pagination-item:first-child{margin-bottom:-1px}a.pagination-item:hover{background-color:#f5f5f5}@media(min-width:30em){.pagination{margin:3rem 0}.pagination-item{float:left;width:50%}.pagination-item:first-child{margin-bottom:0;border-top-left-radius:4px;border-bottom-left-radius:4px}.pagination-item:last-child{margin-left:-1px;border-top-right-radius:4px;border-bottom-right-radius:4px}}.highlight .hll{background-color:#ffc}.highlight .c{color:#999}.highlight .err{color:#a00;background-color:#faa}.highlight .k{color:#069}.highlight .o{color:#555}.highlight .cm{color:#09f;font-style:italic}.highlight .cp{color:#099}.highlight .c1{color:#999}.highlight .cs{color:#999}.highlight .gd{background-color:#fcc;border:1px solid #c00}.highlight .ge{font-style:italic}.highlight .gr{color:red}.highlight .gh{color:#030}.highlight .gi{background-color:#cfc;border:1px solid #0c0}.highlight .go{color:#aaa}.highlight .gp{color:#009}.highlight .gu{color:#030}.highlight .gt{color:#9c6}.highlight .kc{color:#069}.highlight .kd{color:#069}.highlight .kn{color:#069}.highlight .kp{color:#069}.highlight .kr{color:#069}.highlight .kt{color:#078}.highlight .m{color:#f60}.highlight .s{color:#d44950}.highlight .na{color:#4f9fcf}.highlight .nb{color:#366}.highlight .nc{color:#0a8}.highlight .no{color:#360}.highlight .nd{color:#99f}.highlight .ni{color:#999}.highlight .ne{color:#c00}.highlight .nf{color:#c0f}.highlight .nl{color:#99f}.highlight .nn{color:#0cf}.highlight .nt{color:#2f6f9f}.highlight .nv{color:#033}.highlight .ow{color:#000}.highlight .w{color:#bbb}.highlight .mf{color:#f60}.highlight .mh{color:#f60}.highlight .mi{color:#f60}.highlight .mo{color:#f60}.highlight .sb{color:#c30}.highlight .sc{color:#c30}.highlight .sd{color:#c30;font-style:italic}.highlight .s2{color:#c30}.highlight .se{color:#c30}.highlight .sh{color:#c30}.highlight .si{color:#a00}.highlight .sx{color:#c30}.highlight .sr{color:#3aa}.highlight .s1{color:#c30}.highlight .ss{color:#fc3}.highlight .bp{color:#366}.highlight .vc{color:#033}.highlight .vg{color:#033}.highlight .vi{color:#033}.highlight .il{color:#f60}.css .o,.css .o+.nt,.css .nt+.nt{color:#999}html,body{overflow-x:hidden}html{font-family:Merriweather,Georgia,times new roman,serif}h1,h2,h3,h4,h5,h6{font-family:PT Sans,Helvetica,Arial,sans-serif;font-weight:400;color:#313131;letter-spacing:-.025rem}.wrap{position:relative;width:100%}.container{max-width:28rem}@media(min-width:38em){.container{max-width:32rem}}@media(min-width:56em){.container{max-width:38rem}}.masthead{padding-top:1rem;padding-bottom:1rem;margin-bottom:3rem;border-bottom:1px solid #eee}.masthead-title{margin-top:0;margin-bottom:0;color:#505050}.masthead-title a{color:#505050}.masthead-title small{font-size:75%;font-weight:400;color:silver;letter-spacing:0}@media(max-width:48em){.masthead-title{text-align:center}.masthead-title small{display:none}}.sidebar{position:fixed;top:0;bottom:0;left:-14rem;width:14rem;visibility:hidden;overflow-y:auto;font-family:pt sans,Helvetica,Arial,sans-serif;font-size:.875rem;color:rgba(255,255,255,.6);background-color:#202020;-webkit-transition:all .3s ease-in-out;transition:all .3s ease-in-out}@media(min-width:30em){.sidebar{font-size:.75rem}}.sidebar a{font-weight:400;color:#fff}.sidebar-item{padding:1rem}.sidebar-item p:last-child{margin-bottom:0}.sidebar-nav{border-bottom:1px solid rgba(255,255,255,.1)}.sidebar-nav-item{display:block;padding:.5rem 1rem;border-top:1px solid rgba(255,255,255,.1)}.sidebar-nav-item.active,a.sidebar-nav-item:hover,a.sidebar-nav-item:focus{text-decoration:none;background-color:rgba(255,255,255,.1);border-color:transparent}@media(min-width:48em){.sidebar-item{padding:1.5rem}.sidebar-nav-item{padding-left:1.5rem;padding-right:1.5rem}}.sidebar-checkbox{position:absolute;opacity:0;-webkit-user-select:none;-moz-user-select:none;user-select:none}.sidebar-toggle{position:absolute;top:.8rem;left:1rem;display:block;padding:.25rem .75rem;color:#505050;background-color:#fff;border-radius:.25rem;cursor:pointer;line-height:1.5}.sidebar-toggle:before{display:inline-block;width:1rem;height:.75rem;content:"";background-image:-webkit-linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%);background-image:-moz-linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%);background-image:-ms-linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%);background-image:linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%)}.sidebar-toggle:active,#sidebar-checkbox:checked~.sidebar-toggle{color:#fff;background-color:#555}.sidebar-toggle:active:before,#sidebar-checkbox:checked~.sidebar-toggle:before{background-image:-webkit-linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%);background-image:-moz-linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%);background-image:-ms-linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%);background-image:linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%)}@media(min-width:30.1em){.sidebar-toggle{position:fixed}}@media print{.sidebar-toggle{display:none}}.wrap,.sidebar,.sidebar-toggle{-webkit-backface-visibility:hidden;-ms-backface-visibility:hidden;backface-visibility:hidden}.wrap,.sidebar-toggle{-webkit-transition:-webkit-transform .3s ease-in-out;transition:transform .3s ease-in-out}#sidebar-checkbox:checked+.sidebar{z-index:10;visibility:visible}#sidebar-checkbox:checked~.sidebar,#sidebar-checkbox:checked~.wrap,#sidebar-checkbox:checked~.sidebar-toggle{-webkit-transform:translateX(14rem);-ms-transform:translateX(14rem);transform:translateX(14rem)}.page,.post{margin-bottom:4em}.page-title,.post-title,.post-title a{color:#303030}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-.5rem;margin-bottom:.5rem;color:#9a9a9a}.related{padding-top:2rem;padding-bottom:2rem;border-top:1px solid #eee}.related-posts{padding-left:0;list-style:none}.related-posts h3{margin-top:0}.related-posts li small{font-size:75%;color:#999}.related-posts li a:hover{color:#268bd2;text-decoration:none}.related-posts li a:hover small{color:inherit}.pagination{overflow:hidden;margin-left:-1rem;margin-right:-1rem;text-align:center;color:#222}.pagination-item{display:block;padding:1rem;border:1px solid #eee}.pagination-item:first-child{margin-bottom:-1px}a.pagination-item:hover{background-color:#f5f5f5}@media(min-width:30em){.pagination{margin:3rem 0}.pagination-item{float:left;width:50%}.pagination-item:first-child{margin-bottom:0;border-top-left-radius:4px;border-bottom-left-radius:4px}.pagination-item:last-child{margin-left:-1px;border-top-right-radius:4px;border-bottom-right-radius:4px}}.layout-reverse .sidebar{left:auto;right:-14rem}.layout-reverse .sidebar-toggle{left:auto;right:1rem}.layout-reverse #sidebar-checkbox:checked~.sidebar,.layout-reverse #sidebar-checkbox:checked~.wrap,.layout-reverse #sidebar-checkbox:checked~.sidebar-toggle{-webkit-transform:translateX(-14rem);-ms-transform:translateX(-14rem);transform:translateX(-14rem)}.theme-base-08 .sidebar,.theme-base-08 .sidebar-toggle:active,.theme-base-08 #sidebar-checkbox:checked~.sidebar-toggle{background-color:#ac4142}.theme-base-08 .container a,.theme-base-08 .sidebar-toggle,.theme-base-08 .related-posts li a:hover{color:#ac4142}.theme-base-09 .sidebar,.theme-base-09 .sidebar-toggle:active,.theme-base-09 #sidebar-checkbox:checked~.sidebar-toggle{background-color:#d28445}.theme-base-09 .container a,.theme-base-09 .sidebar-toggle,.theme-base-09 .related-posts li a:hover{color:#d28445}.theme-base-0a .sidebar,.theme-base-0a .sidebar-toggle:active,.theme-base-0a #sidebar-checkbox:checked~.sidebar-toggle{background-color:#f4bf75}.theme-base-0a .container a,.theme-base-0a .sidebar-toggle,.theme-base-0a .related-posts li a:hover{color:#f4bf75}.theme-base-0b .sidebar,.theme-base-0b .sidebar-toggle:active,.theme-base-0b #sidebar-checkbox:checked~.sidebar-toggle{background-color:#90a959}.theme-base-0b .container a,.theme-base-0b .sidebar-toggle,.theme-base-0b .related-posts li a:hover{color:#90a959}.theme-base-0c .sidebar,.theme-base-0c .sidebar-toggle:active,.theme-base-0c #sidebar-checkbox:checked~.sidebar-toggle{background-color:#75b5aa}.theme-base-0c .container a,.theme-base-0c .sidebar-toggle,.theme-base-0c .related-posts li a:hover{color:#75b5aa}.theme-base-0d .sidebar,.theme-base-0d .sidebar-toggle:active,.theme-base-0d #sidebar-checkbox:checked~.sidebar-toggle{background-color:#6a9fb5}.theme-base-0d .container a,.theme-base-0d .sidebar-toggle,.theme-base-0d .related-posts li a:hover{color:#6a9fb5}.theme-base-0e .sidebar,.theme-base-0e .sidebar-toggle:active,.theme-base-0e #sidebar-checkbox:checked~.sidebar-toggle{background-color:#aa759f}.theme-base-0e .container a,.theme-base-0e .sidebar-toggle,.theme-base-0e .related-posts li a:hover{color:#aa759f}.theme-base-0f .sidebar,.theme-base-0f .sidebar-toggle:active,.theme-base-0f #sidebar-checkbox:checked~.sidebar-toggle{background-color:#8f5536}.theme-base-0f .container a,.theme-base-0f .sidebar-toggle,.theme-base-0f .related-posts li a:hover{color:#8f5536}.sidebar-overlay #sidebar-checkbox:checked~.wrap{-webkit-transform:translateX(0);-ms-transform:translateX(0);transform:translateX(0)}.sidebar-overlay #sidebar-checkbox:checked~.sidebar-toggle{box-shadow:0 0 0 .25rem #fff}.sidebar-overlay #sidebar-checkbox:checked~.sidebar{box-shadow:.25rem 0 .5rem rgba(0,0,0,.1)}.layout-reverse.sidebar-overlay #sidebar-checkbox:checked~.sidebar{box-shadow:-.25rem 0 .5rem rgba(0,0,0,.1)}.commento-root-min-height{min-height:430px}.commento-root{overflow-x:hidden;padding:0;width:100%}.commento-root *{color:#222}.commento-root code{font-family:Menlo,Monaco,courier new,monospace;font-size:13px;white-space:pre}.commento-root a{color:#228be6;outline:0;text-decoration:none}.commento-root blockquote{margin:0 0 0 8px;padding:0 0 0 5px;border-left:2px solid #adb5bd;color:#868e96}.commento-root .commento-button{display:inline-flex;justify-content:center;align-items:center;text-align:center;cursor:pointer;font-weight:700;line-height:24px;font-size:14px;padding:6px 8px;box-shadow:0 4px 6px rgba(50,50,93,.11),0 1px 3px rgba(0,0,0,.08);border:1px solid transparent;border-radius:3px;color:#fff;margin-left:5px;margin-right:5px}.commento-root .commento-login{width:100%}.commento-root .commento-login .commento-login-text{text-align:right;margin-right:16px;height:38px;color:#868e96;font-weight:700;cursor:pointer;display:block}.commento-root .commento-logged-container{width:100%;text-align:left;margin-bottom:16px;position:relative;height:38px}.commento-root .commento-logged-container .commento-logout{cursor:pointer;position:absolute;top:6px;right:16px;color:#adb5bd}.commento-root .commento-logged-container .commento-logged-in-as{position:relative}.commento-root .commento-logged-container .commento-logged-in-as .commento-name{color:#343a40;border:none;font-weight:700;position:absolute;top:6px;left:48px;cursor:pointer}.commento-root .commento-mod-tools{margin-bottom:16px}.commento-root .commento-mod-tools button{text-transform:uppercase;color:#495057;font-size:12px;font-weight:700;cursor:pointer;margin-left:12px;background:0 0;border:none;display:inline}.commento-root .commento-mod-tools::before{content:"Moderator Tools";text-transform:uppercase;color:#3b5bdb;font-size:12px;font-weight:700}.commento-root .commento-round-check input[type=checkbox],.commento-root .commento-round-check input[type=radio]{display:none}.commento-root .commento-round-check input[type=checkbox]+label,.commento-root .commento-round-check input[type=radio]+label{display:block;position:relative;padding-left:35px;margin-bottom:5px;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none}.commento-root .commento-round-check input[type=checkbox]+label:last-child,.commento-root .commento-round-check input[type=radio]+label:last-child{margin-bottom:0}.commento-root .commento-round-check input[type=checkbox]+label:before,.commento-root .commento-round-check input[type=radio]+label:before{content:"";display:block;width:13px;height:13px;margin-top:2px;background:#f8f9fa;border:1px solid #dee2e6;border-radius:3px;position:absolute;left:0;top:0;transition:all .15s}.commento-root .commento-round-check input[type=checkbox]:disabled+label:before,.commento-root .commento-round-check input[type=radio]:disabled+label:before{background:#f8f9fa;border:1px solid #ced4da;opacity:.4}.commento-root .commento-round-check input[type=checkbox]:checked+label:before,.commento-root .commento-round-check input[type=radio]:checked+label:before{background:#228be6;border:1px solid #228be6}.commento-root .commento-round-check input[type=checkbox]+label:after,.commento-root .commento-round-check input[type=radio]+label:after{position:absolute;left:-7px;top:4px;content:"";display:inline-block;width:3px;height:7px;transform:rotate(45deg);margin-left:12px;margin-right:12px;border:solid transparent;border-width:0 2px 2px 0}.commento-root .commento-round-check input[type=checkbox]:disabled+label:after,.commento-root .commento-round-check input[type=radio]:disabled+label:after{border:solid transparent;border-width:0 2px 2px 0}.commento-root .commento-round-check input[type=checkbox]:checked+label:after,.commento-root .commento-round-check input[type=radio]:checked+label:after{border:solid #f8f9fa;border-width:0 2px 2px 0}.commento-root .commento-round-check .pitch{font-size:14px;color:#a5a5a5;line-height:20px}.commento-root input[type=text],.commento-root textarea{background:#fff;border:1px solid rgba(50,50,93,.1);border-radius:3px;color:#525f7f}.commento-root input[type=text]::placeholder{color:#cacaca}.commento-root textarea::placeholder{color:#868e96;font-size:20px;display:flex;line-height:110px;justify-content:center;align-items:center;text-align:center}.commento-root textarea{display:inline-block;white-space:pre-wrap;padding:8px;outline:0;overflow:auto;min-height:130px;width:100%}.commento-root .commento-red-border{border:1px solid #f03e3e}.commento-root .commento-textarea-container{position:relative;display:flex;justify-content:center;align-items:center}.commento-root .commento-textarea-container .commento-submit-button{transform:none}.commento-root .commento-blurred-textarea{list-style:none;display:flex;flex-wrap:wrap;align-items:center;justify-content:center;justify-content:space-between;will-change:transform}.commento-root .commento-submit-button{float:right;background:#1971c2;text-transform:uppercase;font-size:12px;margin-top:10px}.commento-root .commento-button-margin{padding-top:4px}.commento-root .commento-anonymous-checkbox-container{float:right;margin:20px 16px}.commento-root .commento-anonymous-checkbox-container input[type=checkbox]+label{padding-left:24px;font-size:12px;text-transform:uppercase;font-weight:700;color:#495057}.commento-root .commento-anonymous-checkbox-container input[type=checkbox]+label:before{margin-top:1px}.commento-root .commento-anonymous-checkbox-container input[type=checkbox]+label:after{margin-top:-1px}.commento-root .commento-markdown-button{color:#868e96;margin:0 16px;font-size:12px;text-transform:uppercase;border:none;line-height:58px;font-weight:400;cursor:pointer}.commento-root .commento-markdown-button b{font-size:12px}.commento-root .commento-markdown-help{border:1px solid #dee2e6;padding:8px}.commento-root .commento-markdown-help tr td{padding:0 6px}.commento-root .commento-markdown-help tr td pre{display:inline;font-family:monospace;font-size:13px}.commento-root .commento-card{padding:12px 0 0 12px;margin-top:16px;border-top:1px solid #f0f0f0}.commento-root .commento-card .commento-option-button{border:none;cursor:pointer;outline:0;padding:0;position:absolute;top:0;z-index:3;background:#ced4da}.commento-root .commento-card .commento-option-reply{height:20px;width:20px;-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyB2aWV3Qm94PSIwIDAgNDggNDgiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6Y2M9Imh0dHA6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL25zIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIj48ZyB0cmFuc2Zvcm09Im1hdHJpeCgxLjczNTggMCAwIDEuNzMzNSA0LjI2NDIgMTUuMjE3KSI+PHBhdGggZD0ibTEwIDIuNSAxZS02LTQuNDQyMS05IDcuNDQyMSA5IDYuNTc1OUwxMCA3LjRjNi40MTk0LTEuNDgzOSAxMS43MzkgMi4xNzYyIDExLjczOSAyLjE3NjJzLTIuMDc3NC02LjU0NzUtMTEuNzM5LTcuMDc2MnoiIGZpbGw9IiNhYmJhYzQiLz48L2c+PC9zdmc+);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyB2aWV3Qm94PSIwIDAgNDggNDgiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6Y2M9Imh0dHA6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL25zIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIj48ZyB0cmFuc2Zvcm09Im1hdHJpeCgxLjczNTggMCAwIDEuNzMzNSA0LjI2NDIgMTUuMjE3KSI+PHBhdGggZD0ibTEwIDIuNSAxZS02LTQuNDQyMS05IDcuNDQyMSA5IDYuNTc1OUwxMCA3LjRjNi40MTk0LTEuNDgzOSAxMS43MzkgMi4xNzYyIDExLjczOSAyLjE3NjJzLTIuMDc3NC02LjU0NzUtMTEuNzM5LTcuMDc2MnoiIGZpbGw9IiNhYmJhYzQiLz48L2c+PC9zdmc+);margin:9px 3px}.commento-root .commento-card .commento-option-cancel{height:13px;width:13px;-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAxMjkgMTI5IiB2aWV3Qm94PSIwIDAgMTI5IDEyOSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtNy42IDEyMS40Yy44LjggMS44IDEuMiAyLjkgMS4yczIuMS0uNCAyLjktMS4ybDUxLjEtNTEuMSA1MS4xIDUxLjFjLjguOCAxLjggMS4yIDIuOSAxLjIgMSAwIDIuMS0uNCAyLjktMS4yIDEuNi0xLjYgMS42LTQuMi4wLTUuOGwtNTEuMS01MS4xIDUxLjEtNTEuMWMxLjYtMS42IDEuNi00LjIuMC01LjhzLTQuMi0xLjYtNS44LjBMNjQuNSA1OC43bC01MS4xLTUxLjFjLTEuNi0xLjYtNC4yLTEuNi01LjguMHMtMS42IDQuMi4wIDUuOGw1MS4xIDUxLjEtNTEuMSA1MS4xYy0xLjYgMS42LTEuNiA0LjIuMCA1Ljh6IiBmaWxsPSIjYWJiYWM0Ii8+PC9zdmc+);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAxMjkgMTI5IiB2aWV3Qm94PSIwIDAgMTI5IDEyOSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtNy42IDEyMS40Yy44LjggMS44IDEuMiAyLjkgMS4yczIuMS0uNCAyLjktMS4ybDUxLjEtNTEuMSA1MS4xIDUxLjFjLjguOCAxLjggMS4yIDIuOSAxLjIgMSAwIDIuMS0uNCAyLjktMS4yIDEuNi0xLjYgMS42LTQuMi4wLTUuOGwtNTEuMS01MS4xIDUxLjEtNTEuMWMxLjYtMS42IDEuNi00LjIuMC01LjhzLTQuMi0xLjYtNS44LjBMNjQuNSA1OC43bC01MS4xLTUxLjFjLTEuNi0xLjYtNC4yLTEuNi01LjguMHMtMS42IDQuMi4wIDUuOGw1MS4xIDUxLjEtNTEuMSA1MS4xYy0xLjYgMS42LTEuNiA0LjIuMCA1Ljh6IiBmaWxsPSIjYWJiYWM0Ii8+PC9zdmc+);margin:12px 6px;background:#adb5bd}.commento-root .commento-card .commento-option-collapse{height:14px;width:14px;-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA0MiA0MiIgdmlld0JveD0iMCAwIDQyIDQyIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwb2x5Z29uIHBvaW50cz0iNDIgMjAgMCAyMCAwIDIyIDIwIDIyIDIyIDIyIDQyIDIyIiBmaWxsPSIjMWUyMTI3Ii8+PC9zdmc+);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA0MiA0MiIgdmlld0JveD0iMCAwIDQyIDQyIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwb2x5Z29uIHBvaW50cz0iNDIgMjAgMCAyMCAwIDIyIDIwIDIyIDIyIDIyIDQyIDIyIiBmaWxsPSIjMWUyMTI3Ii8+PC9zdmc+);margin:12px 6px;background:#495057}.commento-root .commento-card .commento-option-uncollapse{height:14px;width:14px;-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA0MiA0MiIgdmlld0JveD0iMCAwIDQyIDQyIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwb2x5Z29uIHBvaW50cz0iNDIgMjAgMjIgMjAgMjIgMCAyMCAwIDIwIDIwIDAgMjAgMCAyMiAyMCAyMiAyMCA0MiAyMiA0MiAyMiAyMiA0MiAyMiIgZmlsbD0iIzFlMjEyNyIvPjwvc3ZnPg==);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA0MiA0MiIgdmlld0JveD0iMCAwIDQyIDQyIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwb2x5Z29uIHBvaW50cz0iNDIgMjAgMjIgMjAgMjIgMCAyMCAwIDIwIDIwIDAgMjAgMCAyMiAyMCAyMiAyMCA0MiAyMiA0MiAyMiAyMiA0MiAyMiIgZmlsbD0iIzFlMjEyNyIvPjwvc3ZnPg==);margin:12px 6px;background:#495057}.commento-root .commento-card .commento-option-downvote,.commento-root .commento-card .commento-option-upvote{height:14px;width:14px;-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyODQuOTI5IDI4NC45MjkiIHZpZXdCb3g9IjAgMCAyODQuOTMgMjg0LjkzIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwYXRoIGQ9Im0yODIuMDggMTk1LjI4LTEzMy4wNS0xMzMuMDRjLTEuOTAxLTEuOTAzLTQuMDg4LTIuODU2LTYuNTYyLTIuODU2cy00LjY2NS45NTMtNi41NjcgMi44NTZsLTEzMy4wNCAxMzMuMDRjLTEuOTA2IDEuOTA2LTIuODU2IDQuMDkzLTIuODU2IDYuNTY4LjAgMi40NzQuOTUzIDQuNjY0IDIuODU2IDYuNTY2bDE0LjI3MiAxNC4yNzFjMS45MDMgMS45MDMgNC4wOTMgMi44NTQgNi41NjcgMi44NTRzNC42NjQtLjk1MSA2LjU2Ny0yLjg1NGwxMTIuMi0xMTIuMiAxMTIuMjEgMTEyLjIxYzEuOTAyIDEuOTAzIDQuMDkzIDIuODQ4IDYuNTYzIDIuODQ4IDIuNDc4LjAgNC42NjgtLjk1MSA2LjU3LTIuODQ4bDE0LjI3NC0xNC4yNzdjMS45MDItMS45MDIgMi44NDctNC4wOTMgMi44NDctNi41NjYuMDAxLTIuNDc2LS45NDQtNC42NjYtMi44NDYtNi41Njl6IiBmaWxsPSIjYWJiYWM0Ii8+PC9zdmc+);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyODQuOTI5IDI4NC45MjkiIHZpZXdCb3g9IjAgMCAyODQuOTMgMjg0LjkzIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwYXRoIGQ9Im0yODIuMDggMTk1LjI4LTEzMy4wNS0xMzMuMDRjLTEuOTAxLTEuOTAzLTQuMDg4LTIuODU2LTYuNTYyLTIuODU2cy00LjY2NS45NTMtNi41NjcgMi44NTZsLTEzMy4wNCAxMzMuMDRjLTEuOTA2IDEuOTA2LTIuODU2IDQuMDkzLTIuODU2IDYuNTY4LjAgMi40NzQuOTUzIDQuNjY0IDIuODU2IDYuNTY2bDE0LjI3MiAxNC4yNzFjMS45MDMgMS45MDMgNC4wOTMgMi44NTQgNi41NjcgMi44NTRzNC42NjQtLjk1MSA2LjU2Ny0yLjg1NGwxMTIuMi0xMTIuMiAxMTIuMjEgMTEyLjIxYzEuOTAyIDEuOTAzIDQuMDkzIDIuODQ4IDYuNTYzIDIuODQ4IDIuNDc4LjAgNC42NjgtLjk1MSA2LjU3LTIuODQ4bDE0LjI3NC0xNC4yNzdjMS45MDItMS45MDIgMi44NDctNC4wOTMgMi44NDctNi41NjYuMDAxLTIuNDc2LS45NDQtNC42NjYtMi44NDYtNi41Njl6IiBmaWxsPSIjYWJiYWM0Ii8+PC9zdmc+);margin:12px 6px}.commento-root .commento-card .commento-option-downvote{transform:rotate(180deg)}.commento-root .commento-card .commento-upvoted{background:#f76707}.commento-root .commento-card .commento-downvoted{background:#4c6ef5}.commento-root .commento-card .commento-option-edit{height:14px;width:14px;-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA1MjguODk5IDUyOC44OTkiIHZpZXdCb3g9IjAgMCA1MjguODk5IDUyOC44OTkiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PHBhdGggZD0ibTMyOC44OCA4OS4xMjUgMTA3LjU5IDEwNy41OS0yNzIuMzQgMjcyLjM0LTEwNy41My0xMDcuNTkgMjcyLjI4LTI3Mi4zNHptMTg5LjIzLTI1Ljk0OC00Ny45ODEtNDcuOTgxYy0xOC41NDMtMTguNTQzLTQ4LjY1My0xOC41NDMtNjcuMjU5LjBsLTQ1Ljk2MSA0NS45NjEgMTA3LjU5IDEwNy41OSA1My42MTEtNTMuNjExYzE0LjM4Mi0xNC4zODMgMTQuMzgyLTM3LjU3Ny4wLTUxLjk1OXptLTUxNy44MSA0NDkuNTFjLTEuOTU4IDguODEyIDUuOTk4IDE2LjcwOCAxNC44MTEgMTQuNTY1bDExOS44OS0yOS4wNjktMTA3LjUzLTEwNy41OS0yNy4xNzMgMTIyLjA5eiIvPjwvc3ZnPg==);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA1MjguODk5IDUyOC44OTkiIHZpZXdCb3g9IjAgMCA1MjguODk5IDUyOC44OTkiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PHBhdGggZD0ibTMyOC44OCA4OS4xMjUgMTA3LjU5IDEwNy41OS0yNzIuMzQgMjcyLjM0LTEwNy41My0xMDcuNTkgMjcyLjI4LTI3Mi4zNHptMTg5LjIzLTI1Ljk0OC00Ny45ODEtNDcuOTgxYy0xOC41NDMtMTguNTQzLTQ4LjY1My0xOC41NDMtNjcuMjU5LjBsLTQ1Ljk2MSA0NS45NjEgMTA3LjU5IDEwNy41OSA1My42MTEtNTMuNjExYzE0LjM4Mi0xNC4zODMgMTQuMzgyLTM3LjU3Ny4wLTUxLjk1OXptLTUxNy44MSA0NDkuNTFjLTEuOTU4IDguODEyIDUuOTk4IDE2LjcwOCAxNC44MTEgMTQuNTY1bDExOS44OS0yOS4wNjktMTA3LjUzLTEwNy41OS0yNy4xNzMgMTIyLjA5eiIvPjwvc3ZnPg==);margin:12px 6px;background:#adb5bd}.commento-root .commento-card .commento-option-remove{height:14px;width:14px;-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA1OSA1OSIgdmlld0JveD0iMCAwIDU5IDU5IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxnIGZpbGw9IiMxZTIxMjciPjxwYXRoIGQ9Im0yOS41IDUxYy41NTIuMCAxLS40NDcgMS0xVjE3YzAtLjU1My0uNDQ4LTEtMS0xcy0xIC40NDctMSAxdjMzYzAgLjU1My40NDggMSAxIDF6Ii8+PHBhdGggZD0ibTE5LjUgNTFjLjU1Mi4wIDEtLjQ0NyAxLTFWMTdjMC0uNTUzLS40NDgtMS0xLTFzLTEgLjQ0Ny0xIDF2MzNjMCAuNTUzLjQ0OCAxIDEgMXoiLz48cGF0aCBkPSJtMzkuNSA1MWMuNTUyLjAgMS0uNDQ3IDEtMVYxN2MwLS41NTMtLjQ0OC0xLTEtMXMtMSAuNDQ3LTEgMXYzM2MwIC41NTMuNDQ4IDEgMSAxeiIvPjxwYXRoIGQ9Ik01Mi41IDZIMzguNDU2Yy0uMTEtMS4yNS0uNDk1LTMuMzU4LTEuODEzLTQuNzExQzM1LjgwOS40MzQgMzQuNzUxLjAgMzMuNDk5LjBIMjMuNWMtMS4yNTIuMC0yLjMxLjQzNC0zLjE0NCAxLjI4OUMxOS4wMzggMi42NDIgMTguNjUzIDQuNzUgMTguNTQzIDZINi41Yy0uNTUyLjAtMSAuNDQ3LTEgMXMuNDQ4IDEgMSAxaDIuMDQxbDEuOTE1IDQ2LjAyMUMxMC40OTMgNTUuNzQzIDExLjU2NSA1OSAxNS4zNjQgNTloMjguMjcyYzMuNzk5LjAgNC44NzEtMy4yNTcgNC45MDctNC45NThMNTAuNDU5IDhINTIuNWMuNTUyLjAgMS0uNDQ3IDEtMVM1My4wNTIgNiA1Mi41IDZ6TTIxLjc5MiAyLjY4MUMyMi4yNCAyLjIyMyAyMi43OTkgMiAyMy41IDJoOS45OTljLjcwMS4wIDEuMjYuMjIzIDEuNzA4LjY4MS44MDUuODIzIDEuMTI4IDIuMjcxIDEuMjQgMy4zMTlIMjAuNTUzQzIwLjY2NSA0Ljk1MiAyMC45ODggMy41MDQgMjEuNzkyIDIuNjgxek00Ni41NDQgNTMuOTc5QzQ2LjUzOCA1NC4yODggNDYuNCA1NyA0My42MzYgNTdIMTUuMzY0Yy0yLjczNC4wLTIuODk4LTIuNzE3LTIuOTA5LTMuMDQyTDEwLjU0MiA4aDM3LjkxNUw0Ni41NDQgNTMuOTc5eiIvPjwvZz48L3N2Zz4=);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA1OSA1OSIgdmlld0JveD0iMCAwIDU5IDU5IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxnIGZpbGw9IiMxZTIxMjciPjxwYXRoIGQ9Im0yOS41IDUxYy41NTIuMCAxLS40NDcgMS0xVjE3YzAtLjU1My0uNDQ4LTEtMS0xcy0xIC40NDctMSAxdjMzYzAgLjU1My40NDggMSAxIDF6Ii8+PHBhdGggZD0ibTE5LjUgNTFjLjU1Mi4wIDEtLjQ0NyAxLTFWMTdjMC0uNTUzLS40NDgtMS0xLTFzLTEgLjQ0Ny0xIDF2MzNjMCAuNTUzLjQ0OCAxIDEgMXoiLz48cGF0aCBkPSJtMzkuNSA1MWMuNTUyLjAgMS0uNDQ3IDEtMVYxN2MwLS41NTMtLjQ0OC0xLTEtMXMtMSAuNDQ3LTEgMXYzM2MwIC41NTMuNDQ4IDEgMSAxeiIvPjxwYXRoIGQ9Ik01Mi41IDZIMzguNDU2Yy0uMTEtMS4yNS0uNDk1LTMuMzU4LTEuODEzLTQuNzExQzM1LjgwOS40MzQgMzQuNzUxLjAgMzMuNDk5LjBIMjMuNWMtMS4yNTIuMC0yLjMxLjQzNC0zLjE0NCAxLjI4OUMxOS4wMzggMi42NDIgMTguNjUzIDQuNzUgMTguNTQzIDZINi41Yy0uNTUyLjAtMSAuNDQ3LTEgMXMuNDQ4IDEgMSAxaDIuMDQxbDEuOTE1IDQ2LjAyMUMxMC40OTMgNTUuNzQzIDExLjU2NSA1OSAxNS4zNjQgNTloMjguMjcyYzMuNzk5LjAgNC44NzEtMy4yNTcgNC45MDctNC45NThMNTAuNDU5IDhINTIuNWMuNTUyLjAgMS0uNDQ3IDEtMVM1My4wNTIgNiA1Mi41IDZ6TTIxLjc5MiAyLjY4MUMyMi4yNCAyLjIyMyAyMi43OTkgMiAyMy41IDJoOS45OTljLjcwMS4wIDEuMjYuMjIzIDEuNzA4LjY4MS44MDUuODIzIDEuMTI4IDIuMjcxIDEuMjQgMy4zMTlIMjAuNTUzQzIwLjY2NSA0Ljk1MiAyMC45ODggMy41MDQgMjEuNzkyIDIuNjgxek00Ni41NDQgNTMuOTc5QzQ2LjUzOCA1NC4yODggNDYuNCA1NyA0My42MzYgNTdIMTUuMzY0Yy0yLjczNC4wLTIuODk4LTIuNzE3LTIuOTA5LTMuMDQyTDEwLjU0MiA4aDM3LjkxNUw0Ni41NDQgNTMuOTc5eiIvPjwvZz48L3N2Zz4=);margin:12px 6px;background:#e03131}.commento-root .commento-card .commento-option-approve{height:14px;width:14px;-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNiAyNiIgdmlld0JveD0iMCAwIDI2IDI2IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwYXRoIGQ9Im0uMyAxNGMtLjItLjItLjMtLjUtLjMtLjdzLjEtLjUuMy0uN2wxLjQtMS40Yy40LS40IDEtLjQgMS40LjBsLjEuMSA1LjUgNS45Yy4yLjIuNS4yLjcuMGwxMy40LTEzLjloLjF2LTg4ODE4ZS0yMGMuNC0uNCAxLS40IDEuNC4wbDEuNCAxLjRjLjQuNC40IDEgMCAxLjRsLTE2IDE2LjZjLS4yLjItLjQuMy0uNy4zcy0uNS0uMS0uNy0uM2wtNy44LTguNC0uMi0uM3oiIGZpbGw9IiMxZTIxMjciLz48L3N2Zz4=);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNiAyNiIgdmlld0JveD0iMCAwIDI2IDI2IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwYXRoIGQ9Im0uMyAxNGMtLjItLjItLjMtLjUtLjMtLjdzLjEtLjUuMy0uN2wxLjQtMS40Yy40LS40IDEtLjQgMS40LjBsLjEuMSA1LjUgNS45Yy4yLjIuNS4yLjcuMGwxMy40LTEzLjloLjF2LTg4ODE4ZS0yMGMuNC0uNCAxLS40IDEuNC4wbDEuNCAxLjRjLjQuNC40IDEgMCAxLjRsLTE2IDE2LjZjLS4yLjItLjQuMy0uNy4zcy0uNS0uMS0uNy0uM2wtNy44LTguNC0uMi0uM3oiIGZpbGw9IiMxZTIxMjciLz48L3N2Zz4=);margin:12px 6px;background:#37b24d}.commento-root .commento-card .commento-option-sticky,.commento-root .commento-card .commento-option-unsticky{height:14px;width:14px;-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA0ODcuMjIyIDQ4Ny4yMjIiIHZpZXdCb3g9IjAgMCA0ODcuMjIgNDg3LjIyIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwYXRoIGQ9Im00ODYuNTUgMTg2LjgxYy0xLjYtNC45LTUuOC04LjQtMTAuOS05LjJsLTE1Mi0yMS42LTY4LjQtMTM3LjVjLTIuMy00LjYtNy03LjUtMTIuMS03LjVzLTkuOCAyLjktMTIuMSA3LjZsLTY3LjUgMTM3LjktMTUyIDIyLjZjLTUuMS44LTkuMyA0LjMtMTAuOSA5LjJzLS4yIDEwLjMgMy41IDEzLjhsMTEwLjMgMTA2LjktMjUuNSAxNTEuNGMtLjkgNS4xIDEuMiAxMC4yIDUuNCAxMy4yIDIuMyAxLjcgNS4xIDIuNiA3LjkgMi42IDIuMi4wIDQuMy0uNSA2LjMtMS42bDEzNS43LTcxLjkgMTM2LjEgNzEuMWMyIDEgNC4xIDEuNSA2LjIgMS41IDcuNC4wIDEzLjUtNi4xIDEzLjUtMTMuNS4wLTEuMS0uMS0yLjEtLjQtMy4xbC0yNi4zLTE1MC41IDEwOS42LTEwNy41YzMuOS0zLjYgNS4yLTkgMy42LTEzLjl6bS0xMzcgMTA3LjFjLTMuMiAzLjEtNC42IDcuNi0zLjggMTJsMjIuOSAxMzEuMy0xMTguMi02MS43Yy0zLjktMi4xLTguNi0yLTEyLjYuMGwtMTE3LjggNjIuNCAyMi4xLTEzMS41Yy43LTQuNC0uNy04LjgtMy45LTExLjlsLTk1LjYtOTIuOCAxMzEuOS0xOS42YzQuNC0uNyA4LjItMy40IDEwLjEtNy40bDU4LjYtMTE5LjcgNTkuNCAxMTkuNGMyIDQgNS44IDYuNyAxMC4yIDcuNGwxMzIgMTguOC05NS4zIDkzLjN6IiBmaWxsPSIjMWUyMTI3Ii8+PC9zdmc+);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCA0ODcuMjIyIDQ4Ny4yMjIiIHZpZXdCb3g9IjAgMCA0ODcuMjIgNDg3LjIyIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwYXRoIGQ9Im00ODYuNTUgMTg2LjgxYy0xLjYtNC45LTUuOC04LjQtMTAuOS05LjJsLTE1Mi0yMS42LTY4LjQtMTM3LjVjLTIuMy00LjYtNy03LjUtMTIuMS03LjVzLTkuOCAyLjktMTIuMSA3LjZsLTY3LjUgMTM3LjktMTUyIDIyLjZjLTUuMS44LTkuMyA0LjMtMTAuOSA5LjJzLS4yIDEwLjMgMy41IDEzLjhsMTEwLjMgMTA2LjktMjUuNSAxNTEuNGMtLjkgNS4xIDEuMiAxMC4yIDUuNCAxMy4yIDIuMyAxLjcgNS4xIDIuNiA3LjkgMi42IDIuMi4wIDQuMy0uNSA2LjMtMS42bDEzNS43LTcxLjkgMTM2LjEgNzEuMWMyIDEgNC4xIDEuNSA2LjIgMS41IDcuNC4wIDEzLjUtNi4xIDEzLjUtMTMuNS4wLTEuMS0uMS0yLjEtLjQtMy4xbC0yNi4zLTE1MC41IDEwOS42LTEwNy41YzMuOS0zLjYgNS4yLTkgMy42LTEzLjl6bS0xMzcgMTA3LjFjLTMuMiAzLjEtNC42IDcuNi0zLjggMTJsMjIuOSAxMzEuMy0xMTguMi02MS43Yy0zLjktMi4xLTguNi0yLTEyLjYuMGwtMTE3LjggNjIuNCAyMi4xLTEzMS41Yy43LTQuNC0uNy04LjgtMy45LTExLjlsLTk1LjYtOTIuOCAxMzEuOS0xOS42YzQuNC0uNyA4LjItMy40IDEwLjEtNy40bDU4LjYtMTE5LjcgNTkuNCAxMTkuNGMyIDQgNS44IDYuNyAxMC4yIDcuNGwxMzIgMTguOC05NS4zIDkzLjN6IiBmaWxsPSIjMWUyMTI3Ii8+PC9zdmc+);margin:12px 6px;background:#adb5bd}.commento-root .commento-card .commento-option-unsticky{-webkit-mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyB2aWV3Qm94PSIwIDAgNDg3LjIyIDQ4Ny4yMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48Zz48dGl0bGU+YmFja2dyb3VuZDwvdGl0bGU+PC9nPjxnPjx0aXRsZT5MYXllciAxPC90aXRsZT48cGF0aCBkPSJtNDg2LjU1IDE4Ni44MWMtMS42LTQuOS01LjgtOC40LTEwLjktOS4ybC0xNTItMjEuNi02OC40LTEzNy41Yy0yLjMtNC42LTctNy41LTEyLjEtNy41cy05LjggMi45LTEyLjEgNy42bC02Ny41IDEzNy45LTE1MiAyMi42Yy01LjEuOC05LjMgNC4zLTEwLjkgOS4ycy0uMiAxMC4zIDMuNSAxMy44bDExMC4zIDEwNi45LTI1LjUgMTUxLjRjLS45IDUuMSAxLjIgMTAuMiA1LjQgMTMuMiAyLjMgMS43IDUuMSAyLjYgNy45IDIuNiAyLjIuMCA0LjMtLjUgNi4zLTEuNmwxMzUuNy03MS45IDEzNi4xIDcxLjFjMiAxIDQuMSAxLjUgNi4yIDEuNSA3LjQuMCAxMy41LTYuMSAxMy41LTEzLjUuMC0xLjEtLjEtMi4xLS40LTMuMWwtMjYuMy0xNTAuNSAxMDkuNi0xMDcuNWMzLjktMy42IDUuMi05IDMuNi0xMy45eiIgZmlsbD0iIzFlMjEyNyIvPjwvZz48L3N2Zz4=);mask-image:url(data:image/svg+xml;utf8;base64,PHN2ZyB2aWV3Qm94PSIwIDAgNDg3LjIyIDQ4Ny4yMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48Zz48dGl0bGU+YmFja2dyb3VuZDwvdGl0bGU+PC9nPjxnPjx0aXRsZT5MYXllciAxPC90aXRsZT48cGF0aCBkPSJtNDg2LjU1IDE4Ni44MWMtMS42LTQuOS01LjgtOC40LTEwLjktOS4ybC0xNTItMjEuNi02OC40LTEzNy41Yy0yLjMtNC42LTctNy41LTEyLjEtNy41cy05LjggMi45LTEyLjEgNy42bC02Ny41IDEzNy45LTE1MiAyMi42Yy01LjEuOC05LjMgNC4zLTEwLjkgOS4ycy0uMiAxMC4zIDMuNSAxMy44bDExMC4zIDEwNi45LTI1LjUgMTUxLjRjLS45IDUuMSAxLjIgMTAuMiA1LjQgMTMuMiAyLjMgMS43IDUuMSAyLjYgNy45IDIuNiAyLjIuMCA0LjMtLjUgNi4zLTEuNmwxMzUuNy03MS45IDEzNi4xIDcxLjFjMiAxIDQuMSAxLjUgNi4yIDEuNSA3LjQuMCAxMy41LTYuMSAxMy41LTEzLjUuMC0xLjEtLjEtMi4xLS40LTMuMWwtMjYuMy0xNTAuNSAxMDkuNi0xMDcuNWMzLjktMy42IDUuMi05IDMuNi0xMy45eiIgZmlsbD0iIzFlMjEyNyIvPjwvZz48L3N2Zz4=);background:#f59f00}.commento-root .commento-card .commento-option-button:focus{outline:0}.commento-root .commento-card .commento-header{font-family:pt sans,Georgia,times new roman,Times,serif;line-height:1.5;margin-bottom:14px}.commento-root .commento-card .commento-avatar::after{content:"";display:block}.commento-root .commento-card .commento-name{font-weight:700;border:none;display:table;z-index:1;margin-left:48px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;width:fit-content;cursor:pointer}.commento-root .commento-card .commento-flagged::after{content:"Flagged";text-transform:uppercase;font-size:10px;background:#f03e3e;color:#fff;margin-left:8px;padding:2px 6px;border-radius:100px;line-height:17px}.commento-root .commento-card .commento-moderator::after{content:"Moderator";text-transform:uppercase;font-size:10px;background:#37b24d;color:#fff;margin-left:8px;padding:2px 6px;border-radius:100px;line-height:17px}.commento-root .commento-card .commento-subtitle{display:block;color:#999;font-size:12px;margin-left:48px}.commento-root .commento-card .commento-timeago{display:inline;color:#888;font-size:12px}.commento-root .commento-card .commento-score{display:inline;color:#888;font-size:12px;font-weight:700}.commento-root .commento-card .commento-timeago.commento-root .commento-card .commento-body p{margin-top:6px;margin-bottom:6px}.commento-root .commento-card .commento-options{float:right;position:relative;height:38px;z-index:2}.commento-root .commento-card .commento-options-mobile{margin-right:12px}.commento-root .commento-card .commento-options-clearfix{height:38px;width:1px;display:block}.commento-root .commento-card .commento-moderation{height:48px}.commento-root .commento-dark-card{background:#fff5f5}.commento-root .commento-highlighted-card{background:#fff9db}.commento-root .commento-avatar{width:38px;height:38px;border-radius:50%;display:flex;justify-content:center;align-items:center;color:#fff;font-size:22px;float:left;margin-right:10px;border:0 transparent}.commento-root .commento-avatar-img{width:38px;height:38px;border-radius:50%;display:flex;justify-content:center;align-items:center;float:left;margin-right:10px}.commento-root .commento-login-box-container{display:flex;justify-content:center;position:relative;width:100%;height:0;overflow:visible}.commento-root .commento-login-box-container .commento-login-box{width:90%;max-width:500px;min-height:100px;background:#f1f3f5;z-index:100;position:absolute;top:8px;padding:16px;opacity:1;transition:opacity .2s}.commento-root .commento-login-box-container .commento-login-box .commento-oauth-buttons-container{display:flex;justify-content:center}.commento-root .commento-login-box-container .commento-login-box .commento-oauth-buttons-container .commento-oauth-buttons{align-items:center;position:absolute;z-index:1;display:contents}.commento-root .commento-login-box-container .commento-login-box .commento-oauth-buttons-container .commento-oauth-buttons .commento-google-button{background:#dd4b39;text-transform:uppercase;font-size:13px;width:70px}.commento-root .commento-login-box-container .commento-login-box .commento-oauth-buttons-container .commento-oauth-buttons .commento-github-button{background:#000;text-transform:uppercase;font-size:13px;width:70px}.commento-root .commento-login-box-container .commento-login-box .commento-oauth-buttons-container .commento-oauth-buttons .commento-twitter-button{background:#00aced;text-transform:uppercase;font-size:13px;width:70px}.commento-root .commento-login-box-container .commento-login-box .commento-oauth-buttons-container .commento-oauth-buttons .commento-gitlab-button{background:#fc6d26;text-transform:uppercase;font-size:13px;width:70px}.commento-root .commento-login-box-container .commento-login-box .commento-oauth-buttons-container .commento-oauth-buttons .commento-sso-button{background:#000;text-transform:uppercase;font-size:13px;width:200px}.commento-root .commento-login-box-container .commento-login-box hr{border:none;background:#e9ecef;height:1px;margin:24px 0}.commento-root .commento-login-box-container .commento-login-box .commento-login-box-subtitle{color:#868e96;text-align:center;margin:12px 0;font-size:15px}.commento-root .commento-login-box-container .commento-login-box button,.commento-root .commento-login-box-container .commento-login-box input,.commento-root .commento-login-box-container .commento-login-box textarea{font-family:helvetica neue,Helvetica,Arial,sans-serif}.commento-root .commento-login-box-container .commento-login-box{font-size:14px;color:#495057;background:#f8f9fa}.commento-root .commento-login-box-container .commento-login-box body{margin:0}.commento-root .commento-login-box-container .commento-login-box a{text-decoration:none}.commento-root .commento-login-box-container .commento-login-box a:hover{cursor:pointer}.commento-root .commento-login-box-container .commento-login-box .commento-email-container .commento-email,.commento-root .commento-login-box-container .commento-login-box .shadow{box-shadow:0 1px 3px rgba(50,50,93,.15),0 1px 0 rgba(0,0,0,.02)}.commento-root .commento-login-box-container .commento-login-box .footer{position:relative;bottom:0;width:100%;margin-top:72px}.commento-root .commento-login-box-container .commento-login-box .footer .copyright{align-items:none;color:#dee2e6;background:#fff;text-align:center;padding:12px}.commento-root .commento-login-box-container .commento-login-box .footer .footer-inner{width:100%;background:#fff;display:flex;justify-content:center;align-items:center}.commento-root .commento-login-box-container .commento-login-box .footer .footer-inner .links{display:flex;justify-content:center;width:600px}.commento-root .commento-login-box-container .commento-login-box .footer .footer-inner .link-group{margin:40px}.commento-root .commento-login-box-container .commento-login-box .footer .footer-inner .link-group .header{text-transform:uppercase;font-weight:700;font-size:12px;color:#adb5bd}.commento-root .commento-login-box-container .commento-login-box .footer .footer-inner .link{margin-top:12px;margin-bottom:12px;display:block;color:#adb5bd;transition:all .2s}.commento-root .commento-login-box-container .commento-login-box .footer .footer-inner .link:hover{color:#495057}@media only screen and (max-width:1000px){.commento-root .commento-login-box-container .commento-login-box .footer .footer-inner .link-group{display:block}.commento-root .commento-login-box-container .commento-login-box .footer .footer-inner .links{display:block;width:90%}}.commento-root .commento-login-box-container .commento-login-box .commento-email-container{display:flex;justify-content:center;width:100%;margin:8px 0}.commento-root .commento-login-box-container .commento-login-box .commento-email-container .commento-email{border-radius:4px;background:#fff;width:100%;max-width:400px}.commento-root .commento-login-box-container .commento-login-box .commento-email-container .commento-email .commento-input{display:inline;height:40px;background:#fff;border:none;outline:0;padding:5px 5px 5px 10px;width:calc(100% - 150px)}.commento-root .commento-login-box-container .commento-login-box .commento-email-container .commento-email .commento-input::placeholder{color:#adb5bd}.commento-root .commento-login-box-container .commento-login-box .commento-email-container .commento-email .commento-email-button{height:40px;min-width:110px;float:right;background:#fff;border:none;outline:0;padding:0 10px;border-left:1px solid #f1f3f5;font-size:12px;text-transform:uppercase;text-align:center;font-weight:700;color:#1c7ed6;cursor:pointer;transition:all .2s;width:unset}.commento-root .commento-login-box-container .commento-login-box .commento-email-container .commento-email .commento-email-button:hover{color:#228be6}.commento-root .commento-login-box-container .commento-login-box .commento-email-container .commento-email .commento-email-button:disabled{cursor:default;color:#868e96}.commento-root .commento-login-box-container .commento-login-box .commento-forgot-link-container,.commento-root .commento-login-box-container .commento-login-box .commento-login-link-container{margin:16px;width:calc(100% - 32px);text-align:center}.commento-root .commento-login-box-container .commento-login-box .commento-forgot-link,.commento-root .commento-login-box-container .commento-login-box .commento-login-link{font-size:14px;font-weight:700;border-bottom:none}.commento-root .commento-login-box-container .commento-login-box .commento-forgot-link{font-size:13px;color:#868e96;font-weight:400}.commento-root .commento-login-box-container .commento-login-box .commento-login-box-close{position:absolute;right:16px;top:16px;width:16px;height:16px;opacity:.3}.commento-root .commento-login-box-container .commento-login-box .commento-login-box-close:hover{opacity:1;cursor:pointer}.commento-root .commento-login-box-container .commento-login-box .commento-login-box-close:after,.commento-root .commento-login-box-container .commento-login-box .commento-login-box-close:before{position:absolute;left:7px;content:" ";height:17px;width:2px;background-color:#495057}.commento-root .commento-login-box-container .commento-login-box .commento-login-box-close:before{transform:rotate(45deg)}.commento-root .commento-login-box-container .commento-login-box .commento-login-box-close:after{transform:rotate(-45deg)}.commento-root .commento-footer{margin:36px 0 12px;padding-right:12px}.commento-root .commento-footer .commento-logo-container{float:right}.commento-root .commento-footer .commento-logo-container .commento-logo{border:none;width:auto;height:32px;display:flex;align-items:center;padding:5px;border-radius:3px}.commento-root .commento-footer .commento-logo-container .commento-logo-text::before{content:"Powered by ";font-weight:400}.commento-root .commento-footer .commento-logo-container .commento-logo-text{font-size:13px;color:#495057;display:inline;line-height:24px;position:relative;font-weight:700}.commento-root .commento-hidden{display:none}.commento-root .commento-blurred{opacity:.4}.commento-root .commento-main-area{transition:filter .2s}.commento-root .commento-error-box{width:100%;border-radius:4px;height:32px;text-align:center;color:#f03e3e;font-weight:700}.commento-root .commento-moderation-notice{width:100%;border-radius:4px;height:32px;text-align:center;color:#f76707;font-weight:700;margin-top:16px}.commento-markdown{font-size:12px;font-weight:400}#commento-textarea-root{width:99%}.commento-comment-text{font-size:14px}.icon-side-bar{padding-left:2%;padding-right:2%}.pull-right{float:right}.pull-right a{color:#9a9a9a}.post p{text-align:justify}.side-share{position:fixed;top:42%;left:50%;margin-left:-24em;list-style:none}#loved-count{font-size:10px;margin-left:-7px;letter-spacing:.2px;font-family:pt sans,Helvetica,Arial,sans-serif}#love-share{cursor:pointer}#love-share-mb{cursor:pointer}.side-share-mb{display:none}#love-share-sign:hover{fill:pink}#fb-share-sign:hover{fill:#527ad1}#tw-share-sign:hover{fill:#07bdeb}#ln-share-sign:hover{fill:#0e76a8}.post-title a:hover,.post-title a:focus{text-decoration:none;color:#666}h2 a:hover{text-decoration:none}a:active,a:hover{outline:0}.hanchor{font-size:100%;visibility:hidden;color:#fff}h1:hover a,h2:hover a,h3:hover a,h4:hover a{visibility:visible}.sidebar-icons{text-align:left;margin-top:-5%;margin-bottom:-10%}.thin .utterances{max-width:100%}.sidebar-icons a:hover{text-decoration:none}.sidebar-icons a:visited{text-decoration:none}.pull-right svg{margin-bottom:-2px}.language-json .err{background-color:transparent;color:#9a9a9a}.language-javascript .err{background-color:transparent;color:inherit}pre{white-space:pre;overflow:auto}.img-container{overflow:hidden;max-height:350px;margin-bottom:20px;display:flex;width:100%}.img-container img,.img-container amp-img{border-radius:0;width:100%;align-self:center}img{display:block;margin-left:auto;margin-right:auto}.photo-credit{text-align:center;margin-bottom:3px;margin-top:-10px;color:#736d6d;font-size:12px;font-family:pt sans,Helvetica,Arial,sans-serif}.highlight{position:relative}.highlight pre{padding-right:75px}.highlight-copy-btn{position:absolute;bottom:7px;right:7px;border:0;border-radius:4px;padding:1px;font-size:.7em;line-height:1.8;color:#fff;background-color:#777;opacity:.5;min-width:55px;text-align:center}.highlight-copy-btn:hover{background-color:#666}.search-form{max-height:41px;display:none;width:108%;margin-bottom:2px;font-weight:200}.sidebar-toggle{border-style:solid;border-width:1px;border-color:#555}.masthead{background:#202020}.masthead-search-form{margin-top:-30px;font-weight:200}.sidebar-toggle{position:fixed;top:.8rem;left:1rem;display:block;padding-left:.6rem;padding-right:.6rem;padding-top:.25rem;padding-bottom:.15rem;color:#fff;background-color:#555;border-radius:.25rem;cursor:pointer}.sidebar-toggle:before{display:inline-block;width:1rem;height:.75rem;content:"";background-image:-webkit-linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%);background-image:-moz-linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%);background-image:-ms-linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%);background-image:linear-gradient(to bottom,#fff,#fff 20%,#555 20%,#555 40%,#fff 40%,#fff 60%,#555 60%,#555 80%,#fff 80%,#fff 100%)}.sidebar-toggle:active:before,#sidebar-checkbox:checked~.sidebar-toggle:before{background-image:-webkit-linear-gradient(to bottom,#555,#555 20%,#fff 20%,#fff 40%,#555 40%,#555 60%,#fff 60%,#fff 80%,#555 80%,#555 100%);background-image:-moz-linear-gradient(to bottom,#555,#555 20%,#fff 20%,#fff 40%,#555 40%,#555 60%,#fff 60%,#fff 80%,#555 80%,#555 100%);background-image:-ms-linear-gradient(to bottom,#555,#555 20%,#fff 20%,#fff 40%,#555 40%,#555 60%,#fff 60%,#fff 80%,#555 80%,#555 100%);background-image:linear-gradient(to bottom,#555,#555 20%,#fff 20%,#fff 40%,#555 40%,#555 60%,#fff 60%,#fff 80%,#555 80%,#555 100%)}.sidebar-toggle:active,#sidebar-checkbox:checked~.sidebar-toggle{color:#555;background-color:#fff;border-color:#555}.comment-newsletters{float:right;color:#000;font-weight:700;padding-left:10px}.comment-newsletters span{color:#1971c2}.newsletter-section{text-align:center;border-style:solid;border-width:1px;border-color:#bebbbb;border-radius:3px}#footer{padding-top:5px;background:#202020;border-bottom:5px solid #fff;border-top:10px solid #fff;text-align:center;color:#fff;font-size:.8em;padding:10px;line-height:16px;clear:both}.comments{color:#000}.comments a{color:#000;font-weight:800}.post-comment-avatar{height:38px;text-align:left;border-radius:50%;float:left}.post-comment-name{font-weight:600;font-size:15px;padding-left:30px;font-family:Merriweather,Georgia,times new roman,serif;line-height:1.5;display:block}.post-comment-date{font-weight:400;font-size:14px;font-family:Merriweather,Georgia,times new roman,serif}fieldset{border-style:none}.tiny p{text-align:left}.commento-options:hover .onhover{filter:invert(38%)sepia(1%)saturate(2184%)hue-rotate(23deg)brightness(97%)contrast(98%)}.post-date{color:#736d6d}.commento-root h1{font-weight:800}.commento-root h2{font-weight:500}.pagination-link{height:50px;font-family:pt sans,Helvetica,Arial,sans-serif;color:#313131;line-height:1.2rem}.pagination-link a{font-weight:400}.pagination-image{padding-top:5px;border-radius:0;overflow:hidden;height:150px;display:flex}.pagination-arrow{color:#222;font-family:pt sans,Helvetica,Arial,sans-serif;font-weight:700}.pagination-image img,.pagination-image amp-img{border-radius:0;width:100%;align-self:center}.toc li{list-style:decimal}#TableOfContents ul li ul{margin-bottom:0}#comment-submitted-box{font-size:18px;color:#2f343f}#comment-submitted-box h3{margin-top:0;text-align:center}#comment-error h3{margin-top:0;text-align:center;color:#a80f0f}.commento-markdown-help{border:0}.pagination-item{font-size:16px;padding:.5rem 1rem 1rem}.submit-github{background:#000}.tftextinput{height:28px;border-radius:2px;font-size:20px;border-style:none;padding-top:4px;padding-bottom:4px;max-width:200px;margin-right:2px;padding-right:29px}.tfbutton{margin:0;padding-right:25px;font-size:14px;outline:none;cursor:pointer;text-align:center;text-decoration:none;margin-top:-10px;margin-left:-37px;background:0 0;border:none}.tfbutton svg{padding-top:4px;margin-bottom:-4px;margin-right:-20px}.tfbutton svg:hover{fill:#6e6f70}.tfbutton::-moz-focus-inner{border:0}.tfclear{clear:both}.post .home-title{font-size:32px}.progress-container{width:100%;height:8px;position:fixed;top:0;left:0;z-index:4}.page-404{text-align:center}.progress-bar{height:.2rem;background:#642ecc;background:-moz-linear-gradient(90deg,rgba(100,46,204,1) 0%,rgba(64,38,203,1) 20%,rgba(46,114,204,1) 40%,rgba(46,183,204,1) 60%,rgba(46,204,180,1) 80%,rgba(46,204,113,1) 100%);background:-webkit-linear-gradient(90deg,rgba(100,46,204,1) 0%,rgba(64,38,203,1) 20%,rgba(46,114,204,1) 40%,rgba(46,183,204,1) 60%,rgba(46,204,180,1) 80%,rgba(46,204,113,1) 100%);background:linear-gradient(90deg,rgba(100,46,204,1) 0%,rgba(64,38,203,1) 20%,rgba(46,114,204,1) 40%,rgba(46,183,204,1) 60%,rgba(46,204,180,1) 80%,rgba(46,204,113,1) 100%);filter:progid:DXImageTransform.Microsoft.gradient(startColorstr="#642ecc",endColorstr="#2ecc71",GradientType=1);width:0%}#snackbar{font-family:pt sans,Helvetica,Arial,sans-serif;visibility:hidden;min-width:250px;margin:auto;background-color:#333;color:#fff;text-align:center;border-radius:5px;padding:10px;position:fixed;z-index:1000;bottom:30px;font-size:13px;left:50%;transform:translateX(-50%)}#snackbar.show{visibility:visible;-webkit-animation:fadein .5s;animation:fadein .5s}@-webkit-keyframes fadein{from{bottom:0;opacity:0}to{bottom:30px;opacity:1}}@keyframes fadein{from{bottom:0;opacity:0}to{bottom:30px;opacity:1}}@-webkit-keyframes fadeout{from{bottom:30px;opacity:1}to{bottom:0;opacity:0}}@keyframes fadeout{from{bottom:30px;opacity:1}to{bottom:0;opacity:0}}#close-privacy{cursor:pointer;color:#fff;padding-left:2px;font-size:10px;opacity:.9}#close-privacy:hover{opacity:1}#newsletter-submitted p{text-align:center}.newsletter-submitted-txt{padding-bottom:10px}#cover-spin{position:fixed;width:100%;left:0;right:0;top:0;bottom:0;background-color:rgba(255,255,255,.7);z-index:9999;display:none}@-webkit-keyframes spin{from{-webkit-transform:rotate(0deg)}to{-webkit-transform:rotate(360deg)}}@keyframes spin{from{transform:rotate(0deg)}to{transform:rotate(360deg)}}#cover-spin::after{content:"";display:block;position:absolute;left:48%;top:40%;width:50px;height:50px;border-style:solid;border-color:#1971c2;border-image:linear-gradient(45deg,rgba(100,57,242,1) 0%,rgba(242,55,55,1) 100%);border-image-slice:9;border-top-color:transparent;border-width:4px;border-radius:50%;-webkit-animation:spin .8s linear infinite;animation:spin .8s linear infinite}.commento-name{cursor:default}.newsletter-header{font-weight:700}.newsletter-txt{padding-bottom:14px}.generic-share{float:none;display:none;padding-top:3px}.generic-share .tw-share{padding-left:5px}.generic-share .ln-share{padding-left:5px}.generic-share a:hover{text-decoration:none}.container a code{color:#3273a8}input,button,label,textarea{font-family:pt sans,Helvetica,Arial,sans-serif}.pagination-text{line-height:1.5rem;font-size:14px;padding-top:4px;margin-bottom:0}.post h3,.post h4,.post h5,.post h6{font-weight:700}#list,.post-date,.commento-name{font-family:pt sans}.rss-feed{margin-bottom:-1px;fill:rgba(255,255,255,.6)}amp-img img{object-fit:contain}.tag-links-a{color:#736d6d}@media screen and (max-width:64em){.tfbutton{margin-left:-41px}.tfbutton svg{margin-bottom:-3px}}@media screen and (max-width:56em){body{line-height:1.5}.pagination-item{min-height:358px}.side-share{margin-left:-20em}.photo-credit{margin-top:-20px}}@media screen and (max-width:48em){.side-share{display:none}.generic-share{display:block}.side-share-mb{display:block}.side-share-mb .generic-share{padding-top:0}#loved-count{font-size:8px;margin-left:-5px;letter-spacing:.2px}.masthead-search-form{display:none}.search-form{display:block}.tftextinput{max-width:167px;height:26px;padding-bottom:2px;padding-top:2px;padding-right:29px}.tfbutton svg{margin-bottom:-3px}.pull-right{display:none}.sidebar-icons{margin-top:-5%;margin-bottom:-5%}.masthead-title{text-align:center}.masthead-title small{display:inline-block}.img-container{margin-bottom:0}.photo-credit{margin-top:-10px}.sidebar-toggle{padding-top:.2rem;padding-bottom:.2rem}.photo-credit{margin-top:0}}@media(max-width:30em){.progress-bar{height:.3rem}.pagination-item{min-height:296px}.post .home-title{font-size:32px}.post-title{font-size:32px}.post-date{font-size:16px}.tfbutton{margin-left:-36px}.pagination .older{margin-top:20px}.tfbutton svg{margin-bottom:-3px}.comment-newsletters .subscribe-to-nl{display:none}.commento-logged-container{margin-top:10px}.post-date .generic-share{float:none}.pagination-item{font-size:16px;padding:.5rem 1rem}.photo-credit{margin-top:-8px}}@media(max-width:15em){.tfbutton{margin-left:-35px}.masthead-title{text-align:center}.masthead-title small{display:none}.photo-credit{margin-top:-5px}.commento-logged-container{margin-bottom:10px}.tfbutton svg{margin-bottom:-3px}#comment-submitted-box{padding-top:40px;margin-bottom:-10px}.pagination-item{min-height:305px}}blockquote p strong{color:#696767}input:focus{outline:none;box-shadow:none}</style><meta name=twitter:label1 value="Reading time"><meta name=twitter:data1 value="0 Minutes"><link rel=alternate type=application/json href=https://ruddra.com/search/index.json><link rel=amphtml type=text/html href=https://ruddra.com/search/amp/><link rel=alternate type=application/rss+xml href=https://ruddra.com/search/index.xml><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon.png><link rel="shortcut icon" href=/favicon.ico><link rel=icon href=https://ruddra.com/logo.png><link rel=alternate type=application/rss+xml title=RSS href=/sitemap.xml><link rel=search href=/opensearch.xml title="Search at Ruddra" type=application/opensearchdescription+xml><meta name=generator content="Hugo 0.70.0"><meta name=twitter:card content="summary_large_image"><meta property="og:locale" content="en_US"><meta property="og:site_name" content="Ruddra.com | Blog by Arnab Kumar Shil"><meta property="article:publisher" content="https://ruddra.com"><meta property="article:section" content="Bulletin"><meta property="article:published_time" content="1016-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2020-05-27T00:00:00.00Z"><meta property="og:updated_time" content="2020-05-27T00:00:00.00Z"><meta property="og:image" content="https://ruddra.com/avatar.png"><meta property="og:image:secure_url" content="https://ruddra.com/avatar.png"><meta name=revised content="Ruddra.com, 06/07/27056"><meta property="og:image:width" content="720"><meta property="og:image:height" content="350"><meta name=twitter:site content="@ruddra"><meta name=twitter:creator content="@ruddra"><meta name=twitter:title content="Search"><meta name=twitter:description content><meta property="og:type" content="article"><meta property="og:title" content="Search"><meta property="og:description" content><meta name=twitter:image content="https://ruddra.com/avatar.jpg"><meta name=twitter:image:src content="https://ruddra.com/avatar.jpg"><meta property="article:tag" content="ruddra"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Search","keywords":["Blog"],"url":"https:\/\/ruddra.com\/search\/","dateCreated":"0001-01-01T00:00:00.00Z","datePublished":"0001-01-01T00:00:00.00Z","dateModified":"2020-05-27T00:00:00.00Z","articleBody":"","mainEntityOfPage":{"@type":"WebPage","@id":"https://ruddra.com"},"image":{"@type":"ImageObject","url":"https:\/\/ruddra.com","height":300,"width":750},"author":{"@type":"Person","name":"Arnab Kumar Shil","url":"https://ruddra.com"},"wordCount":"0","commentCount":"0","copyrightYear":"0001","discussionUrl":"https:\/\/ruddra.com\/search\/#comment-headline","isFamilyFriendly":"true","maintainer":{"@type":"Person","name":"Arnab Kumar Shil","url":"https://ruddra.com"},"publisher":{"@type":"Organization","name":"Ruddra.com | Blog by Arnab Kumar Shil","url":"https://ruddra.com","logo":{"@type":"ImageObject","url":"https:\/\/ruddra.com\/logo.png"}},"thumbnailUrl":"https:\/\/ruddra.com"}</script></head><body><div id=cover-spin></div><input type=checkbox class=sidebar-checkbox id=sidebar-checkbox><div class=sidebar id=sidebar><div class=sidebar-item><img data-src=/avatar.jpg class=lozad alt=avatar width=175px height=175px><p>Hi! I am Arnab, a full stack developer specializing in <b><a href=/tags/python/>Python</a></b>,
<b><a href=/tags/django/>Django</a></b>, <b><a href=/tags/docker/>Docker</a></b>, <b><a href=/tags/reactjs/>React</a></b>, <b><a href=/tags/openshift/>OpenShift</a></b>.</p><p class=sidebar-icons><a class=icon-side-bar href=https://stackoverflow.com/users/2696165/ruddra target=_blank rel="noopener me" title=stackoverflow><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="10 10 100 100" fill="currentcolor" class="feather"><path d="M84.4 93.8V70.6h7.7v30.9H22.6V70.6h7.7v23.2z"/><path d="M38.8 68.4l37.8 7.9 1.6-7.6-37.8-7.9-1.6 7.6zm5-18 35 16.3 3.2-7-35-16.4-3.2 7.1zm9.7-17.2 29.7 24.7 4.9-5.9-29.7-24.7-4.9 5.9zm19.2-18.3-6.2 4.6 23 31 6.2-4.6-23-31zM38 86h38.6v-7.7H38V86z"/></svg></a><a class=icon-side-bar href=https://facebook.com/ruddra.blog/ target=_blank rel="noopener me" title=facebook><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-facebook"><path d="M18 2h-3a5 5 0 00-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 011-1h3z"/></svg></a><a class=icon-side-bar href=https://twitter.com/ruddraarnab target=_blank rel="noopener me" title=twitter><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-twitter"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a><a class=icon-side-bar href=https://www.linkedin.com/in/ruddraarnab target=_blank rel="noopener me" title=linkedin><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-linkedin"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a><a class=icon-side-bar href=https://github.com/ruddra target=_blank rel="noopener me" title=github><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77a5.44 5.44.0 00-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></p></div><nav class=sidebar-nav><a class=sidebar-nav-item href=/>Home</a>
<a class=sidebar-nav-item href=/about/>About Me</a>
<a class=sidebar-nav-item href=/privacy/>Privacy Policy</a>
<a class=sidebar-nav-item href=/contact/>Contact Me</a><form target=_top class="sidebar-nav-item search-form" method=get action=/search><input type=text class=tftextinput name=q size=21 maxlength=120><button type=submit class=tfbutton><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="rgb(155,155,155)"><path d="M9.145 18.29c-5.042.0-9.145-4.102-9.145-9.145S4.103.0 9.145.0s9.145 4.103 9.145 9.145-4.102 9.145-9.145 9.145zm0-15.167c-3.321.0-6.022 2.702-6.022 6.022s2.702 6.022 6.022 6.022 6.023-2.702 6.023-6.022-2.702-6.022-6.023-6.022zm9.263 12.443c-.817 1.176-1.852 2.188-3.046 2.981l5.452 5.453 3.014-3.013-5.42-5.421z"/></svg></button></form></nav><div class=sidebar-item id=copyright><p>&copy; 2020 All rights reserved &#183; <a href=/posts/index.xml><svg class="rss-feed" xmlns="http://www.w3.org/2000/svg" width="10" height="9" viewBox="0 0 24 24"><path d="M6.503 20.752c0 1.794-1.456 3.248-3.251 3.248-1.796.0-3.252-1.454-3.252-3.248s1.456-3.248 3.252-3.248c1.795.001 3.251 1.454 3.251 3.248zM0 8.18v4.811c6.05.062 10.96 4.966 11.022 11.009h4.817c-.062-8.71-7.118-15.758-15.839-15.82zm0-3.368c10.58.046 19.152 8.594 19.183 19.188H24c-.03-13.231-10.755-23.954-24-24v4.812z"/></svg></a></p></div></div><div id=snackbar>By using our website, you agree to our <a href=/privacy>privacy policy</a> <a id=close-privacy>&#x2715;</a></div><div class=wrap><div class=masthead><div class=container><h3 class=masthead-title><a href=/ title=Home style=color:#cfcdcd>Ruddra</a><small style=color:#838383;font-size:1.25rem>.com</small></h3><form target=_top class="masthead-search-form pull-right" method=get action=/search><input type=text class=tftextinput name=q size=21 maxlength=120><button type=submit class=tfbutton><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="rgb(155,155,155)"><path d="M9.145 18.29c-5.042.0-9.145-4.102-9.145-9.145S4.103.0 9.145.0s9.145 4.103 9.145 9.145-4.102 9.145-9.145 9.145zm0-15.167c-3.321.0-6.022 2.702-6.022 6.022s2.702 6.022 6.022 6.022 6.023-2.702 6.023-6.022-2.702-6.022-6.023-6.022zm9.263 12.443c-.817 1.176-1.852 2.188-3.046 2.981l5.452 5.453 3.014-3.013-5.42-5.421z"/></svg></button></form></div></div><div class=progress-container><div class=progress-bar id=myBar></div></div><div class="container content"><div class=post><h1 class=post-title><b>Search: </b><span class=search-content></span></h1><ul class=search-items id=list></ul><script src=/js/lunr.min.js></script><script>const posts=[{"content":"If you want to use data science packages like numpy, scipy, pandas etc with your docker container and using pip to install them, then it will take forever to build the images. Because pip compiles the C extended code and that compilation needs a lot of time. For numpy it took me around 4 minutes. For scipy, I terminated the build process after 30 minutes or so.Today, I am going to share some ways in which you can build them faster. You can follow any one of them. Table of contents    One: build using Anaconda Two: use apt-get Three: use separate layers to install packages Four: build your own wheel In conclusion   One: build using Anaconda Anaconda is a free and open-source distribution of the Python(and R programming language as well, but we are not going to consider it here) for scientific computing. It uses a pre-compiled binary version of the packages so you do not have to compile it on your machine. Here is an example Dockerfile based on official anaconda image:FROMcontinuumio/miniconda3# Set environment variablesENV PYTHONDONTWRITEBYTECODE 1ENV PYTHONUNBUFFERED 1# Set work directoryWORKDIR/code# Install dependenciesCOPY requirement.txt /code/RUN conda install -c conda-forge --file requirement.txt# Copy projectCOPY . /code/Here is an another example using alpine linux:FROMfrolvlad/alpine-miniconda3# Set environment variablesENV PYTHONDONTWRITEBYTECODE 1ENV PYTHONUNBUFFERED 1# Set work directoryWORKDIR/code# Install dependenciesCOPY requirement.txt /code/RUN conda install -c conda-forge --file requirement.txt# Copy projectCOPY . /code/Two: use apt-get If you do not want to use anaconda, then apart from pip, there is another package manager called apt which distributes data science packages like numpy, scipy, pandas. apt is the official package manager for Ubuntu and it is very reliable. Although versions of these packages are not latest, you can install the newest versions with pip. This time installation using pip will take much less time.FROMpython:3.7# Set environment variablesENV PYTHONDONTWRITEBYTECODE 1ENV PYTHONUNBUFFERED 1# Install packagesRUN apt-get update -y \u0026amp;\u0026amp; \\  apt-get install -y python-scipy\\  python-numpy python-pandas \u0026amp;\u0026amp;\\  apt-get clean \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/*# Install dependenciesCOPY requirement.txt /code/RUN pip install --no-cache-dir -r /code/requirement.txtThree: use separate layers to install packages If you install packages which take longer in a separate layer, then docker will cache them in subsequent builds. In that way your first time build will take a long time, but after that building time will be significantly reduced for next builds.FROMpython:3.7# Set environment variablesENV PYTHONDONTWRITEBYTECODE 1ENV PYTHONUNBUFFERED 1# Installing scipyRUN pip3 install --no-cache-dir --disable-pip-version-check scipy==1.3.1# Installing numpy, scipy, psycopg2, gensimRUN pip3 install --no-cache-dir \\  pandas==0.25.2 \\  numpy==1.17.3 \\  psycopg2==2.8.4 \\  gensim==3.8.1#Install dependenciesCOPY requirement.txt /code/RUN pip install -r requirement.txtAdvantage of this separation of layers is that even if you change your requirements.txt file or change in source code, it will not hamper the cached layer of data science packages.Four: build your own wheel Finally, you can build your own wheel files using pip-wheel. Wheel archives for your requirements and dependencies. Wheel is a built-package format, and offers the advantage of not recompiling your software during every install. Here is how you can do that:pip wheel numpy scipy pandas -w wheels It will store wheel files(extension .whl) inside the wheels directory. You can either add them to your docker image(like following example). FYI, keeping wheel files with repositories will increase its size. Alternatively you can put it somewhere in the cloud and download it during the docker build process.FROMpython:3.7# Set environment variablesENV PYTHONDONTWRITEBYTECODE 1ENV PYTHONUNBUFFERED 1# Installing dependenciesCOPY requirement.txt /code/COPY ./wheels /tmp/wheelsRUN pip install --find-links=/tmp/wheels -r /code/requirements.txtRUN rm -rf /tmp/wheelsIn conclusion In this article, we saw that there are several ways to reduce build time of the docker image with data science packages and you can follow any of them. If you have any thoughts regarding this, please let me know at the comment section below.","date":"May 24, 2020","link":"/posts/docker-reduce-build-time-for-data-science-packages/","readTime":3,"title":"Reduce Build Time for Docker Image with Data Science Packages"},{"content":"Let us say, you need an authentication service in a rush, which needs to be cutting edge and uses JWT based authentication. Or you need a stand alone authentication service to be plugged in with other microservices. Do not worry, you can create one in 10 minutes. All you need to have is Python and Docker installed in your machine. 10 minutes may sound a bit exaggerating but trust me, it should not take longer than that . In this article, I am going to share how you can do this in a few simple steps.FYI, for this project you do not need prior django knowledge. Just having a coding background should be good enough or some Python syntax knowledge would be nice. Without further ado, let us start our stopwatch and begin: Table of contents    Install CookieCutter Create Django project Setup virtual environment(optional) Setup RESTful API Setup JWT authentication Spin up the docker Run outside of docker(optional) Miscellaneous  Rebuild docker images Remove unnecessary dependencies Remove unnecessary codes from urls.py Add social authentication Add API documentation Change header Tweak authentication service   Why wait 10 minutes when you can clone it In conclusion   Install CookieCutter First, we need to install the cookiecutter. It allows us to create projects from templates. To install it, we need to run:python3 -m pip install cookiecutter --user Create Django project Now we are going to create the django project using cookiecutter. We are going to use cookiecutter-django as template project:cookiecutter https://github.com/pydanny/cookiecutter-django It will prompt for many options. Here is my setup: We have prompted yes for Django Rest Framework and Docker. Rest of the setup can be based on your preference. Cool, now we have our boilerplate project.Setup virtual environment(optional) This is an optional step, only required if you want to run this project outside of docker.Let us set up a virtual environment using Python\u0026rsquo;s build in venv and activate it:python3 -m venv venv source venv/bin/activate Now we can install dependencies inside the project:pip install -r requirements/local.txt FYI, some dependency installation might throw errors. Use google to resolve them .Setup RESTful API Now we have our django application, let us integrate API. We already have Django Rest Framework pre-installed with cookiecutter, so we are going to use a library which utilizes that and provide authentication. My preference is djoser.According to their documentation, let us add the app in config/settings/base.py, like this:THIRD_PARTY_APPS = [ \u0026#34;crispy_forms\u0026#34;, \u0026#34;allauth\u0026#34;, \u0026#34;allauth.account\u0026#34;, \u0026#34;allauth.socialaccount\u0026#34;, \u0026#34;rest_framework\u0026#34;, \u0026#34;djoser\u0026#34; # \u0026#34;rest_framework.authtoken\u0026#34;, #\u0026lt;--- ] We are commenting out rest_framework.authtoken(marked with #\u0026lt;---) from THIRD_PARTY_APPS, as we plan to use JWT based authentication.Let us change in config/urls.py as well:urlpatterns += [ # API base url path(\u0026#34;api/\u0026#34;, include(\u0026#34;config.api_router\u0026#34;)), # DRF auth token # path(\u0026#34;auth-token/\u0026#34;, obtain_auth_token), #\u0026lt;-- # djoser auth path(\u0026#34;auth/\u0026#34;, include(\u0026#39;djoser.urls\u0026#39;)), ] Keep in mind that cookiecutter-django provides a token based authentication. As our plan is to use JWT based authentication, so we will be removing token based authentication related codes from the project(marking by #\u0026lt;-- at the end).Setup JWT authentication Our project is ready for integration with JWT based authentication. For that, we are going to use django-rest-framework-jwt:Let us add JWTAuthentication authentication class to the DEFAULT_AUTHENTICATION_CLASSES in config/settings/base.py:REST_FRAMEWORK = { \u0026#34;DEFAULT_AUTHENTICATION_CLASSES\u0026#34;: ( \u0026#34;rest_framework_simplejwt.authentication.JWTAuthentication\u0026#34;, ), \u0026#34;DEFAULT_PERMISSION_CLASSES\u0026#34;: (\u0026#34;rest_framework.permissions.IsAuthenticated\u0026#34;,), } We need to add token generation url paths to config/urls.py:urlpatterns += [ path(\u0026#39;auth/\u0026#39;, include(\u0026#39;djoser.urls\u0026#39;)), path(\u0026#39;auth/\u0026#39;, include(\u0026#39;djoser.urls.jwt\u0026#39;)), ] Now, let us add the new dependencies in requirements/base.txt file:djangorestframework-simplejwt==4.4.0 djoser==2.0.3 (Optional) You can install it in virtual environment by:pip install djoser pip install djangorestframework_simplejwt Please check the documentation regarding API endpoints.Spin up the docker Let us start our docker containers using docker-compose. If you don\u0026rsquo;t have it, then install it from docker-compose installation guide. My suggestion is to use pip:python -m pip install docker-compose --user Now, let us start the development server by(from root of the project):docker-compose -f local.yml up Awesome, we have our project running. For production server use:docker-compose -f production.yml up But, you need to keep in mind, if you want mail sending functionality, make sure to add them either in .envs/.product/.django file or set as an environment variable in *.yml file.Run outside of docker(optional) This is an optional step because we are assuming you will be using docker.If you have added environment variables properly(in .envs folder or in environment variable), then run:python manage.py migrate To populate tables inside the database. To run the project, use:python manage.py runserver FYI, it is a development server. For production grade servers, use gunicorn or uwsgi.Miscellaneous Okay, now our project is running. So let us clean it up a little. Also, some of the setup might be a bit complex and may require some django knowledge.Rebuild docker images If you have changed something inside code, better rebuild the docker images. You can do that by:docker-compose -f local.yml build Remove unnecessary dependencies This step is useful if you want to make this application only as a REST API based backend.We are going to clean up codes regarding django allauth and crispy form. Just remove them from THIRD_PARTY_APPS inside config/settings/base.py .THIRD_PARTY_APPS = [ # \u0026#34;crispy_forms\u0026#34;, #\u0026lt;--- # \u0026#34;allauth\u0026#34;, #\u0026lt;--- # \u0026#34;allauth.account\u0026#34;, #\u0026lt;--- # \u0026#34;allauth.socialaccount\u0026#34;, #\u0026lt;--- \u0026#34;rest_framework\u0026#34;, \u0026#34;rest_framework.authtoken\u0026#34;, ] Also, we need to remove some unnecessary codes from template passport/templates/base.html. Otherwise it will cause errors because of missing libraries. Let us replace it with something simple:\u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello from base\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; We also need to clean up allauth related codes from config/urls.py:urlpatterns = [ path(\u0026#34;\u0026#34;, TemplateView.as_view(template_name=\u0026#34;pages/home.html\u0026#34;), name=\u0026#34;home\u0026#34;), path( \u0026#34;about/\u0026#34;, TemplateView.as_view(template_name=\u0026#34;pages/about.html\u0026#34;), name=\u0026#34;about\u0026#34; ), # Django Admin, use {% url \u0026#39;admin:index\u0026#39; %} path(settings.ADMIN_URL, admin.site.urls), # User management path(\u0026#34;users/\u0026#34;, include(\u0026#34;passport.users.urls\u0026#34;, namespace=\u0026#34;users\u0026#34;)), # path(\u0026#34;accounts/\u0026#34;, include(\u0026#34;allauth.urls\u0026#34;)), #\u0026lt;-- # Your stuff: custom urls includes go here ] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) Commented out part is marked by #\u0026lt;--.Remove unnecessary codes from urls.py The config/urls.py is in untidy condition. Let us replace it with following:from django.conf import settings from django.conf.urls.static import static from django.contrib import admin from django.urls import include, path from django.views import defaults as default_views from django.views.generic import TemplateView urlpatterns = [ path(\u0026#34;\u0026#34;, TemplateView.as_view(template_name=\u0026#34;pages/home.html\u0026#34;), name=\u0026#34;home\u0026#34;), # Django Admin, use {% url \u0026#39;admin:index\u0026#39; %} path(settings.ADMIN_URL, admin.site.urls), ] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) # API URLS urlpatterns += [ # DRF auth token path(\u0026#34;auth-token/\u0026#34;, obtain_auth_token), ] if settings.DEBUG: # This allows the error pages to be debugged during development, just visit # these url in browser to see how these error pages look like. urlpatterns += [ path( \u0026#34;400/\u0026#34;, default_views.bad_request, kwargs={\u0026#34;exception\u0026#34;: Exception(\u0026#34;Bad Request!\u0026#34;)}, ), path( \u0026#34;403/\u0026#34;, default_views.permission_denied, kwargs={\u0026#34;exception\u0026#34;: Exception(\u0026#34;Permission Denied\u0026#34;)}, ), path( \u0026#34;404/\u0026#34;, default_views.page_not_found, kwargs={\u0026#34;exception\u0026#34;: Exception(\u0026#34;Page not Found\u0026#34;)}, ), path(\u0026#34;500/\u0026#34;, default_views.server_error), ] if \u0026#34;debug_toolbar\u0026#34; in settings.INSTALLED_APPS: import debug_toolbar urlpatterns = [path(\u0026#34;__debug__/\u0026#34;, include(debug_toolbar.urls))] + urlpatterns Add social authentication As per documentation of djoser, you can add social authentication using social-auth-app-django. You can add it to requirements to requirements/base.txt by:social-auth-app-django==3.1.0 (Optional) Install it in virtual environment by:pip install social-auth-app-django==3.1.0 You need to configure the social backends to accommodate which oauth services you will be using. The api will be accessible from http://localhost:800/auth/o/{{provider}}. Check the social endpoints provided by djoser documentation.Add API documentation We can easily add some API documentation to our project. My choice is to use a solution based on reDoc, for example: drf-yasg. We can simply add the dependency in requirements/local.txt:drf-yasg==1.17.1 (Optional) Install it in virtual environment by:pip install drf-yasg==1.17.1 And add the following codes at the bottom of config/urls.py:if \u0026#34;drf_yasg\u0026#34; in settings.INSTALLED_APPS: from drf_yasg.views import get_schema_view from drf_yasg import openapi from rest_framework import permissions schema_view = get_schema_view( openapi.Info( title=\u0026#34;Snippets API\u0026#34;, default_version=\u0026#39;v1\u0026#39;, description=\u0026#34;Test description\u0026#34;, terms_of_service=\u0026#34;https://www.google.com/policies/terms/\u0026#34;, contact=openapi.Contact(email=\u0026#34;contact@snippets.local\u0026#34;), license=openapi.License(name=\u0026#34;BSD License\u0026#34;), ), public=True, permission_classes=(permissions.AllowAny,), ) urlpatterns = [ path(\u0026#39;redoc/\u0026#39;, schema_view.with_ui(\u0026#39;redoc\u0026#39;, cache_timeout=0), name=\u0026#39;schema-redoc\u0026#39;), ] + urlpatterns Change header By default, HEADER will look be like this JWT \u0026lt;XXXXXXXX\u0026gt;, which is not Bearer schema. You can change it to Bearer \u0026lt;XXXXXXX\u0026gt; by adding the following lines in config/settings/base.py:SIMPLE_JWT = { \u0026#39;AUTH_HEADER_TYPES\u0026#39;: (\u0026#39;Bearer\u0026#39;,), } Tweak authentication service You can change authentication service settings, like if it will send email or not. From djoser documentation, here is an example settings(in config/settings/base.py):DJOSER = { \u0026#39;PASSWORD_RESET_CONFIRM_URL\u0026#39;: \u0026#39;#/password/reset/confirm/{uid}/{token}\u0026#39;, \u0026#39;USERNAME_RESET_CONFIRM_URL\u0026#39;: \u0026#39;#/username/reset/confirm/{uid}/{token}\u0026#39;, } Why wait 10 minutes when you can clone it Assuming you do not want to take the trouble of creating the project from scratch, then simply clone my project from github and spin up the docker.In conclusion Steps till miscellaneous took me around 5-10 minutes. But it may take longer depending on various conditions like on internet speed, machine configuration and prior python/django knowledge. But, this solution is top notch and you can easily deploy it with docker anywhere. You can also plug it in as a microservice and handle authentication smoothly.Thank you for reading. Let me find your experience in the comment section below.","date":"April 22, 2020","link":"/posts/django-jwt-auth-app-in-10-mins/","readTime":7,"title":"Create an Application with JWT based Authentication in 10 Minutes using Django"},{"content":"AWS CodeBuild has nice integration with different Git repository hosting service providers(like GitHub, BitBucket, even amazon\u0026rsquo;s own CodeCommit etc). Even using WebHook is pretty easy. You can start a build in CodeBuild for every push, pull, PR created, PR merged etc. But it can be bit tricky when it comes to trigger for every tag push only, as it us not a default event type provided by CodeBuild. You need to add some customized settings to the filter section of the Source for it. Here is how I implemented it step by step. Table of contents    Enable re-build on code change Change event type Track branch Add filter Use tag in buildspec.yml file In conclusion   Enable re-build on code change First you need to enable Rebuild every time a code change is pushed to this repository from the source settings. Change event type You need to change the Event Type to push to trigger the CodeBuild. Track branch I am assuming you want to track tags from any branch. So I have put * on branch settings: Add filter Finally, I have added the filter in Start a build under this condition. The filter is ^refs/tags/.* under HEAD_REF. Use tag in buildspec.yml file If you want to use the Tag number in buildspec.yml, then use this:- TAG_NUMBER=\u0026#34;$(git describe --tags --abbrev=0)\u0026#34;But above code won\u0026rsquo;t work if you are tracking a build which is based on a commit that is not associated a tag or a build that is not triggered by a tag push. To resolve this, you need to increase depth value. You can set it to full but it will increase build time depending of the size of the repository. It is in the Additional configuration section. In conclusion IMHO, CodeBuild is a fantastic tool for implementing CI/CD, but documentation lacks some minor details. It might give you subtle hints but you need to dig deeper to find the exact solution. Let us talk more in the comment section if you think this article is missing something. Cheers!!","date":"April 16, 2020","link":"/posts/aws-codebuild-use-git-tags/","readTime":2,"title":"Run a Build in AWS CodeBuild When a Git Tag is Pushed"},{"content":"Hugo is a fantastic framework to generate static site from markdown and serve them. Using environment variables in templates is a breeze but using them in markdown files can be a bit tricky. You need to use custom shortcodes as workaround to resolve this issue. Table of contents    What is shortcodes Custom shortcodes Use environment variable in shortcodes  By getenv variable By site variable   In conclusion   What is shortcodes Now the question comes, what is shorcode? From Hugo\u0026rsquo;s documentation: Shortcodes can access page variables and also have their own specific built-in variables. Meaning you can use this to access different variables like from config file or you can write your own shortcodes to provide your custom variables. We will be using both to access environment variables. Let us see how to create custom shortcodes. This shortcodes will be used in markdown files.Custom shortcodes First, you need to create a new folder named shortcodes inside the layouts directory. Inside that, you need to create a template which will be treated as a shortcode. For example, if you create layouts/shortcodes/abc.html, then you can access variables inside home_dir.html in markdown files via {{\u0026lt; home_dir \u0026gt;}}. The folder structure will look like this:Hugo Project  content  layouts   shortcodes   home_dir.html   custom_dir.html  static  themes config.toml More information can be found in documentation.Use environment variable in shortcodes Now it is time to use environment variables in these shortcodes. There are two ways you can achieve this.By getenv variable You can use getenv in the shortcodes. For example, if you write {{ getenv \u0026quot;HOME\u0026quot; }} in home_dir.html then, if you use {{\u0026lt; home_dir \u0026gt;}} in markdown, it should render the home directory name in rendered pages.By site variable You can directly access site-level variables(or global variables) in shortcodes. For that, first you need to declare them in config.toml(or json or other formats):theme = \u0026#34;your theme\u0026#34; baseURL = \u0026#34;/\u0026#34; [params] homeDir = \u0026#34;/home/arnab\u0026#34; Then in home_dir.html, you can write:{{ .Site.Params.homeDir }} Finally, if you want to provide your environment variable as homeDir, then pass it as argument when running hugo server:env HUGO_PARAMS_homeDir=${HOME} hugo server Or if you want to generate the html files, then run:env HUGO_PARAMS_homeDir=${HOME} hugo In conclusion It is very easy to use environment variables in markdown using Hugo. Although there are many debates on if you should use them or not. That is why there is no straight forward process to do it in Hugo.Thank you for reading. If you have any questions or queries, please use the comment section below.","date":"April 3, 2020","link":"/posts/hugo-use-env-variable-in-md/","readTime":3,"title":"Hugo: Use Environment Variable in Markdown Files"},{"content":" Disclaimer: this article has been generated as part of IBM Data Science Professional Certificate course\u0026rsquo;s final submission. This is the report of the project for IBMs Data Science Professional Certificate on Coursera. Table of contents    Business problem Data Approach Data preparation Source code Methodology  Step one: New York city data with latitude and longitude Step two: New York city data with population Step three: combine step one and step two Step four: collect hospital data from Foursquare Step five: collect hospital bed data from NYS Health Profile Step six: combine step four and step five Step seven: combine data from step three and step six Step eight: add bed and icu per hundred people to data frame Step nine: K-means clustering Step ten: merge cluster labels with dataset Step eleven: visualize with folium Step twelve: use scatter plot Step thirteen: see which borough goes to which cluster Step fourteen: see neighborhoods without hospital   Results and discussion  What could be done better   Conclusion   Business problem Right now, New York is one of the worst hit state by COVID-19 in USA. New York city is at the center of the disaster. The hospitals are already stretched thin with patients overflowing. According to New York Times report, (at the moment of writing) death toll was 365, case count topped 23,000.I was motivated by this to create something useful which would give some insight on this situation. In this project we are going to determine which neighborhood is best prepared for this pandemic, by finding out the best ratio of hospital beds per person for each neighborhood in this city.By all means, the reports here should not be used as a measuring tool, because in reality the situation has been changed a lot since Coronavirus COVID-19 has hit the city.Data We will be collecting data from following sources: New York City data that contains borough, neighborhoods along with their latitudes and longitudes.  Data source: NYC data set.   We are going to get population data from Scraping Wikipedia.  Data source: Wikipedia page of NYC neighborhood. We are going to go through each of the links of neighborhood and find the population of each of them.   Hospital information is going to be fetched from foursquare API.  Data source: foursquare API   Hospital bed information is going to be fetched from NYS Health Profile website.  Data source: NYS Health Profile.    Approach This is our approach to resolve issue: Collect the New York city data from here. Collect population data for each neighborhood by scraping Wikipedia. Using Foursquare API we will get hospitals for each neighborhood. Collect hospital bed data by scraping data from NYS Health Profile. Data Visualization and some statistical analysis. Analyzing using Clustering (Specially K-Means). Find the best value of K Visualize the neighborhood max density of hospital beds per 100 people. Visualize the neighborhood max density of hospital ICU beds per 100 people. Inference From these results and related conclusions.  Data preparation Data used in the analysis are listed below: First, get the json data from here, which will contain borough, neighborhood, latitude and longitude information. neighborhood data in New York City will be collected from scraping the Wikipedia page. links given in the neighborhood section of the table will be visited via scraper, and find the population for each of them. Then data will be cleaned up and used to create a data frame containing borough, neighborhood and population. Hospitals per neighborhood information will be collected from foursquare API. We will collect bed and icu capacity information from NYS Health Profile website. Will be using selenium based scraping as this is a dynamic site.  Source code Source code of this project can be found on github.Methodology Step one: New York city data with latitude and longitude We are using requests to get the json data from nyc dataset and stored it in a data frame. Step two: New York city data with population Then we can use BeautifulSoup to scrape boroughs from Wikipedia. Then we have collected every link given in neighborhood column of the table. From each link, we can run iteration via requests to visit those Wikipedia pages, and scrap population data from right hand side table. Step three: combine step one and step two We can combine data frames from previous steps into one based on \u0026ldquo;neighborhood\u0026rdquo; and \u0026ldquo;borough\u0026rdquo;: Here is a box chart of \u0026ldquo;Population\u0026rdquo; per \u0026ldquo;borough\u0026rdquo;: Also, another box chart of \u0026ldquo;neighborhood\u0026rdquo; per \u0026ldquo;borough\u0026rdquo;: Step four: collect hospital data from Foursquare After collecting population data, now it is time to collect the hospital data. We can use the Foursquare API to fetch hospital data for latitude and longitude of each neighborhood from the previous dataset. Step five: collect hospital bed data from NYS Health Profile We can also collect hospital bed related data from NYS Health Profile website. We can scrap data by using Selenium with BeautifulSoap. We have collected the IDs of hospitals in NYC manually, and based on those IDs, we have scraped data from NYS Health Profile website. The data frame looks like this: Step six: combine step four and step five Now we are going to combine data from step four and step five. We are going to internally join the data frame based on \u0026ldquo;neighborhood\u0026rdquo; and \u0026ldquo;borough\u0026rdquo;. We are going to clean up the data a little bit and sum up bed count and icu bed count grouping by \u0026ldquo;neighborhood\u0026rdquo; and \u0026ldquo;borough\u0026rdquo;: Here is a box charts of \u0026ldquo;bed count\u0026rdquo; per \u0026ldquo;borough\u0026rdquo;: Also another box charts of \u0026ldquo;ICU bed count\u0026rdquo; per \u0026ldquo;borough\u0026rdquo;: Step seven: combine data from step three and step six Now we are going to combine data from step three and step six. Means, we are going to combine the population data with hospital bed count data. We are going to merge two data frames based on \u0026ldquo;neighborhood\u0026rdquo; and \u0026ldquo;borough\u0026rdquo;. New data frame looks like this: Step eight: add bed and icu per hundred people to data frame Now we are going to calculate bed per hundred people based on two rows: Population and Bed Number. Then add this to the data frame. Similarly, we are going to add ICU data to data frame: Step nine: K-means clustering Now we are going to use k-means clustering to partition the data into k groups. we will be using elbow method to find the optimal number of k. The \u0026ldquo;elbow\u0026rdquo; (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. In the visualizer \u0026ldquo;elbow\u0026rdquo;, value of k is 3. Step ten: merge cluster labels with dataset After that, we are going to merge cluster labels of groups with data frames. The data frame looks like this: Step eleven: visualize with folium Now, we are going to use folium to visualize the distribution. The first map illustrates the clusters where the radius of the Circle marker is proportional to hospital beds per hundred people. The second map illustrates the clusters where the radius of the Circle marker is proportional to icu beds per hundred people. We can see that one of the clusters (blue circle) consists in one borough - Manhattan.Step twelve: use scatter plot Let\u0026rsquo;s look at the scatter plots of our data and define our clusters with colors. The grey circle marker is representing the centroid of each cluster. Don\u0026rsquo;t forget that our data is normalized, so the axes do not deliver real values.   We can observe the obvious outlier here. This neighborhood has a high number of beds per people ratio. From maps above we can easily say that it is Murray Hill.Step thirteen: see which borough goes to which cluster Let us see which boroughs belong to which clusters.Here is the dataset for cluster 0: Here is the dataset for cluster 1: Here is the dataset for cluster 2: Step fourteen: see neighborhoods without hospital So far, we have analyzed dataset for neighborhoods with hospitals. Now, we should look into neighborhoods without hospital data: If we see the indexes of neighborhoods with and without hospital, it should look like this: We can see that there are 100 neighborhoods which does not have any hospital.Results and discussion During the analysis, three clusters were defined. One cluster(cluster 2), that consists of only one area, has been defined as the outsider, due to the high number of hospital beds, which means it is better equipped to handle this pandemic. Two other groups were clustered according to bed per hundred people and icu bed per hundred people. It is obvious that the cluster with the lowest beds per person is the place where we should concentrate on providing beds and other equipment(Cluster 0). We also should look into conditions in Queens Village and Williamsburg as they have very low beds per hundred people. Furthermore, in hundred other neighborhoods, there is no hospital data. Hence, people living there are at high risk of not being treated during pandemic.What could be done better Foursquare doesn\u0026rsquo;t represent the full picture, since many hospitals are not on the list. For that reason, other maps could be utilized such as Google map or OpenStreet map.NYS Health Profile website might lacks the latest information regarding hospital information. It could lack information regarding new hospitals. Also, hospital ids were extracted manually from NYS, which could have missing hospitals. We also dropped neighborhoods which did not have any hospital data matching in NYS Health Profile website. For this project, we are only using data from 74 hospitals in NYC.We are using fuzzywuzzy to match hospital data from Foursquare and NYS Health Profile. It is not a correct measure because we are matching the names nearest possible, it could be wrong in real life scenario.We are also only considering hospital data. We did not consider other medical facilities like nursing home or health clinic.We used population data from 2010(as per Wikipedia pages), which are not accurate currently. We should have used the latest population data.Finally, to battle COVID-19, we should have had patient data for the neighborhood. Unfortunately, we could not find it like this(for example, get patient per latitude longitude) from any source, hence could not incorporate it.Conclusion To conclude, the basic data analysis was performed to identify the most well equipped hospital in the NYC neighborhoods. During the analysis, several important statistical features of the boroughs/neighborhoods were explored and visualized. Furthermore, clustering helped to highlight the group of optimal areas. Finally, Manhattan-Murray Hill was chosen as the most well equipped(as per hospital bed count and icu bed count) area to battle pandemic.","date":"March 30, 2020","link":"/posts/project-battle-of-capstones/","readTime":9,"title":"Capstone Project: Find Best Neighborhood to Fight Pandemic in NYC"},{"content":"If you are using Hugo to generate static pages, you are familiar with CLI commands which are to build the static pages in your local machine and make push to your \u0026lt;username\u0026gt;.github.io repository. When it comes to using Hugo for blogging, compared to platforms like Medium or WordPress, it is very painful because you do not have any web interface to make changes whenever you want or wherever you want.But thanks to GitHub Actions, you can almost overcome this problem, just you won\u0026rsquo;t have an admin panel like other platforms, but use GitHub\u0026rsquo;s UI to make changes. In this article, we will see how you can do that and without using any CLI, deploy your static pages!! Table of contents    What is GitHub Actions Steps to deploy Hugo with GitHub actions  Step 1: Setting up repo named \u0026lt;username\u0026gt;.github.io in GitHub Step 2: Install and create a Hugo project Step 3: Push Hugo site code in GitHub Step 4: Create GitHub token Step 5: Add token as secret in GitHub Step 6: Create A GitHub Action Step 7: Push to GitHub Step 8: Never use CLI again   In conclusion   What is GitHub Actions GitHub Actions is a CI/CD tool to automate test, build and deployment process in GitHub. It is kind of equivalent to GitLab CI/CD or BitBucket Pipelines. GitHub actions will make it easy to automate all your software workflows.Steps to deploy Hugo with GitHub actions We are going to deploy our site in GitHub Static Pages for this article. Let us go step by step from setting up Hugo to create actions for deployment. Also, in the first few steps you need to use CLI from your local machine, but just bear with me and you will be rewarded handsomely at the end . FYI: If you already have a Hugo based setup in GitHub, you can skip the first three steps. Step 1: Setting up repo named \u0026lt;username\u0026gt;.github.io in GitHub You need to create a repository named \u0026lt;your GitHub username\u0026gt;.github.io. Contents on that page will be accessible via url same name as the repo.Step 2: Install and create a Hugo project You need to install Hugo in your local machine and use that to create a site. Please take a look at the official documentation on how to do that.Step 3: Push Hugo site code in GitHub Now create a new GitHub repo for that Hugo site, and push to master branch. Just follow this steps:git init git remote add origin git@github.com:\u0026lt;username\u0026gt;/\u0026lt;repo-name\u0026gt; git add --all git commit -m \u0026#34;Commit MSG\u0026#34; git push origin master If you have added a theme, then consider adding it as submodule(also stated in the documentation):git submodule add https://github.com/budparr/gohugo-theme-ananke.git themes/ananke Better if you update the submodule to get latest theme changes before pushing the code:git submodule update --init --recursive git push origin master Or you can always update the latest theme in the actions. Example has been shared in create github action section.Step 4: Create GitHub token Now you need to generate a GitHub token with repo access from the tokens page. You will get a 40 character long token by generating the token. Store it somewhere securely. Step 5: Add token as secret in GitHub The token last step, you can store it in the Secrets setting of the repo. It can be accessible from https://github.com/\u0026lt;username\u0026gt;/\u0026lt;repo-name\u0026gt;/settings/secrets. Store it like this: Step 6: Create A GitHub Action Now it is time to do the fun stuff. Let us create an action in .github/workflows/ folder inside the repo(hugo site repo) and name it main.yml.name:CIon:pushjobs:deploy:runs-on:ubuntu-18.04steps:- name:Gitcheckoutuses:actions/checkout@v2- name:Updatetheme# (Optional)If you have the theme added as submodule, you can pull it and use the most updated versionrun:gitsubmoduleupdate--init--recursive- name:Setuphugouses:peaceiris/actions-hugo@v2with:hugo-version:\u0026#34;0.64.0\u0026#34;- name:Build# remove --minify tag if you do not need it# docs: https://gohugo.io/hugo-pipes/minification/run:hugo--minify- name:Deployuses:peaceiris/actions-gh-pages@v3with:personal_token:${{secrets.TOKEN}}external_repository:\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.iopublish_dir:./public# keep_files: trueuser_name:\u0026lt;username\u0026gt; user_email: \u0026lt;username@email.com\u0026gt;publish_branch:master# cname: example.comAs mentioned from main.yml file, it is named CI and this is going to be triggered when something is pushed to the repo. It will be using an ubuntu-18.04 based VPS to run the pipeline. Now let us go through steps to understand how it works:  In the Git checkout step, we are going to fetch the latest code of our repository which contains Hugo site.  In the Setup hugo step, we are going to use peaceiris/actions-hugo to install Hugo. You need to specify which hugo version you want to use. I would recommend using hugo version of your local machine(command: hugo version).  In the Build step, we are going to build the static contents using hugo --minify command. By using --minify, we are going to minify the assets in the site. For more information, checkout the documentation.  Finally, the Deploy step. Now we are going to deploy the static contents from the last step. And we are going to use peaceiris/actions-gh-pages actions to run the deployment. Here, we used external_repository: \u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io because otherwise the static contents would be pushed in the same repo(in a different branch). As we specified the external repository, the static contents will be pushed to \u0026lt;username\u0026gt;.github.io. For this step, we will use the personal token which we specified in Step 5. If you uncomment keep_files: true, then the deployment will keep old files from \u0026lt;username\u0026gt;.github.io, otherwise it will replace everything. Finally, if you have a custom domain, then configuring cname is necessary. For more information, please check documentation in GitHub marketplace.  Step 7: Push to GitHub Now push to your Hugo site repository and voila, your action will start automatically. You can check its progress in the actions tab. After each successful run, it will push the static contents to your static page repository.Step 8: Never use CLI again Now you have everything in GitHub, so you do not need to come back to your local machine and use CLI to push changes. Use the web interface, and GitHub actions will take care of the rest.In conclusion Although you need to create a Hugo site in your local machine and push it manually to GitHub at least for the first time and consequent changes can be done from the web(but you can do that from your local machine as well). Just create a markdown file in repo and boom! It is on the internet.Thank you for reading. If you have any questions or a better solution, let us talk in the comment section below. Cheers!!","date":"March 6, 2020","link":"/posts/hugo-deploy-static-page-using-github-actions/","readTime":5,"title":"Hugo: Deploy Static Site using GitHub Actions"},{"content":" DISCLAIMER: This is not the recommended process. Ideally you should have the database running in a separate container and use network to interact with that container. Usually, we dont ship application with the database inside the same container. But sometimes we are forced to do that. In this article we are going to see a minimalist way of shipping application with database built-in inside docker container. Table of contents    Overview of the implementation Codes for PostgreSQL based image  Write Dockerfile for PostgreSQL Write entrypoint and application related scripts for PostgreSQL Build and run docker with PostgreSQL   Codes for MySQL based image  Write Dockerfile for MySQL Write entrypoint and application related scripts for MySQL Build and run docker with MySQL   Working examples in github In conclusion   Overview of the implementation We will inherit the database image(ie official PostgreSQL image or MySQL image etc) as base and build it from there. We will write our own entrypoint script, and call the parent image\u0026rsquo;s entrypoint script inside our entrypoint with arguments we receive from CMD directive. With that, we will run additional commands which are required for running our application. Key points to remember here are: Override the base image\u0026rsquo;s entrypoint script inside our entrypoint script. Keep the CMD of the child(our Dockerfile) same as the parent. For example: CMD [\u0026quot;postgres\u0026quot;] for PostgreSQL, CMD [\u0026quot;mysqld\u0026quot;] for MySQL. In our entrypoint, we will add additional commands and scripts for running our application.  Codes for PostgreSQL based image Write Dockerfile for PostgreSQL First let us write a Dockerfile based on postgres:12-alpine:FROMpostgres:12-alpineENV POSTGRES_USERNAME \u0026#39;postgres\u0026#39;ENV POSTGRES_PASSWORD \u0026#39;postgres\u0026#39;ENV POSTGRES_DB \u0026#39;postgres\u0026#39;# Install your dependencies hereWORKDIR/srcADD ./src /srcCOPY my-entry-point.sh /RUN chmod +x /my-entry-point.sh# not necessary if you are planning to run commands without a bash fileCOPY application-related-commands.sh /RUN chmod +x /application-related-commands.sh# entrypointENTRYPOINT [\u0026#34;/my-entry-point.sh\u0026#34;]CMD [\u0026#34;postgres\u0026#34;]# expose port necessary for your applicationEXPOSE8000Write entrypoint and application related scripts for PostgreSQL Now, write necessary commands inside application-related-commands.sh to run our app and put it in the same directory as Dockerfile. Finally add your my-entry-point.sh in the same directory as Dockerfile like this:#!/bin/sh /usr/local/bin/docker-entrypoint.sh \u0026#34;$@\u0026#34; \u0026amp; /application-related-commands.sh Instead of /application-related-commands, you can add your custom commands without needing any files.Here we are running the parent image\u0026rsquo;s(PostgreSQL) entry point script by /usr/local/bin/docker-entrypoint.sh because if you check the source, you will see that the entry point script is being stored in /usr/local/bin/ folder.Build and run docker with PostgreSQL Use this command to build:docker build . -t psql-based-image And run with:docker run -p 8000:8000 -v /your/path:/var/lib/postgresql/data/ -t psql-based-image Codes for MySQL based image Write Dockerfile for MySQL This is similar to the above implementation, apart from the CMD directive. Let us write a Dockerfile based on mysql:8:FROMmysql:8ENV MYSQL_ROOT_PASSWORD \u0026#39;mypassword\u0026#39;# Install your dependencies hereWORKDIR/srcADD ./*.jar /src/runable.jarCOPY my-entry-point.sh /RUN chmod +x /my-entry-point.sh# not necessary if you are planning to run commands without a bash fileCOPY application-related-commands.sh /RUN chmod +x /application-related-commands.sh# entrypointENTRYPOINT [\u0026#34;/my-entry-point.sh\u0026#34;]CMD [\u0026#34;mysqld\u0026#34;]# expose port necessary for your applicationEXPOSE7654Write entrypoint and application related scripts for MySQL Implementation of the rest of the stuff(my-entry-point.sh and application-related-commands.sh) are same as PostgreSQL image based implementation.Build and run docker with MySQL Use this command to build:docker build . -t mysql-based-image And run with:docker run -p 7654:7654 -t mysql-based-image Working examples in github In this article, we have seen an overview on how we might be able to ship applications with database. For working examples, please checkout this repository on github. It has examples of: A Python Application with MySQL in Docker A Python Application with PostgreSQL in Docker A Django Project with MySQL in Docker A Django Project with PostgreSQL in Docker A Java App with MySQL in Docker A Java App with PostgreSQL in Docker  In conclusion I still think this solution is far from perfect, maybe you can suggest better ideas. Let us discuss more in the comment section below.","date":"February 28, 2020","link":"/posts/docker-ship-database-with-container/","readTime":3,"title":"Ship Application with Database Inside Docker Container"},{"content":"Maintaining authentication layer in ReactJS can be painful. Back in the old days(even now-a-days), you needed to override every componentDidMount method in class based components to see if the user is authenticated. In the latest ReactJS, there are hooks(specially useEffect) where you need to write these checks. Either way, some codes will be redundant.No worries, with the help of redux and react router, you can overcome this problem easily using redux middleware. In this article, I am going to share how you can achieve this!! Table of contents    Prerequisite Connect router to redux Create a middleware Add middleware to redux store Create path to authentication map Use \u0026lsquo;pathToAuthMap\u0026rsquo; to validate path Use \u0026lsquo;checkAuthRequired\u0026rsquo; in middleware Advantages of this approach Disadvantages of this approach In conclusion   Prerequisite You need to use react router and redux, otherwise it won\u0026rsquo;t work. So probably it is better to avoid this approach in smaller projects where you are not using redux or react router, and it will over-complicate the project.Connect router to redux First, you need to connect react router and redux. To connect them, you need to use connected react router. Just follow their steps to install and configure the package.Create a middleware Once you have connected the router to redux, now it is time to listen to Route Changes. We are going to use a middleware to listen to it. Let us create one:// authentication middleware export const authentication = (store) =\u0026gt; (next) =\u0026gt; (action) =\u0026gt; { if (action.type === \u0026#34;@@router/LOCATION_CHANGE\u0026#34;) { const path = action.payload.location.pathname; console.log(path); } return next(action); }; As you can see here, we are looking for an action type named @@router/LOCATION_CHANGE. This action is fired when a route is changed. From it\u0026rsquo;s payload, we can grab the path name as well. Later we are going to use that path for certain functions and actions.Add middleware to redux store Now, we need to add that middleware to redux store like this:import { applyMiddleware, createStore, compose } from \u0026#34;redux\u0026#34;; import authentication from \u0026#34;./authenticationMiddleware\u0026#34;; // some code  export const history = createBrowserHistory({ basename: \u0026#34;baseName\u0026#34;, }); const router = routerMiddleware(history); const middlewareEnhancer = applyMiddleware(thunkMiddleware, authentication); const composedEnhancers = compose(middlewareEnhancer, otherMiddlewareEnhancers); export default function configureStore(preloadedState) { return createStore( createRootReducer(history), preloadedState, composedEnhancers ); } Create path to authentication map We have the path information available in middleware. So we are going to use that information to restrict which path can be accessed with/without authentication. Let us do that://authenticationHelper.js const pathToAuthMap = [ { url: \u0026#34;/\u0026#34;, isRegex: false, requireAuth: true, }, { url: \u0026#34;/login\u0026#34;, isRegex: false, requireAuth: false, }, { url: /item\\/\\d+/, isRegex: true, requireAuth: true, }, ]; Use \u0026lsquo;pathToAuthMap\u0026rsquo; to validate path In the last step, we described which path requires authentication, and which don\u0026rsquo;t. Now, we are going to write a function to evaluate if a certain path requires authentication.//authenticationHelper.js const checkAuthRequired = (path) =\u0026gt; { for (const element of pathToAuthMap) { if (element.isRegex) { if (element.url.test(path)) { return element.requireAuth; } } else if (element.url == path) { return element.requireAuth; } } return false; // default value }; Use \u0026lsquo;checkAuthRequired\u0026rsquo; in middleware Finally, we have everything in our hands, now it is time to connect them together. Means, we are going to use our checkAuthRequired method in middleware.// authentication middleware import { push } from \u0026#34;connected-react-router\u0026#34;; import { checkAuthRequired } from \u0026#34;./authenticationHelper\u0026#34;; export const authentication = (store) =\u0026gt; (next) =\u0026gt; (action) =\u0026gt; { if (action.type === \u0026#34;@@router/LOCATION_CHANGE\u0026#34;) { const path = action.payload.location.pathname; if (checkAuthRequired(path) \u0026amp;\u0026amp; !store.getState().isAuthenticated) { return store.dispatch(push(\u0026#34;/login\u0026#34;)); } } return next(action); }; Well, the code above is kind of self explanatory. It is checking if a path requires authentication or not, and if yes and user is not authenticated(we are getting that value from a reducer isAuthenticated, assuming we have that reducer from which we can identify if a user is authenticated or not), then we redirect the user to /login page. We are using push function from connected react router.And that is it, we have a authentication middleware.Advantages of this approach  Cleaner code and keeps the rest of the components simple. Easily maintainable as all the logics for authentication in the same place. No redundant code in components. More reusable components.  Disadvantages of this approach  Over engineering. Performance issue might occur as we are checking each and every route change.  In conclusion Redundancy or over engineering, this topic is up for debate. But I feel this approach offers more pros than cons. I have not tested this code against server side rendering, but works fine for a create-react-app based solution.Thank you for reading, if you need any help or have questions, please use the comment section below.Cheers!!","date":"February 14, 2020","link":"/posts/reactjs-maintain-auth-layer-from-redux-middleware-router/","readTime":4,"title":"Maintain Authentication Layer from Redux Middleware Using React Router"},{"content":"AWS CodeBuild is an extraordinary tool for building your code. Recently I have been using it to build projects and store docker images in AWS ECR. During those CodeBuild processes, I needed Database for running tests. As the need of the Database was for a limited time and I didn\u0026rsquo;t want to pay extra for it(that is why I did not use AWS RDS), I decided to create database using docker inside CodeBuild. In this post, I am going to describe how I did it. Table of contents    Create \u0026lsquo;buildspec.yml\u0026rsquo; file Configure \u0026lsquo;CodeBuild\u0026rsquo; Start build In conclusion   Create \u0026lsquo;buildspec.yml\u0026rsquo; file You need to create a buildspec.yml file in the root of the project. There you need to define instruction on how to pull docker image of the database you want to run. Here is an example:version:0.2env:variables:PGDATABASE:\u0026#34;XXXXX\u0026#34;PGUSER:\u0026#34;XXXXX\u0026#34;PGPASSWORD:\u0026#34;XXXXX\u0026#34;phases:install:runtime-versions:python:3.8commands:- dockerpullpostgres:latestpre_build:commands:- echoRunningpostgres- dockerrun-ePGPASSWORD-ePGUSER-ePGDATABASE-d-p5432:5432postgresbuild:commands:- echoBuildstartedon`date`- echoInstallingdependencies- pipinstall-rrequirements.txt- echoRunningtests- pythonmanage.pytestpost_build:commands:- echoBuildcompletedon`date`reports:myReport:files:- \u0026#34;**/*\u0026#34;cache:paths:- \u0026#34;/root/.cache/pip/*\u0026#34;As you can see in the install section of buildspec.yml, I pulled the Postgres docker image. Then in pre_build section, I ran the image with environment variables which were defined in env section. After that, I ran tests in django.Configure \u0026lsquo;CodeBuild\u0026rsquo; Now its time to configure the CodeBuild in AWS console. I think the configuration is quite straightforward. But you need to make sure to tick mark the privileged section of the settings. It should look like this: Start build After configuring CodeBuild, now it is time to start the build process from the dashboard. It can be either triggered from git(if you configured it like this), or you need to start the build manually. That is it, inside CodeBuild processes, it will start a Postgres Database, and allow django to run migrations.In conclusion Spinning up a docker container for database, seems to me like a very efficient solution. Similarly you can spin up any docker container to use as a temporary service, for example run redis, memcache etc.","date":"January 31, 2020","link":"/posts/aws-codebuild-use-database/","readTime":2,"title":"Use Docker for Accessing Database in AWS CodeBuild"},{"content":"We already know that Docker is used for containerization. But networking in docker makes it even better tool for deployment. You can make standalone deployments in containers and communicate between them, or make containers act as host, or even assign mac addresses to containers and act them as devices. It does not matter which host machine you are in, you can make dockers communicate in-between them. In this article, we are going to discuss about bridge networking in docker, and in the end will see on how to communicate from container to host or vice versa. Table of contents    Communication between containers How it works Create bridge network Connect containers to network Disconnect containers from network Examples on how to communicate between containers Example with \u0026lsquo;Docker Compose\u0026rsquo; Communication between host and container  From container to host From host to container   In conclusion   Communication between containers If you have several standalone applications in different containers and need to communicate between them, then you can use bridge as network driver in docker. Its the default driver, it will be assigned if you don\u0026rsquo;t specify network driver type. It is used when you deploy standalone applications in different containers and they need communication.How it works The documentation states: In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. Means, the containers connected to bridge network can communicate with each other, but can\u0026rsquo;t communicate which are outside of that network. It won\u0026rsquo;t be exposed unless you explicitly do that.Create bridge network You can create a bridge network by:docker network create skynet Or remove it:docker network rm skynet You can also inspect the network using:docker network inspect skynet Connect containers to network After creating the network, now its time to add your containers to that network like this:docker create --name nz01 \\  --network skynet \\  --publish 8080:80 \\  nginx:latest Or you can add a running container to network:docker network connect skynet nz01 Disconnect containers from network You can use the following command to do that:docker network disconnect skynet nz01 Examples on how to communicate between containers It is pretty simple, you need to use the name of a container and port it is exposing. For example, if you want to access nginx container(named nz01 in previous example) from another container, then try like this:wget nz01:80 Keep in mind that the port number is the port where the application is running inside the container, not the port it has exposed. In docker, port mappings are writtern in \u0026lt;EXPOSED PORT\u0026gt;:\u0026lt;INTERNAL PORT\u0026gt;, in this example 8080 is accessable from outside world, where nginx is running in port 80 inside the container.Here is another example, lets say an web application is running in container web at port 8000 and from nginx you want to expose it. Then the nginx configuration should look like this:upstream web { ip_hash; server web:8000; // here web is the name of the container or service if you are using docker compose and application is running at port 8000 } server { location / { proxy_pass http://web/; } listen 8000; server_name localhost; } Example with \u0026lsquo;Docker Compose\u0026rsquo; Docker Compose provides a better way of writing networks. Here is an example:version:\u0026#34;3\u0026#34;services:nginx:image:nginx:alpinecontainer_name:nz01ports:- \u0026#34;8000:8000\u0026#34;volumes:- ./config/nginx:/etc/nginx/conf.ddepends_on:- webnetworks:- skynetweb:build:context:.dockerfile:compose/django/Dockerfilecontainer_name:dz01volumes:- ./src:/srcexpose:- \u0026#34;8000\u0026#34;networks:- skynetnetworks:skynet:driver:bridgeHere I just provided a networks section in each of the services defining which network to connect. And inside networks section at the bottom, I have defined which driver to use for communication. We could have defined multiple networks if needed.Communication between host and container From container to host Communication from container to host is pretty straight forward. You just need to expose the container in certain port and from host machine access it through localhost:port or 127.0.0.1:8000. For example:\u0026gt; docker run -p 8000:80 -it web \u0026gt; wget localhost:8000 From host to container This communication has been made simple for MacOs and Windows, but not in linux(at the moment of writing). All you need to do is use host.docker.internal. For example, if you want to use postgres database in django from host machine:DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.postgresql_psycopg2\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;PASSWORD\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;host.docker.internal\u0026#39;, \u0026#39;PORT\u0026#39;: 5432, } } But for linux machine, you can add the following lines in docker-entrypoint.sh(copy pasted from this article):HOST_DOMAIN=\u0026#34;host.docker.internal\u0026#34; ping -q -c1 $HOST_DOMAIN \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 if [ $? -ne 0 ]; then HOST_IP=$(ip route | awk \u0026#39;NR==1 {print $3}\u0026#39;) echo -e \u0026#34;$HOST_IP\\t$HOST_DOMAIN\u0026#34; \u0026gt;\u0026gt; /etc/hosts fi (main entrypoint) What its doing is that if host.docker.internal does not work when pining, it will add the host.docker.internal as alias of HOST IP and add it in /etc/hosts.In conclusion Although in this article we focused on bridge networking, but there or other types of networking which can be used based on situations. Networking in docker is a breeze compared to doing it manually, and making it a very powerful means of deployment and distribution(along with its awesome platform agnostic services). Thank you for reading, if you have any opinion regarding this article, please share in the comment section below.","date":"January 15, 2020","link":"/posts/networking-with-docker/","readTime":5,"title":"Basic Networking in Docker"},{"content":"In this article, I am going to share how you can set up a newsletter subscription using Staticman. No need to use third party newsletter subscription services.Collecting data in static sites or serverless sites(with POST request) is a hassle because you don\u0026rsquo;t have a server to receive POST requests and store data. But Staticman is here to help. It will enable users to make POST requests, that will allow you to gather these data. In this article, I am going to tweak Staticman a little to implement newsletter subscription.FYI: _I am not going to share how to make a commenting system using Staticman because there are countless examples available on the internet, even from Eduardo Bouas, creator of Staticman._ Table of contents    How \u0026lsquo;Staticman\u0026rsquo; works How are we going to tweak \u0026lsquo;Staticman\u0026rsquo; Steps for setting up newsletter subscription  Host your \u0026lsquo;Staticman\u0026rsquo; server Create a staticman.yml in your public repository Setup the form Push changes After mailing list is created   In conclusion Useful links   How \u0026lsquo;Staticman\u0026rsquo; works Before starting anything, you need to understand how Staticman works. First, you have a form, when you submit the form(with any kind of data), it will make a POST request to Staticman server, then the data is sent your git repository. Checkout the following image for understanding it better: How are we going to tweak \u0026lsquo;Staticman\u0026rsquo; What Staticman does when notification is allowed, that it creates a mailing list in mailgun. For each data entry, they create a mailing list/add emails to an existing mailing list. Similarly for newsletter, we need a mailing list. So, we are going to use it to collect email addresses to create a mailing list for newsletter.Steps for setting up newsletter subscription Host your \u0026lsquo;Staticman\u0026rsquo; server Default Staticman server is already being over used by many people, so it does not work properly. So, its better to host your own. Checkout this article to install Staticman in Heroku server.Create a staticman.yml in your public repository You need to create a public git repo, there you need to create a staticman.yml file with following(which has bare minimum settings):comments:allowedFields:[\u0026#34;email\u0026#34;,\u0026#34;other_fields_if_necessary\u0026#34;]moderation:truename:\u0026#34;newsletter-subscription\u0026#34;requiredFields:[\u0026#34;email\u0026#34;]notifications:enabled:trueapiKey:\u0026#34;XXXXXX\u0026#34;domain:\u0026#34;XXXXXX\u0026#34;Important thing to remember here is that, you need to allow email field and also add email in requiredFields. You can add additional fields in allowedField(like if you already have a commenting system them fields like name, comment etc are necessary).Most crucial part, you must configure mailgun to generate API Key, then put the value in notifications. Value of API Key and Domain must be base64 encoded.You can enable moderation to keep the code clean, so when someone submits a newsletter form, it will not be stored in git immediately, instead you can close the Pull Request(because the email is already stored in mailing list).Setup the form For newsletter subscription, you need to render a form. For that, you can use the following:\u0026lt;h1\u0026gt;Newsletter Subscription\u0026lt;/h1\u0026gt; \u0026lt;form id=\u0026#34;comment-form\u0026#34; method=\u0026#34;post\u0026#34; action=\u0026#34;https://your.Staticman.server\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;options[redirect]\u0026#34; value=\u0026#34;https://your-domain.com/submitted\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;options[parent]\u0026#34; value=\u0026#34;a-random-name-which-will-consistant-in-all-pages\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;options[slug]\u0026#34; value=\u0026#34;a-random-name-which-will-consistant-in-all-pages\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;options[origin]\u0026#34; value=\u0026#34;https://your-domain.net/\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;options[redirectError]\u0026#34; value=\u0026#34;https://your-doamin.net/error\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;email\u0026#34; name=\u0026#34;fields[email]\u0026#34; placeholder=\u0026#34;Enter your email\u0026#34; required /\u0026gt; \u0026lt;input name=\u0026#34;options[subscribe]\u0026#34; type=\u0026#34;checkbox\u0026#34; value=\u0026#34;email\u0026#34; required /\u0026gt; I want to receive emails \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Subscribe\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; In the form, for input field you need to make sure you a consistant value for slug and parent in every page where you put this form. Because based on the value of slug a mailing list will be created.Push changes All is done. Now its time for testing. Upload the code to your git repository or serverless hosting(like Netlify) and see how it goes. There for each form submission, a new email will be added to a specific mailing list in mailgun.After mailing list is created Now its up to you, you need to create templates and send bulk emails to that mailing list.In conclusion Although Staticman is for collecting any kind of data, most of the people use it for a decent commenting system. My idea here was to use Staticman for both commenting and newsletter subscription. In this Article, I wrote about Newsletters(by the way they are GDPR complient), maybe you will find more usage of Staticman.Useful links  https://www.gabescode.com/staticman/2019/01/04/staticman-comments-for-jekyll.html https://networkhobo.com/hugo-staticman-nested-replies-and-email-notifications/ https://yasoob.me/posts/running_staticman_on_static_hugo_blog_with_nested_comments/ https://yasoob.me/posts/staticman_comment_notifications_mailgun/ https://binarymist.io/blog/2018/02/24/hugo-with-staticman-commenting-and-subscriptions/  ","date":"December 12, 2019","link":"/posts/implement-newsletters-using-staticman/","readTime":4,"title":"Implement Newsletter Subscription Using Staticman"},{"content":"Have you worked with Docker before? Do you think you have trouble with it, like when you are developing an application, you need to build every time to see the results, or thinking of the best way to load data in DB, or may be you are annoyed because there are lots of markdown files, screenshots etc that are taking a lot of space in your container. Here are some tips about VOLUME, ENTRYPOINT, and .dockerignore file to fix these problems. Table of contents    Use VOLUME wisely  Usage of volume   Do heavy lifting by ENTRYPOINT  What is ENTRYPOINT Usage of entry point   Exclude unnecessary files using .dockerignore In conclusion Relevent posts References   Use VOLUME wisely VOLUME is a fantastic feature where you can share a space from your local machine to container. But there are other usages to that as well. For example:  You can put your source code in Docker Container through Volume. Then whenever you change in your source code, it will be immediately reflected inside Docker Container. FYI: this is not recommended for production. In production, you should use COPY to copy the source code in docker file.  VOLUME directory is a place where you can change without any permission. I mean, normally after an Image is built, you can\u0026rsquo;t change inside Container apart from temp directory. If you need to do that, then you need to give permission to the docker container user to allow write. Instead, use volume to do the necessary changes, like installing packages. Even you can install package in local machine and it will automatically synced inside docker container.  Usage of volume If you want to run it in traditional way, then use a -v tag:docker run -v /db-data /var/lib/mysql/data -it mysql Or if you are using Docker Compose, then use the following:version:\u0026#34;3.7\u0026#34;mysql:image:mysqlvolumes:- db-data:/var/lib/mysql/dataDo heavy lifting by ENTRYPOINT If you want to load data in your docker container or execute scripts before running the actual command, it is better to do them in ENTRYPOINT.What is ENTRYPOINT ENTRYPOINT is the binary which is executing. You can define it in Dockerfile or you can provide like this docker run --entrypoint=\u0026quot;/bin/bash/\u0026quot; .... CMD is the default argument to container. Without the ENTRYPOINT, default argument is command that is executed. With ENTRYPOINT, CMD is passed to entrypoint as argument. You can emulate a command with entrypoint.$ cat Dockerfile FROM python:3-alpine ... ENTRYPOINT [\u0026#34;/bin/sh\u0026#34;] $ docker run -it your_docker_image your_cmd Usage of entry point For that lets write a docker-entrypoint.sh file:#!/bin/bash bash -e python manage.py makemigrations python manage.py migrate python manage.py collectstatic --no-input exec \u0026#34;$@\u0026#34; Then use it in Dockerfile:FROMpython:3-alpineCOPY docker-entrypoint.sh /usr/local/bin/RUN ln -s /usr/local/bin/docker-entrypoint.sh / # backwards compatibleENTRYPOINT [\u0026#34;docker-entrypoint.sh\u0026#34;]CMD [\u0026#34;python\u0026#34;,\u0026#34;manage.py\u0026#34;, \u0026#34;runserver\u0026#34;]Exclude unnecessary files using .dockerignore You can ignore unnecessary files like readme files, version controlling, example codes etc when building the image, using .dockerignore file. Here is an example:.git .ipynb_checkpoints .DS_Store .gitignore README.md env.* # To prevent storing dev/temporary container data *.csv tmp Also, be sure to read .dockerignore file whenever you are using a repository(if there is one with .dockerignore). Because it might ignore necessary files in Docker which are needed by you.In conclusion VOLUME, ENTRYPOINT and .dockerignore, they are all powerful features of Docker. They can make your life a breeze when you know how to use them. But If you want to master them, it is better to try them yourself.Relevent posts  Tips on Writing Dockerfile, Reduce Size and Build Time of an Image Tips on using Docker Compose, Build Argument and Environment Variable  References  Official Documentation on Builder: https://docs.docker.com/engine/reference/builder/ Official Documentation on Volume: https://docs.docker.com/storage/volumes/  ","date":"November 9, 2019","link":"/posts/docker-volume-entrypoint-ignorefile/","readTime":3,"title":"Tips on Using Volume, Entrypoint and Ignore Files in Docker"},{"content":"Imagine, you have some microservices and you want to use API from one service to another, how can you do that if all of them are running in different docker container? Docker Compose has a solution for you.Suppose you want to deploy a project in test, stage and production environment with same Dockerfile for all of them through a single Dockerfile. You can do that through Build Argument and Environment Variable. Table of contents    Supercharge your containers using docker-compose  Example Use multiple compose file in same project Share services from one container to another Docker compose shell and logs   Build arguments or ARG  Basic usages of ARG   Envionment variable or ENV  Basic usages of ENV Set default value for environment variables Loading environment variables from file Using .env in docker compose Use host machine environment variables   ARG vs ENV In conclusion Relevent posts References   Supercharge your containers using docker-compose docker-compose is a great tool for orchestrating multiple docker instances. But it is not like kubernetes or docker swarm. Rather, its a simple application which allows you to communicate between docker containers, allow you to easily configure things, even help you generate service, route, build config etc for kubernetes if you use kompose.Example Here is an example of docker-compose.yml:version:3services:web:build:.container_name:dz01db:image:postgres:alpinecontainer_name:pz01Use multiple compose file in same project If you want to have different compose files for building the same project(like one for test server, another for production server), then just write two or more .yml files and when building one of them or run them just use -f argument. Like this:docker-compose -f your_file_name.yml build Share services from one container to another Let us say you have a database in one container and a server in another container, how can you use that database from that server? Rather than doing the communication manually, I would recommend using docker compose. For example:version:\u0026#34;2\u0026#34;services:web:build:.container_name:dz01depends_on:- dbvolumes:- ./src:/srcexpose:- \u0026#34;8000\u0026#34;db:image:postgres:alpinecontainer_name:pz01port:- \u0026#34;5432:5432\u0026#34;In here, database service is defined as db. From web container, you can simply access the database using:psql -h db -p 5432 Even wget db:5432 will work, because the word db will be resolved automatically through docker compose.Docker compose shell and logs Access docker compose shell by:docker exec -ti container_name /bin/sh Access docker compose log by:docker-compose logs service_name Build arguments or ARG Build arguments or ARG is value which is available during build. It is useful for many cases:. Set value of environment variable during build.. Use same Dockerfile in multiple projects. For example, you have a django Docker Image, and you can use it in similar projects. Just you need to know project root and where the requirements file is. Example code:ARG PROJECT_ROOT ARG REQUIREMENTS_DIR ENV SRC_DIR PROJECT_ROOT # Setting the environment variable from Build Argument ADD SRC_DIR /app WORKDIR /app RUN pip install -r \u0026#34;${REQUIREMENTS_DIR}\u0026#34; # Installing Requirements from ARGS CMD [\u0026#34;python\u0026#34;, \u0026#34;manage.py\u0026#34;, \u0026#34;runserver\u0026#34;] Basic usages of ARG Pass build arguments when building the image like this:docker build --build-arg PROJECT_ROOT=./src REQUIREMENTS_DIR=./requirements/local.txt Or if you use docker-compose.yml then update file like this:version:\u0026#34;3\u0026#34;services:django:build:context:./appdockerfile:Dockerfileargs:PROJECT_ROOT:./srcREQUIREMENTS_DIR:./requirements/local.pipEnvionment variable or ENV Using ENV will create environment variables inside Docker container. You can also send them from outside when building the Dockerfile(through Build Arguments) or running the image.Basic usages of ENV If you are running the Image using docker command in terminal or command line, then use argument -e.docker run -e \u0026#34;SRC_DIR=./src\u0026#34; django Or you can declare them in docker-compose.yml file:version:\u0026#34;3\u0026#34;services:django:environment:- \u0026#34;SRC_DIR=./src\u0026#34;Set default value for environment variables You can simply do that by:# With default value ENV ABC ./src # Without Default value ENV XYZ And you can override the value of ABC here by passing the environment from outside of container, ie docker run -e ABC=./abc ....Loading environment variables from file You can store your environment variables in a file and load them in docker-compose.yml via env_file:django:env_file:- web-variables.envWhich is similar to what you can use docker run --env-file=FILE ... for load the environment file when running a docker image.Using .env in docker compose You can put environment variables inside .env file and it will be picked up automatically from docker compose. This pretty cool feature as there are some interesting usages within the docker compose file itself.$ cat .env PORT=8088 $ cat docker-compose.yml version: \u0026#39;3\u0026#39; services: django: build: context: . dockerfile: ./compose/django/Dockerfile image: django ports: - \u0026#34;${PORT}:8000\u0026#34; So when you run docker-compose up then the variable ${PORT} will be replaced by 8088.Use host machine environment variables It will work almost same as above example. Just define the value of PORT in your host machine, then it will automatically be picked up when you run docker-compose up$ export PORT=8088 $ cat docker-compose.yml version: \u0026#39;3\u0026#39; services: django: build: context: . dockerfile: ./compose/django/Dockerfile image: django ports: - \u0026#34;${PORT}:8000\u0026#34; ARG vs ENV As explained above, ARG is only build time variables. Means it will be available when you are building the Image from a Dockerfile. On the other hand, ENV can be accessed when you are running the Image. But you can still use the environment variable when building the Image. Then you have to use either ARG to update the ENV value or use a default value.In conclusion In this post, I tried to explain usages of docker compose, build arguments and environment variables in as detail as possible. But its better to use them in your own project to understand these better. Also checkout the reference links given below which has more detailed explanation.Relevent posts  Tips on Writing Dockerfile, Reduce Size and Build Time of an Image Tips on Using Volume, Entrypoint and .dockerignore file  References  https://vsupalov.com/docker-arg-env-variable-guide/ Official Documentation on Environment Variable: https://docs.docker.com/compose/environment-variables/ Official Documentation Docker Compose: https://docs.docker.com/compose/  ","date":"November 9, 2019","link":"/posts/docker-compose-arg-env/","readTime":5,"title":"Tips on Using Docker Compose, Build Argument and Environment Variable"},{"content":"For the past couple of years, I have been creating Dockerfile for different projects. based on those experiences I am going to share some tips on writing docker files to communication between containers._FYI: before reading this article, please read the article from official docker blog site to learn about best practices for creating Dockerfile._ Table of contents    Look for official docker images Reduce the size of docker image  1. Use Alpine based images 2. Delete packages after dependent applications are installed 3. Remove cache 4. Reduce layers in Dockerfile 5. No need to install debug tools   Reduce build time by caching layers In conclusion Relevent posts References   Look for official docker images When you are trying to run a project in Docker, its better to use official images rather than writing your own. If you need to install different packages that does not come with the official image, then you can extend the official images. For example, if you want to install Flask in your Docker Container, instead of creating an Image from Ubuntu or Alpine Linux, use Python\u0026rsquo;s official image:FROMpython:3.7-alpineRUN pip install flaskReduce the size of docker image Reducing size of a docker image is very important. Because before you know it, your image might take 1-3 GB of space. There are different ways you can reduce it. For example:1. Use Alpine based images For most of the official docker images, there is a alpine varient. Alpine varients are much more light weight then their ubuntu/debian counter parts. Also, alpine base image is only 5MB.2. Delete packages after dependent applications are installed Lets say you want to install psycopg2 python package in your docker, for that you need to add packages like postgres-dev, python-dev, musl-dev etc. But those packages won\u0026rsquo;t be needed once psycopg2 is installed, so its better to delete them. For that, in alpine you can use the following code:RUN apk update \\  \u0026amp;\u0026amp; apk add --virtual build-deps gcc python3-dev musl-dev \\  \u0026amp;\u0026amp; apk add postgresql-dev \\  \u0026amp;\u0026amp; pip install psycopg2 \\  \u0026amp;\u0026amp; apk del build-depsExplanation: here I am installing the packages using --virtual build-deps tag, which means those packages will be installed build-deps directory. After psycopg2 is installed, I can simply delete it.3. Remove cache Remove cache from alpine image by adding --no-cache at the end of apk add. But down side is that each time you build the image, your packages will be installed again.Probably you can do the same in Ubuntu or debian based images using -- no-install-recommends on apt-get install.You can do the same thing with package manager, for example in pip:RUN pip install --no-cache-dir django4. Reduce layers in Dockerfile Reduce multiple lines into one if possible. Because each layer takes space in image. For example, reduce the following:RUN apk add gccRUN apk add python3-devTo this one:RUN apk add gcc python3-dev5. No need to install debug tools You don\u0026rsquo;t need debug tools like cURL or Vim in Docker(at least not in production). Install only the necessary packages.Reduce build time by caching layers Sometimes its better to cache layers which takes long time to install or download. For example:COPY /PLUGIN_DIR/pom.xml .RUN mvn dependency:go-offlineRUN mvn packageIn this code, I use an extra layer RUN mvn dependency:go-offline. When the image is built for the first time, this layer executes to resolve dependencies. But consecutive builds after that, they don\u0026rsquo;t execute this layer as its alredy cached. It reduces build time significantly.Here is another example:RUN pip3 install \\  pandas==0.25.2 \\  numpy==1.17.3 \\  psycopg2==2.8.4 \\  gensim==3.8.1RUN pip3 install -r requirements.pipHere you can see I am using extra layer to install pandas, numpy, scipy etc, and that layer will be cached after first build.In conclusion Thats it for now. I will add more stuff in future in this article or future articles. So stay tuned  . Finally, feel free to share your feedback in comment section below.Cheers!!Relevent posts  Tips on using Docker Compose, Build Argument and Environment Variable Tips on Using Docker Volume, Entrypoint and .dockerignore file  References  https://hackernoon.com/tips-to-reduce-docker-image-sizes-876095da3b34  ","date":"November 8, 2019","link":"/posts/docker-write-dockerfile-and-reduce-size-build-time-for-image/","readTime":4,"title":"Tips on Writing Dockerfile, Reduce Sizes and Build Time of Docker Image"},{"content":"For the past couple of days, I am testing out Manjaro linux. Most of the time, I was working with KDE Plasma Edition. In my opinion, it is the most beautiful desktop environment from Manjaro official distributions.I am a fan of Ubuntu Unity workspaces and wanted to use similar features in KDE plasma as well. Here is how I configure.Along with that, we shall see some visual and usability improvement shortcuts which can be very useful. Like stopping the dancing icons on application launch \u0026hellip; Table of contents    \u0026lsquo;Workspace\u0026rsquo; or \u0026lsquo;Virtual Desktop\u0026rsquo;  Store session per application in workspaces 2X2 workspaces No wrap around for workspaces Workspace switch shortcut   Visual and Usability Improvement Shortcuts  Turn keyboard backlight on/off Fix icon from taskbar or menu bar Change mouse click animation Mouse single click Application launch animation Close button (X) on left In conclusion     \u0026lsquo;Workspace\u0026rsquo; or \u0026lsquo;Virtual Desktop\u0026rsquo; Store session per application in workspaces Imagine how many clicks you have to do when you restart your PC because you need to start multiple apps you need and place them in different workspaces. As a developer, I find pretty hard to do that for every restart/logout. You can fix it from Desktop Session. Click on Super Key(Key with windows icon) to open Application Launcher and type Desktop Session, there you should see the options to store session after logout.  2X2 workspaces I prefer 2 desktop 2X2(2 rows and 2 columns) for work purpose. Each workspace has its own app, so I don\u0026rsquo;t have to click mouse to get to that application. You can turn this setup on from: Application Launcher \u0026gt; Virtual Desktops(you can find it by typing in search bar in application launcher).No wrap around for workspaces To stop circulating in between workspaces, you enable no wrap around virtual desktop from Application Launcher \u0026gt; Virtual Desktops settings. Just untic Navigations wraps around.Workspace switch shortcut Change the workspace shortcut from: Application Launcher \u0026gt; Global Shortcuts \u0026gt; KWin. I use the following ones: Visual and Usability Improvement Shortcuts Turn keyboard backlight on/off Usually keyboard backlight is off immediately after the laptop goes in battery mode. I find it annoying as most of the time I am working in the dark when there is load shedding. But you can turn that on by changing settings from Application Launcher \u0026gt; Energy Saving \u0026gt; [your preferred power mode] \u0026gt; Keyboard Backlight. Or click on the battery icon on the task bar and change the backlight settings there.Fix icon from taskbar or menu bar It might happen that your Application\u0026rsquo;s icon is missing from Taskbar or Menu bar(for repeated install/uninstall from different package managers). Probably you need to fix that from any one of the following directories:1. __/usr/share/applications/*.desktop__ 2. __/home/\u0026lt;usename\u0026gt;/.local/share/__  Change mouse click animation Change this settings from:Application Launcher \u0026gt; Desktop Effect \u0026gt; Mouse Click AnimationMouse single click By default, single click acts as double click in KDE. Turn that off from: Application Launcher \u0026gt; Workspace Behaviour \u0026gt; double click open file.Application launch animation Another annoying effect on KDE is application launch animation(the dancing application icon), you can turn it off from: Application Launcher \u0026gt; Application Launch Feedback \u0026gt; No Animation.Close button (X) on left I like Mac\u0026rsquo;s orientation of close and other title bar buttons. You can arrange it like that in KDE from: Application Launcher \u0026gt; Window Decorations \u0026gt; Title Bar buttons.In conclusion That\u0026rsquo;s it for now. Please suggest me more in the comment section below.Cheers!!","date":"October 30, 2019","link":"/posts/manjaro-kde-plasma-workspace-visual-improvements/","readTime":3,"title":"Manjaro KDE Virtual Desktop Management and Visual Improvement Shortcuts"},{"content":"Recently I tried out Manjaro operating system, and did some experiment with XFCE and KDE desktop environments. Here I am going to share some tips and tricks to setup Manjaro in you machine.This article is part of a series based on Manjaro. Tips shared here are applicable for all Manjaro desktop distributions. Now, let us dive in. Table of contents    Copy bash history to ZSH Backlight On using terminal Install visual studio code(VS Code) Wireless headphone muddy sound fix GPG verify key error fix Package managers  AUR or build your own packages Pacman Snap   Bootstrap manjaro setup IOS/Android file copy Rows and columns in workspaces Backup and restore Copy environment to other machine   Copy bash history to ZSH Lets say you are using bash for a long time, and now you have installed ZSH. All of a sudden all of your command history is gone, so you want to transfer you bash history to ZSH. You can do that using the following script(copy pasted from this snippet):import sys import time def main(): timestamp = None for line in sys.stdin.readlines(): line = line.rstrip(\u0026#39;\\n\u0026#39;) if line.startswith(\u0026#39;#\u0026#39;) and timestamp is None: t = line[1:] if t.isdigit(): timestamp = t continue else: sys.stdout.write(\u0026#39;: %s:0;%s\\n\u0026#39; % (timestamp or time.time(), line)) timestamp = None if __name__ == \u0026#39;__main__\u0026#39;: main() Usage: you can run like this $ cat ~/.bash_history | python bash-to-zsh-hist.py \u0026gt;\u0026gt; ~/.zsh_history.Backlight On using terminal You can turn backlight from command prompt like this:echo 2 | sudo tee /sys/class/leds/tpacpi::kbd_backlight/brightness Install visual studio code(VS Code) install VSCode using the following commands:git clone https://aur.archlinux.org/visual-studio-code-bin.git cd visual-studio-code-bin makepkg -si Another way is to use snap. Use snap install code --classic to do that.Wireless headphone muddy sound fix Wireless headphone setup/fix guide can be found here. But the most common problem is hearing muddy sound in wireless headphones, its due to lack of a2dp profile in bluetooth settings. You can do it by:pacmd set-card-profile card_number a2dp_sink More information can be found in this section of the Bluetooth Headset Article.GPG verify key error fix Sometimes when installing a package using pacman you will face an error GPG signature could not be verified, this means you are missing the required public key to verify that package or subpackage. So install that using:gpg --search \u0026lt;key\u0026gt; gpg --recv-key \u0026lt;key\u0026gt; Package managers There are several ways to install packages through package managers, LikeAUR or build your own packages AUR is the repository for softwares for ArchLinux. You can install the packages using:git clone \u0026lt;package\u0026gt; cd \u0026lt;package-dir\u0026gt; makepkg -si // -sic You can install the following packages from AUR: 1. Dropbox 2. Visual Studio Code. 3. Google Chrome. 4. Pulse Audio Bluetooth Module Git. 5. Pulse Audio A2DP Profile.Pacman Pacman is the package manager for Archlinux. Basic usages are:sudo pacman -S \u0026lt;package\u0026gt; # installing a package sudo pacman -R \u0026lt;package\u0026gt; # uninstalling a package Here are some packages which can be installed using pacman: docker ifuse usbmuxd libplist libimobiledevice vsftpd git deluge zsh hugo  Snap Snaps are containerised software packages that are simple to create and install. It comes built it with Manjaro. Here are some basic usages:snap install \u0026lt;package\u0026gt; snap remove \u0026lt;package\u0026gt; Some packages which can be installed by snap: Spotify: spotify VSCode: code --classic  Bootstrap manjaro setup Here are some gists containing useful scripts to bootstrap your manjaro setup with necessary packages: https://gist.github.com/gabrielmoreira/561ec9374e03567fa710 https://github.com/hypnoglow/manjaro-bootstrap https://gist.github.com/ruddra/db73009f778c0d36ebe88a9c9006bcd1  IOS/Android file copy Manjaro works well with andriod, but for IOS it does not have proper support(as there is no itunes). You can install the following packages to copy files from IOS:sudo pacman -Syyuu ifuse usbmuxd libplist libimobiledevice Rows and columns in workspaces Although KDE has this sorted out, but XFCE has some issues with it(only one row, no columns). You can sort it out by:xprop -root -f _NET_DESKTOP_LAYOUT 32cccc -set _NET_DESKTOP_LAYOUT 0,2,2,0 xprop -root -f _NET_NUMBER_OF_DESKTOPS 32c -set _NET_NUMBER_OF_DESKTOPS 4 Backup and restore As Archlinux is kind of bleeding edge thing, so updates might break your system, better to use a backup tool like timeshift.Copy environment to other machine You can use Aptik to do that but as its now a paid software, its upto you to decide if you want to use it or not.Thats it for now. If you have any feedback or suggestion, then please use the comment section below.Thanks for reading. I will see you in the next article.","date":"October 14, 2019","link":"/posts/manjaro-tricks-and-tips/","readTime":4,"title":"Manjaro Tips and Tricks"},{"content":"Lets say you have an API which is being used by many stores. Now for a specific store ABC, they want to have a different response format. So how would you make this implementation generic without making different views for store ABC? Simple, use middleware. Table of contents    How it works Usage Another usage scenario   How it works First update the url like this:url = [ path(\u0026#39;api/v1/\u0026lt;str:store_code\u0026gt;/\u0026#39;, include(\u0026#34;api.urls\u0026#34;)), ] Then write a middleware to capture the value of store_code:class StoreCodeMiddleware: def __init__(self, get_response): self.get_response = get_response def __call__(self, request): response = self.get_response(request) return response def process_view(request, view_func, view_args, view_kwargs): store_code = view_kwargs.get(\u0026#39;store_code\u0026#39;, None) if store_code: request.session[\u0026#39;store_code\u0026#39;] = store_code Here I am using process_view to capture the url argument. Because in process_view, there is a keyword argument named view_kwargs which is a dictionary, which will be passed to the view. Now, we can attach the store_code to request object through another middleware.class StoreToRequestMiddleware: def __init__(self, get_response): self.get_response = get_response def __call__(self, request): store_code = request.session.get(\u0026#39;store_code\u0026#39;, None) if store_code: request.store, _ = Store.objects.get_or_create(code=store_code) # assuming `Store` is the model class where store information are being stored response = self.get_response(request) return response Now lets add these middlewares in settings.py:MIDDLEWARES = [ # rest of the middlewares \u0026#39;path.to.StoreCodeMiddleware\u0026#39;, \u0026#39;path.to.StoreToRequestMiddleware\u0026#39; ] Make sure StoreCodeMiddleware is on top of StoreToMiddleware.Usage Lets say you have an API view, then use store info like this:class StoreAPI(APIView): def get(self, request, *args, **kwargs): serializer = CommonSerializer(request.store) if request.store.code == \u0026#34;ABC\u0026#34;: serializer = ABCSerializer(request.store) return Response(serializer.data) Another usage scenario Lets say you are using a package like black box which provides everything through url(means you can\u0026rsquo;t override anything inside the views or models), then the approach is useful. For example, when using djoser or django-rest-auth, they provide authentication services(like login, registration, password reset etc), you just need to include their url in your root url. When using these libraries, you might need to store additional information coming through url; then the given approach is really helpful.Hope it helps. Thanks for reading. Cheers!!","date":"October 2, 2019","link":"/posts/django-store-url-argument-through-middleware/","readTime":2,"title":"Django Access URL Arguments Through Middleware"},{"content":"Adding a plugin to Solr is relatively simple process. In this article, we are going to see how to add a plugin by creating a jar file using a docker container, then add that to a Solr which also runs inside docker container. Table of contents    Building the plugin  Workflow for building the plugin Example of solr image Explanation   Run solr  Without docker-compose With docker-compose   Configure plugin inside docker In conclusion   Building the plugin Workflow for building the plugin Steps for building a Solr plugin inside Solr docker file are like this: Example of solr image Here is an example to build a Solr plugin using Maven in Docker:FROM maven:alpine as build WORKDIR /app COPY /PLUGIN_DIR/pom.xml . RUN mvn dependency:go-offline COPY /PLUGIN_DIR/src /app/src RUN mvn package FROM solr:slim COPY --from=build /app/target/*.jar /opt/solr/dist/ EXPOSE 8983 CMD [\u0026#34;solr-precreate\u0026#34;, \u0026#34;gettingstarted\u0026#34;] Explanation Let us go through the Dockerfile:  We will be using a Maven based Docker image(using alpine to make it lightweight) for building the package. You can use Gradle based image as well if we are using gradle building system.  Then we use /app directory as work directory and inside that we will copy the pom.xml.  After that, we will install dependencies using mvn dependency:go-offline command. Reason for using this instead of clean is that, this step will be cached next time we build the image, which will reduce the building time a lot.  Then we add the source directory in docker and build the jar file.  Finally, we copy the jar file to Solr container at directory /opt/solr/dist.  We will be using solr-precreate command, which will create a core named gettingstarted if it does not exist.  Run solr Without docker-compose If you don\u0026rsquo;t use docker compose, then you can use the following instructions:Build the docker image using:docker build . -t my_solr Then run it:mkdir solrdata docker run -d -v \u0026#34;$PWD/solrdata:/var/solr\u0026#34; -p 8983:8983 my_solr Then the Solr will be running with core gettingstarted. Solr data will be shared with your local machine with solrdata directory(through volume).With docker-compose docker-compose.yml should contain the following service:version:\u0026#34;3\u0026#34;services:solr:container_name:my_solrbuild:context:.dockerfile:./Dockerfileimage:my_solrvolumes:- \u0026#34;./solrdata/:/var/solr/\u0026#34;ports:- \u0026#34;8983:8983\u0026#34;Then you can build the image by:docker-compose -f docker-compose.yml build solr Then run the service by:docker-compose -f docker-compose.yml up Configure plugin inside docker FYI: This step will vary from solr to solr based on its configuration, but the idea of using the plugin is almost same for any solr standalone mode.We need to load the plugin inside the core by putting the following line in data/gettingstarted/conf/solrconfig.xml inside solrdata volume.\u0026lt;lib dir=\u0026#34;${solr.install.dir:../../../..}/dist/\u0026#34; regex=\u0026#34;*.jar\u0026#34; /\u0026gt; Thats it, the plugin will be loaded in that core. Now you need to restart the solr to make the plugin usable.In conclusion Thank you for reading. Please use the comment section below if you have any feedback on this setup. Cheers!!","date":"October 1, 2019","link":"/posts/solr-docker-plugin-install/","readTime":3,"title":"Build and Configure Plugins Inside Solr Using Docker"},{"content":"VS Code is the most popular IDE at the moment. You can use it for developing applications in almost any programming language. In addition to that, you can do remote development in Docker,VM etc. In this post, we are going to discuss about how to use Docker Environment in VS Code._DISCLAIMER: This feature is already well documented in official documentation. But when I started integrating Docker environment, I found it bit hard to understand. So, here I tried to describe it in a easier and organized way._ Table of contents    What does it mean by remote development in VS Code Steps for set up  First step: appropriate docker environment Second step: create .devcontainer folder Third step: configuring devcontainer.json Fourth step: installing Remote Development Extension Pack Final step: run VS Code from container   In conclusion   What does it mean by remote development in VS Code It means VS Code will be used for developing the source code inside remote environment. When we are using docker, source codes are inside the docker container. VS Code will simply allow you to access that code inside the docker container but it will be running in your local machine. You can even access the packages/references from docker file, also it will allow you to debug from docker container. How cool is that!! Following image will clear up the concept(copy pasted from documentation): There are options available for either integrating one Docker environment or multiple environments using docker-compose.Steps for set up First step: appropriate docker environment Well, if you have a Dockerfile or a docker-compose.yml file, thats super cool. If you don\u0026rsquo;t have it, thats fine too. In that case, you can use an Docker Image to build the docker environment. FYI, if you are using Alpine Based Docker environment, then you need to use VS Code Insiders Edition.Second step: create .devcontainer folder In this step, you need to create a new folder named .devcontainer inside your source directory. Inside that, create a devcontainer.json file.Third step: configuring devcontainer.json Based on your setup, you need to use any one of the configurations:For \u0026lsquo;Dockerfile\u0026rsquo; If you have a Dockerfile, then copy and paste the following code:{ \u0026#34;name\u0026#34;: \u0026#34;Your Project\u0026#34;, \u0026#34;context\u0026#34;: \u0026#34;..\u0026#34;, \u0026#34;dockerFile\u0026#34;: \u0026#34;../Dockerfile\u0026#34;, // Path to docker file // Use the next line if you want to publish any ports. \u0026#34;appPort\u0026#34;: 3000, // If you want to use a post create command \u0026#34;postCreateCommand\u0026#34;: \u0026#34;npm install\u0026#34;, \u0026#34;extensions\u0026#34;: [ // extensions which is going to be installed inside the docker environment \u0026#34;ms-python.python\u0026#34;, \u0026#34;dbaeumer.vscode-eslint\u0026#34; ], \u0026#34;settings\u0026#34;: { // additional settings for VS Code configurations // You can copy paste them from `settings.json` of your workspace // Reference: https://code.visualstudio.com/docs/getstarted/settings#_settings-file-locations \u0026#34;python.pythonPath\u0026#34;: \u0026#34;/usr/local/bin/python\u0026#34;, \u0026#34;python.linting.pylintEnabled\u0026#34;: true, \u0026#34;python.linting.enabled\u0026#34;: true } } For \u0026lsquo;Docker Compose\u0026rsquo;(multiple docker environment) If you are using docker-compose, then you can use the following code:{ \u0026#34;name\u0026#34;: \u0026#34;Python 3\u0026#34;, \u0026#34;context\u0026#34;: \u0026#34;..\u0026#34;, \u0026#34;dockerComposeFile\u0026#34;: [\u0026#34;../docker-compose.yml\u0026#34;], // You need to point it your `docker-compose.yml` file with proper path. // Uncomment the next line if you want to publish any ports. \u0026#34;appPort\u0026#34;: [3000, \u0026#34;8921:5000\u0026#34;], // Uncomment the next line to run commands after the container is created. // \u0026#34;postCreateCommand\u0026#34;: \u0026#34;python --version\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;your_service\u0026#34;, // You must define which service you are going to use from docker compose. \u0026#34;workspaceFolder\u0026#34;: \u0026#34;/app\u0026#34;, // path to your source inside docker file \u0026#34;extensions\u0026#34;: [ // extensions which is going to be installed inside the docker environment \u0026#34;ms-python.python\u0026#34;, \u0026#34;dbaeumer.vscode-eslint\u0026#34; ], \u0026#34;settings\u0026#34;: { // additional settings for VS Code configurations // You can copy paste them from `settings.json` of your workspace // Reference: https://code.visualstudio.com/docs/getstarted/settings#_settings-file-locations \u0026#34;python.pythonPath\u0026#34;: \u0026#34;/usr/local/bin/python\u0026#34;, \u0026#34;python.linting.pylintEnabled\u0026#34;: true, \u0026#34;python.linting.enabled\u0026#34;: true } } Docker image If you don\u0026rsquo;t use either of them, then using an Image is also fine:{ \u0026#34;name\u0026#34;: \u0026#34;My Project\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/dotnet/core/sdk:latest\u0026#34;, \u0026#34;appPort\u0026#34;: 8090, \u0026#34;extensions\u0026#34;: [\u0026#34;ms-vscode.csharp\u0026#34;] } Fourth step: installing Remote Development Extension Pack Install Remote Development Extension Pack from VS Code Market, or use VS Code\u0026rsquo;s integrated market to install it:  Final step: run VS Code from container  After installing, an icon will appear at bottom left of your VS Code:   Now, click on that, and few options will appear like this:   Now click on Remote-Containers: Reopen Folder in Container option, the VS Code will reload. And voil, you are inside the Docker Environment!! You can also create a debugger and put break points in source code to see if it hits any.  In conclusion I feel that with this feature(remote development), VS code has become the best free IDE ever(even comparable to IDEA or VS). BTW, thanks for reading. Cheers!!","date":"August 6, 2019","link":"/posts/vs-code-remote-docker-development/","readTime":4,"title":"Use VS Code Inside Docker Container for Development"},{"content":"Djangos serialization framework provides a mechanism for translating Django models into other formats. Usually they are in json, yaml, XML, GeoJSON etc text based formats.Here, we are going to supercharge these serializers to do more things, even try to render properties and instance methods.This article will be useful to you if you are: Interested to output some data in serialized formats(given above) and don\u0026rsquo;t want to integrate any third party libraries for that. Want to Develop an API without any help of third party libraries. Using Django serializer and need to output extra values from property/instance methods. Not really a big fan of third party libraries.  This article is divided into three parts, first one is how to use serializers to show ManyToMany and ForignKey Fields. In second part, we are going to show how to override the serializers to display property and instance methods. In third section, there are some small examples using DRF(a third party library), if you are in real hurry. Table of contents    Proper way: the natural implementation Our way: overriding serializer classes  Render property or instance method(for \u0026lsquo;JSON\u0026rsquo;, \u0026lsquo;Python\u0026rsquo;, \u0026lsquo;GeoJSON\u0026rsquo;, \u0026lsquo;YAML\u0026rsquo; serializer) Only render specific fields using __field_name in \u0026lsquo;ForeignKey\u0026rsquo;(For \u0026lsquo;JSON\u0026rsquo;, \u0026lsquo;Python\u0026rsquo;, \u0026lsquo;GeoJSON\u0026rsquo;, \u0026lsquo;YAML\u0026rsquo; serializer) Show property and instance method(for \u0026lsquo;XML\u0026rsquo; serializer)   Third party: quicker way  Show ForignKey fields Show M2M fields Show property and instance methods     Proper way: the natural implementation For example purpose, let\u0026rsquo;s create three models:from django.db import models class User(models.Model): name = models.CharField(max_length=255) class Tag(models.Model): name = models.CharField(max_length=255) class Blog(models.Model): author = models.ForeignKey( User, on_delete=models.CASCADE ) tags = models.ManyToManyField(Tag) text = models.TextField() @property def a_property_method(self): return \u0026#34;I am a Property Method\u0026#34; def an_instance_method(self): return \u0026#34;I am a Instance Method\u0026#34; To use django\u0026rsquo;s implementation of rendering ForeignKey(we will address it as FK) and ManyToMany(we will address it as M2M) fields, we need to add natural_key method inside models:class User(models.Model): name = models.CharField(max_length=255) def natural_key(self): return (self.name) class Tag(models.Model): name = models.CharField(max_length=255) def natural_key(self): return (self.name) Then, if we try to render FK and M2M field like this:In [1]:from django.core import serializers In [2]:serialized_data = serializers.serialize(\u0026#39;json\u0026#39;, Blog.objects.all(), use_natural_foreign_keys=True, fields=[\u0026#39;author\u0026#39;, \u0026#39;tags\u0026#39;], indent=4 ) In [3]:print(serialized_data) Out [4]: [ { \u0026#34;model\u0026#34;: \u0026#34;experiment.blog\u0026#34;, \u0026#34;pk\u0026#34;: 1, \u0026#34;fields\u0026#34;: { \u0026#34;author\u0026#34;: \u0026#34;User\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;Tag 1\u0026#34;, \u0026#34;Tag 2\u0026#34; ] } } ] FYI, django serializer by default will not render @property or instance methods.Our way: overriding serializer classes This is our way by overriding the serializer class.Render property or instance method(for \u0026lsquo;JSON\u0026rsquo;, \u0026lsquo;Python\u0026rsquo;, \u0026lsquo;GeoJSON\u0026rsquo;, \u0026lsquo;YAML\u0026rsquo; serializer) To render @property or instance methods, we have to override the Serializer classes. We are going to override the def end_object(self, obj): method of any serializer.from django.core.serializers.json import Serializer from django.db.models import Manager # FYI: It can be any of the following as well: # from django.core.serializers.pyyaml import Serializer # from django.core.serializers.python import Serializer # from django.contrib.gis.serializers.geojson import Serializer class CustomSerializer(Serializer): def end_object(self, obj): for field in self.selected_fields: if field == \u0026#39;pk\u0026#39;: continue elif field in self._current.keys(): continue else: try: self._current[field] = getattr(obj, field)() # for model methods continue except TypeError: pass try: self._current[field] = getattr(obj, field) # for property methods continue except AttributeError: pass super(CustomSerializer, self).end_object(obj) Usage of property or instance method serializers = CustomSerializer() queryset = Blog.objects.all() data = serializers.serialize(queryset, fields=(\u0026#39;a_property_method\u0026#39;, \u0026#39;an_instance_method\u0026#39;)) Only render specific fields using __field_name in \u0026lsquo;ForeignKey\u0026rsquo;(For \u0026lsquo;JSON\u0026rsquo;, \u0026lsquo;Python\u0026rsquo;, \u0026lsquo;GeoJSON\u0026rsquo;, \u0026lsquo;YAML\u0026rsquo; serializer) In this approach, we are going to take pass name of the fields inside FK which we want to display in our response as forignkey__field_name. Passing these parameters going to look like this: fields=['fk__name1', 'fk__name2'] etc. So, let\u0026rsquo;s override the Serializerfrom django.contrib.gis.serializers.geojson import Serializer from django.db.models import Manager # FYI: It can be any of the following as well: # from django.core.serializers.pyyaml import Serializer # from django.core.serializers.python import Serializer # from django.core.serializers.json import Serializer JSON_ALLOWED_OBJECTS = (dict,list,tuple,str,int,bool) class CustomSerializer(Serializer): def end_object(self, obj): for field in self.selected_fields: if field == \u0026#39;pk\u0026#39;: continue elif field in self._current.keys(): continue else: try: if \u0026#39;__\u0026#39; in field: fields = field.split(\u0026#39;__\u0026#39;) value = obj for f in fields: value = getattr(value, f) if value != obj and isinstance(value, JSON_ALLOWED_OBJECTS) or value == None: self._current[field] = value except AttributeError: pass super(CustomSerializer, self).end_object(obj) Usage for specific fields in ForeignKey Usage will look like this:serializers = CustomSerializer() queryset = Blog.objects.all() data = serializers.serialize(queryset, fields=(\u0026#39;author__name\u0026#39;, \u0026#39;text\u0026#39;)) Output [ { \u0026#34;author__name\u0026#34;: \u0026#34;user1\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Some Text\u0026#34; }, { \u0026#34;author__name\u0026#34;: \u0026#34;user2\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Some Text\u0026#34; } ] Show property and instance method(for \u0026lsquo;XML\u0026rsquo; serializer) Same as JSON, Python, GeoJSON, YAML serializer, here we will override the end_object method. But in a slightly different way:from django.core.serializers.xml_serializer import Serializer class CustomSerializer(Serializer): def _init_options(self): self.non_model_fields = self.options.get(\u0026#39;non_model_fields\u0026#39;, []) def start_serialization(self): self._init_options() super(CustomSerializer, self).start_serialization() def end_object(self, obj): for field in self.non_model_fields: value = None try: value = getattr(obj, field)() # object method continue except TypeError: pass try: value = getattr(obj, field) # property method continue except AttributeError: pass self.indent(2) self.xml.startElement(\u0026#39;field\u0026#39;, { \u0026#39;name\u0026#39;: field, \u0026#39;type\u0026#39;: value.__class__.__name__, }) if value: self.xml.characters(str(value)) else: self.xml.addQuickElement(\u0026#34;None\u0026#34;) self.xml.endElement(\u0026#34;field\u0026#34;) return super(CustomSerializer, self).end_object(obj) Usage of \u0026lsquo;XML\u0026rsquo; serializer serializers = CustomSerializer() queryset = Blog.objects.all() data = serializers.serialize(queryset, non_model_fields=(\u0026#39;a_property_method\u0026#39;, \u0026#39;an_instance_method\u0026#39;)) Third party: quicker way We will be using Django Rest Framework and their serializer in this section.Show ForignKey fields Show ForignKey values using depth.class BlogSerializer(models.Model): class Meta: model = Blog fields = \u0026#39;__all__\u0026#39; depth = 1 Show M2M fields We can define a serializer for Nested class to be shown in M2M. Like this:from rest_framework import serializers class TagSerializer(serializers.ModelSerializer): class Meta: model = Tag fields = \u0026#39;__all__\u0026#39; class BlogSerializer(serializers.ModelSerializer): tags = TagSerializer(many=True) class Meta: model = Blog fields = \u0026#39;__all__\u0026#39; depth = 1 Show property and instance methods We can do that by specifying an extra field:class BlogSerializer(serializers.ModelSerializer): a_property_method = serializers.CharField(source=\u0026#39;a_property_method\u0026#39;, read_only=True) an_instance_method = serializers.CharField(source=\u0026#39;an_instance_method\u0026#39;, read_only=True) class Meta: model = Blog fields = \u0026#39;__all__\u0026#39; Thats all for today, thank you for reading. You can checkout my answers on this topic on StackOverflow: https://stackoverflow.com/a/56557206/2696165 https://stackoverflow.com/a/53352643/2696165  If you have any question, please ask in the comments section below, Cheers!!","date":"June 13, 2019","link":"/posts/django-serialize-foreignkey-m2m-property-instance-fields/","readTime":5,"title":"Django Serialize ForeignKey, ManyToMany, Instance, Property Method"},{"content":"When you are using OpenShift, you will be using routes to expose a route from a service. Let\u0026rsquo;s say you want to expose a path in example.com/dummy. When you do that for a django application(without any reverse proxy server), it usually becomes a problem, because the sub directory does not work well with Django\u0026rsquo;s urls. You will probably face error 404 page not found.If you are not using NGINX/Apache or any other reverse proxy server to serve the django application, you will not be able to set X-SCRIPT-NAME. To me, it does not make sense to use reverse proxy in pods when OpenShift provides routes(or ingress in kubernetes) using NGINX/HA-Proxy(depending on your setup).In this situation, I have used the following solution: Table of contents    Solution  Disable Namespace Ownership Check in Router Set FORCE_SCRIPT_NAME Update wsgi.py   Big thanks to   Solution Disable Namespace Ownership Check in Router You need to patch the router in OpenShift before starting any step(from OpenShift 3.9). By disabling Ownership check, you can allow a service to use a xyz.com/abc even if there is another service taking xyz.com. You can do it like this(from documentation):oc adm router ... --disable-namespace-ownership-check=true oc env dc/router ROUTER_DISABLE_NAMESPACE_OWNERSHIP_CHECK=true FYI: this also works in OKD as well.Set FORCE_SCRIPT_NAME I have set FORCE_SCRIPT_NAME in settings.py. Usually I fetched the value from environment varibles, which I have set in the Deployment Config:FORCE_SCRIPT_NAME = os.environ.get(\u0026#39;DJANGO_FORCE_SCRIPT_NAME\u0026#39;, \u0026#39;/subdirectory\u0026#39;) # without trailing slash Using FORCE_SCRIPT_NAME will configure django\u0026rsquo;s url in path /subdirectory. But it is not sufficient to serve the django application in sub path, as SCRIPT_NAME in requests are not updated, also it will show some erratic behaviour such as showing double entry of /subpath in url, 404 Page not found issues etc. To fix this, lets go to the second step:Update wsgi.py Lets update the wsgi.py file like this:import os from django.core.wsgi import get_wsgi_application from django.conf import settings os.environ.setdefault(\u0026#39;DJANGO_SETTINGS_MODULE\u0026#39;, \u0026#39;dummy.settings\u0026#39;) _application = get_wsgi_application() # Here is the important part def application(environ, start_response): script_name = getattr(settings, \u0026#39;FORCE_SCRIPT_NAME\u0026#39;, None) if script_name: environ[\u0026#39;SCRIPT_NAME\u0026#39;] = script_name path_info = environ[\u0026#39;PATH_INFO\u0026#39;] if path_info.startswith(script_name): environ[\u0026#39;PATH_INFO\u0026#39;] = path_info[len(script_name):] scheme = environ.get(\u0026#39;HTTP_X_SCHEME\u0026#39;, \u0026#39;\u0026#39;) if scheme: environ[\u0026#39;wsgi.url_scheme\u0026#39;] = scheme return _application(environ, start_response) In above code, we are getting the value of script_name from FORCE_SCRIPT_NAME, then putting that value in wsgi environ(environment variable).Big thanks to I got the implementation idea from this stackoverflow answer. Big thanks to @whp.That is it for today, hope it helps. If you have a better approach to resolve this problem please make a comment in comment section below.","date":"April 24, 2019","link":"/posts/deploy-django-subpath-openshift/","readTime":2,"title":"Deploy Django App in Sub Directory Using OpenShift"},{"content":"Connecting to MySQL from Python in MacOS is a very problematic and painful process. In this post, we are going to see how to install MySQL and connect a Python application to it using mysqlclient. Table of contents    Step one: install Homebrew Step two: install MySQL Step three: install MySQL-Connector-C Step four: install XCode-Select Step five: install OpenSSL Step six: unlink MySQL and link MySQL-Connector-C Step seven: install mysqlclient Step eight: link MySQL back again Step nine: add mysqlclient in virtual env(optional) In conclusion   Step one: install Homebrew You need to install Homebrew in you local machine. You can do it by:/usr/bin/ruby -e \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\u0026#34; Step two: install MySQL Using Homebrew you can install mysql simply by:brew install mysql Then setup the credentials in MySQL server using the following command:mysql_secure_installation Finally if you want to start at login and as a background service, run this:brew services start mysql Elsemysql.server start Step three: install MySQL-Connector-C For connecting any other application to MySQL, you need to install a connector. You can do it like this:brew install mysql-connector-c Then according to mysqlclient\u0026rsquo;s documentation, you need to put a bugfix at mysql_config. For that first type mysql_config in terminal.:\u0026gt;\u0026gt; mysql_config Usage: /usr/local/bin/mysql_config [OPTIONS] Compiler: ... It will show where you need to find mysql_config. Then you can use any of the editor of your liking and change the following lines inside the mysql_config:Change# on macOS, on or about line 112: # Create options libs=\u0026#34;-L$pkglibdir\u0026#34; libs=\u0026#34;$libs-l \u0026#34; To# Create options libs=\u0026#34;-L$pkglibdir\u0026#34; libs=\u0026#34;$libs-lmysqlclient -lssl -lcrypto\u0026#34; Step four: install XCode-Select You can do this by:xcode-select --install Step five: install OpenSSL Please run the following command:brew install openssl Then add its path to environment using following line:export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/opt/openssl/lib/ Step six: unlink MySQL and link MySQL-Connector-C You need to unlink mysql and link mysql-connector-c:brew unlink mysql brew link --overwrite mysql-connector-c Step seven: install mysqlclient You should be able to use pip install mysqlclient without any errors.Step eight: link MySQL back again Now, you need to do the opposite of Step six:brew unlink mysql-connector-c brew link --overwrite mysql Hopefully now mysqlclient should work fine and will have no problem connecting your application from mysql to python.Step nine: add mysqlclient in virtual env(optional) if you have mysqlclient installed in the system, then you can use --system-site-packages when creating virtual environment:python3 -m venv venv_name --system-site-packages Orvirtualenv venv_name --system-site-packages But, if you do not want to include all the system packages, and you still face error installing mysqlclient in virtual environment, then try following steps:source venv/bin/activate brew unlink mysql brew link --overwrite mysql-connector-c pip install mysqlclient brew unlink mysql-connector-c brew link --overwrite mysql In conclusion In this article, we have seen installing MySQL from Homebrew. I never used MySQL installed from Oracle website, so don\u0026rsquo;t know how to fix it. Better if you uninstall that and install MySQL from scratch using Homebrew.","date":"April 19, 2019","link":"/posts/install-mysqlclient-macos/","readTime":3,"title":"Install MySQL and MySQLClient(Python) in MacOS"},{"content":"Lets say you have been using a class named ClassA, objects created from that class has been used in many places. Now you have a requirement that you need to make a simillar class ClassB, which has almost simillar functionality. Objects of ClassA and ClassB will have simillar method signatures, and could be swapable. Or in many places, instead of ClassA you need to use ClassB. So how can you do that? Answer is, make a proxy class. Table of contents    Purpose of proxy object Implementation Explanation Usage How it helps Thanks   Purpose of proxy object The Proxy class instance will be acting as instance of either class ClassA or ClassB depending on certain conditions. You can put that logic inside Proxy Class.Implementation from path.to.a import ClassA from path.to.b import ClassB class ProxyClass(object): def do_some_condition_checks(self, *args, **kwargs): # do conditon checks return True # Or False def __init__(self, *args, **kwargs): condition = self.do_some_condition_checks(*args, **kwargs) object.__setattr__( self, \u0026#34;_obj\u0026#34;, ClassA(*args, **kwargs) if condition else ClassB(*args, **kwargs) ) # # proxying (special cases) # def __getattribute__(self, name): return getattr(object.__getattribute__(self, \u0026#34;_obj\u0026#34;), name) def __delattr__(self, name): delattr(object.__getattribute__(self, \u0026#34;_obj\u0026#34;), name) def __setattr__(self, name, value): setattr(object.__getattribute__(self, \u0026#34;_obj\u0026#34;), name, value) def __nonzero__(self): return bool(object.__getattribute__(self, \u0026#34;_obj\u0026#34;)) def __str__(self): return str(object.__getattribute__(self, \u0026#34;_obj\u0026#34;)) def __repr__(self): return repr(object.__getattribute__(self, \u0026#34;_obj\u0026#34;)) def __hash__(self): return hash(object.__getattribute__(self, \u0026#34;_obj\u0026#34;)) Explanation Here we will be setting an attribute _obj inside Proxy class which will be either an instance of ClassA or ClassB. Then we have provided overrides for built-in functions inside an object, which will get attributes from _obj instead of the Proxy object itself.Usage from path.to.proxy import ProxyClass instance = ProxyClass(foo_bar, foo=bar) How it helps Lets say you have usage of ClassA in many places of the code, where you might need to use ClassB when certain conditions meet. There you can simply use ProxyClass\u0026lsquo;s object so that you don\u0026rsquo;t need to define if/else conditons everywhere. It will make code cleaner, also if you need to change some conditions, you can change it in ProxyClass, then the logic will be reflected everywhere. You can even make less change, with naming the proxy class as ClassA, then you don\u0026rsquo;t have to change import references everywhere.Thanks I got help from this link.","date":"April 12, 2019","link":"/posts/python-proxy-object/","readTime":2,"title":"Create Proxy Object in Python"},{"content":"Today, I am going to write about few useful snippets/functionalities which I have used for Amazon S3 or any S3 compitable storage using Boto3 and Django Storage. FYI, this post focuses on using S3 with Django.So without further ado, let us begin. Table of contents    Configuring S3 Using S3 which it does not belong to AWS Serve S3 in different domain(through cloudFront or varnish) Create new bucket See all buckets Change access control to a bucket Delete objects from a bucket(also bucket itself) Use different folders when storing media and static contents Make \u0026lsquo;collect static\u0026rsquo; faster Check ACL status In conclusion   Configuring S3 When using django storage, you don\u0026rsquo;t need to put configurations like ~/.aws/credentials which is required in boto3. You can define those in settings in django settings, like:# settings.py AWS_ACCESS_KEY_ID = \u0026#39;XXXX\u0026#39; AWS_SECRET_ACCESS_KEY = \u0026#39;XXXX\u0026#39; AWS_STORAGE_BUCKET_NAME = \u0026#39;your-bucket\u0026#39; Using S3 which it does not belong to AWS If you want to use S3 provided by any company other than AWS, then configure the URL End Point in settings.py:AWS_S3_ENDPOINT_URL = \u0026#34;your-bucket-proivder.domain\u0026#34; Serve S3 in different domain(through cloudFront or varnish) Lets say, you want to serve S3 contents in cdn.abc.com/static/...(maybe through cloudfront or varnish), then put the following configuration in settings.py:AWS_S3_CUSTOM_DOMAIN = \u0026#39;cdn.abc.com\u0026#39; Create new bucket Creating buckets is fairly easy in boto3. Here I am going to share how you can do that using Django Storage:import boto3 from django.conf import settings session = boto3.session.Session() s3 = session.resource( \u0026#39;s3\u0026#39;, aws_access_key_id=settings.AWS_ACCESS_KEY_ID, aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY, endpoint_url=settings.AWS_S3_ENDPOINT_URL ) s3.create_bucket(Bucket=\u0026#34;your-bucket\u0026#34;) See all buckets Use the following command to see all buckets:import boto3 from django.conf import settings s3 = boto3.client( \u0026#39;s3\u0026#39;, aws_access_key_id=settings.AWS_ACCESS_KEY_ID, aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY, endpoint_url=settings.AWS_S3_ENDPOINT_URL ) for bucket in s3.buckets.all(): print(bucket.name) # to see items inside buckets for item in bucket.objects.all(): print(item) Change access control to a bucket You can change a bucket\u0026rsquo;s access control like this:s3 = boto3.client(\u0026#39;s3\u0026#39;) bucket = s3.Bucket(\u0026#39;my-bucket\u0026#39;) bucket.Acl().put(ACL=\u0026#39;public-read\u0026#39;) # Set all Bucket Policy for bucket in s3.buckets.all(): if bucket.name == bucket_name: bucket.Acl().put(ACL=\u0026#39;public-read\u0026#39;) Delete objects from a bucket(also bucket itself) You can use the following code to remove a bucket:bucket = s3.Bucket(\u0026#39;my-bucket\u0026#39;) bucket.objects.all().delete() # Or do it like this # for obj in bucket.objects.all(): # obj.delete() # If you also want to completely remove the empty bucket itself: bucket.delete() # Or delete in all buckets for bucket in s3.buckets.all(): bucket.objects.all().delete() bucket.delete() Use different folders when storing media and static contents I got this from django-cookiecutter.For this purpose, we need to create two new storage classes, subclassing from S3Boto3Storage:# in somefile.py from storages.backends.s3boto3 import S3Boto3Storage class StaticRootS3Boto3Storage(S3Boto3Storage): location = \u0026#34;static\u0026#34; class MediaRootS3Boto3Storage(S3Boto3Storage): location = \u0026#34;media\u0026#34; file_overwrite = False Then import them in settings.py:STATICFILES_STORAGE = \u0026#34;path.to.somefile.StaticRootS3Boto3Storage\u0026#34; DEFAULT_FILE_STORAGE = \u0026#34;path.to.somefile.MediaRootS3Boto3Storage\u0026#34; Make \u0026lsquo;collect static\u0026rsquo; faster Django\u0026rsquo;s collect static can be very slow when you will use S3 as storage. For that, you can use Collectfast to make this faster. Install it using pip install Collectfast. Then update the settings.py like this:AWS_PRELOAD_METADATA = True INSTALLED_APPS = ( #  \u0026#39;collectfast\u0026#39;, ) Check ACL status You can do it like this:s3.get_bucket_acl(Bucket='bucket-name')  In conclusion Thats all for now. Will update the post with more functionalities or snippets whenever I find one. Also, please suggest more snippets in the comment section below. Thanks for reading. Cheers!!","date":"April 6, 2019","link":"/posts/aws-boto3-useful-functions/","readTime":3,"title":"Do Extra in S3 Using Django Storage and Boto3"},{"content":"Suppose you are dealing with a poorly written API, which returns different type of objects at different scenarios. For example, sometimes it returns a string(ie. abcd), sometimes it returns a list(ie. [1,2,3,4]), or maybe None. Now, you are suppose to save that data, which you can\u0026rsquo;t predict. As you can\u0026rsquo;t predict the data type of object, you can\u0026rsquo;t design your Django Model accordingly. How are you suppose to take care of that? Table of contents    Convert to byte data Using \u0026lsquo;BinaryField\u0026rsquo; to store in database Usage examples Known issues Other information   Convert to byte data As you don\u0026rsquo;t know which data type you will be dealing with, so its safer to convert it to a common data type, which can be Byte Data. For that, you can use pickle. For example:import pickle byte_data = pickle.dumps(obj) obj = pickle.loads(byte_data) Using \u0026lsquo;BinaryField\u0026rsquo; to store in database You can use BinaryField to store the byte data created using pickle. For example:import pickle class Parameter(models.Model): _value = models.BinaryField() def set_data(self, data): self._value = pickle.dumps(data) def get_data(self): return pickle.loads(self._value) value = property(get_data, set_data) Usage examples You can use the value from the example Model given in last step like this:In: b = Foo.objects.create(value=1) In: b.value Out: 1 In: b = Foo.objects.create(value={1:1,2:2}) In: b.value Out: {1: 1, 2: 2} In: b = Foo.objects.create(value=[1,1]) In: b.value Out: [1, 1] Known issues There are some known issues you need to know about. For example:  You can\u0026rsquo;t make queries in this approach. Means you can\u0026rsquo;t do Foo.objects.get(value=1) or Foo.objects.filter(value=1) or any other DB query.  Also, you need to be careful about not abusing BinaryField. From Django Docs:   Although you might think about storing files in the database, consider that it is bad design in 99% of the cases. This field is not a replacement for proper static files handling. Other information You can look into my Stackoverflow Answer regarding this problem.","date":"February 10, 2019","link":"/posts/django-save-unknown-object-in-database/","readTime":2,"title":"Django: Save Unknown Object in Database"},{"content":"Linked list is a data structure where each object has points to next. In django, linked list can provide an interesting solution when it comes to custom ordering/grouping issues.Lets think of an example like this= you are making a blog site, and you have made a Post Model like this:class Post(models.Model): title=models.CharField(max_length=255) body=models.TextField() author=models.ForeignKey(User, on_delete=models.DO_NOTHING) Now you want to make a group of posts as series. How can you do that? Table of contents    Possible solutions  Without linked list With linked list   Conclusion   Possible solutions Without linked list One way of approaching this problem is to make a new model named Series and have ForeignKey relation from Post to Series. Also there should be a ordering field in Tutorials as well, so that it does not create a series with multiple posts with same order. For example like this:class Series(models.Model): title=models.CharField(max_length=255) class Post(models.Model): series=models.ForeignKey(Series, null=True, default=None) order = models.IntegerField() class Meta: unique_togather=(\u0026#39;series\u0026#39;, \u0026#39;order\u0026#39;,) Pros and cons without linked list There are few pros and cons of this approach. Pros are: the model structure is very flexible and easy to make a query:series = Series.objects.first() posts = series.post_set.all().order_by(\u0026#39;order\u0026#39;) But, a major cons is that, you need to create at-least 1 extra table and 1 extra field to maintain it.With linked list So, when we are going to use linked list, we need to put a reference to next object in model:class Post(models.Model): # rest of the fields previous=models.OneToOneField(\u0026#39;self\u0026#39;, null=True, blank=True, related_name=\u0026#34;next\u0026#34;) Funny thing is that, as linked list we were suppose to mark the next, instead we are creating an entry named previous in the model. But don\u0026rsquo;t worry, it will serve as reference to next object via related objects. Here are some explanations:When you are giving a related_name to a OneToOne(or ForeignKey Field), it will give the related object, for example in the model the previous post, a reference to current post. So when we type previous_post.next, it will return current post.\u0026gt;\u0026gt; p1 = Post.objects.create(...) \u0026gt;\u0026gt; p2 = Post.objects.create(...) \u0026gt;\u0026gt; p2.previous=p1 \u0026gt;\u0026gt; p2.save() \u0026gt;\u0026gt; print(p1.next) \u0026lt;Post: Post object (2)\u0026gt; Now, if we want to create a new series, all we have to do is link a post to its previous one, thats it.But in this approach, querying is bit complicated. You can get all the posts in a series like this:posts = Post.objects.filter(previous__isnull=True) # root posts will not have any previous post series = list() for post in posts: print(post) series_posts=list() series_posts.append(post) while(post.next): print(post.next) post=post.next series_posts.append(post) series.append(series_posts) print(series) Now, in template you can access previous and next posts of a given post like this:{{ post.title }} {{ post.body }} {% if post.previous %} \u0026lt;a href=\u0026#34;{% url \u0026#39;post-detail\u0026#39; post.previous.pk %}\u0026#34;\u0026gt;{{ post.previous.title }}\u0026lt;/a\u0026gt; {% endif %} {% if post.next %} \u0026lt;a href=\u0026#34;{% url \u0026#39;post-detail\u0026#39; post.next.pk %}\u0026#34;\u0026gt;{{ post.next.title }}\u0026lt;/a\u0026gt; {% endif %} Pros and cons with linked list The pros of this approach is that, you don\u0026rsquo;t have to create extra table for this. But a major downside is that, whenever you want to add a new post in middle of the series, you need to write a lot of lines. For example, you might need to change like this:p1=Post.objects.first() p2 = p1.next p3=Post.objects.create(...) p2.previous=p3 p2.save() p3.previous=p1 p3.save() But hopefully the order in Objects won\u0026rsquo;t change too frequently. So its not a major problem.Conclusion Fundamental data structure like linked list is really helpful in maintaining order in objects. I hope this post has been able to put some light on this topic. Thanks for reading.","date":"January 26, 2019","link":"/posts/django-model-linked-list/","readTime":3,"title":"Django: Ordering by Linked List for Model Objects"},{"content":"Every now and then I see some questions pop up in StackOverflow with title: ValueError: Related model \u0026lsquo;app.User\u0026rsquo; cannot be resolved This is a very common issue and pops up when the user tries to run the migration. So, lets talk more about what is this problem and how can we solve it. Table of contents    The problem Why It occurs How to resolve it  Easy way Hard Way Alternative way   For future projects In conclusion   The problem This error occurs when you have been using Django\u0026rsquo;s default User model, and in mid project, you need some changes in auth.User model and decided to use CustomUser.Why It occurs As per documentation: Changing AUTH_USER_MODEL after youve created database tables is significantly more difficult since it affects foreign keys and many-to-many relationship. As auth.User is core part of Django, and when you change the tables, it can\u0026rsquo;t work properly. This problem is hard to identify even for myself, because of the error it throws due to migration.How to resolve it Easy way The easiest(and cleanest) way is to delete all your migration files from all apps, drop the database. Then create a new database and connect your project to that. Then run python manage.py makemigrations and python manage.py migrate to make changes in DB. Means to make a clean start for the project, data wise.Hard Way Dropping the Database is not always possible for us. So there is an another approach to resolve this issue(based on ticket #25313):  Create a custom user model identical to auth.User, call it User (so many-to-many tables keep the same name) and set db_table='auth_user' (so it uses the same table). Like this:from django.contrib.auth.models import AbstractUser class User(AbstractUser): class Meta: db_table=\u0026#39;auth_user\u0026#39;   Throw away all your migrations from all the apps(except for __init__.py file inside the migrations folder).  Now update value of AUTH_USER_MODEL in your settings.py with myapp.User(Custom User Model) like AUTH_USER_MODEL = 'myapp.User'.  Recreate a fresh set of migrations using python manage.py makemigrations.  Make a backup of your database.  Delete all entries from django_migrations table.  Fake-apply the new set of migrations using python manage.py migrate --fake.  Optional: Set db_table=\u0026quot;your_custom_table\u0026quot; or remove it altogether.  Make other changes to the custom model, generate migrations, apply them.  Alternative way You can take a different approach rather than overriding you auth.User model. You can define a new model(lets say Profile), and put your relevant fields there. Then, make a OneToOne relation with auth.User like this:class Profile(models.Model): user = models.OneToOneField(User) some_field = models.CharField(max_length=255, null=True, blank=True, default=None) You can use post_save signals to create a profile automatically every time a User is created.from django.dispatch import receiver from django.core.signals import post_save from django.contrib.auth.models import User @receiver(post_save, sender=User) def create_user_profile(sender, instance=None, created=None, **kwargs): if created: Profile.objects.create(user=instance) You can use this easily in python:user = User.objects.first() user.profile.some_field # Thus you get value of some field from Profile # OR profile = Profile.objects.first() user = profile.user # Thus you get auth.User Or in template:{{ user.profile.some_field }}FYI: this process was mentioned in the documentation as well.For future projects For future projects, you should use your CustomUser model from the beginning. It will reduce a lot of hassle when it comes to customizing User.In conclusion To Be Honest, this an old problem and there is no good way to resolve it(at least not in Django 2.1). Hopefully in future releases, it will be fixed.","date":"January 3, 2019","link":"/posts/django-custom-user-migration-mid-phase-project/","readTime":3,"title":"Django: Changing User Model in Mid-Project"},{"content":" I\u0026rsquo;m not a great programmer; I\u0026rsquo;m just a good programmer with great habits.  Kent Beck Test Driven Development - in short TDD is a practice where you write the tests first then the actual code. It makes the code less vulnerable and it makes a testable software rather than writing the software, then test it.Django Rest Framework - in short DRF is a powerful framework which provides RESTful API support over Django. It provides minimal ways to expose the RESTful API directly from model( or without it, along with many other features out-of-the box like authentication, permission, throttling and so on.).As we will be testing our RESTful APIs though TDD, we need data which should simulate real world data. But writing these data in the code may not be efficient if its in large volume. Also it may take a long time to do so. So we need some tools which will generate those fake data for us. That is where Factory Boy and Faker comes in.Today we are going to discuss how we can use TDD in DRF with shortest implementation possible. Table of contents    Preparation Testing Writing Factory Conclusion   Preparation As always, we need to prepare ourselves before starting something. Obviously we need to setup Django and DRF in our local machine. Now, we can write a model in our system like this:class Blog(models.Model): name = models.CharField(max_length=255) slug = models.SlugField(max_length=255) body = models.TextField() # and so on Then, we need to write a serializer for serializing the Blog objects:class BlogSerializer(serializer.ModelSerializer): class Meta: model = Blog Let\u0026rsquo;s write a small view, which takes slug as url parameter and returns the actual blog response:from rest_framework.views import APIView from rest_framework.response import Response class BlogView(APIView): def get(self, slug): try: blog = Blog.objects.get(slug=slug) serializer = BlogSerializer(blog) return Response(serializer.data, status=status.HTTP_200_OK) except BlogDoesNotExist: return Response(status=status.HTTP_404_NOT_FOUND) except Exception: return Response(status=status.HTTP_500_INTERNAL_SERVER_ERROR) Another view to get all Blogs:from rest_framework.views import APIView from rest_framework.response import Response class BlogListView(APIView): def get(self): try: blogs = Blog.objects.all() serializer = BlogSerializer(blog, many=True) return Response(serializer.data, status=status.HTTP_200_OK) except Exception: return Response(status=status.HTTP_500_INTERNAL_SERVER_ERROR) Finally attach this view to an URL:path(\u0026#39;blog/\u0026lt;slug:title\u0026gt;/\u0026#39;, BlogView.as_view(), name=\u0026#34;blog-get\u0026#34;), path(\u0026#39;blogs/\u0026#39;, BlogView.as_view(), name=\u0026#34;blog-all\u0026#34;), Thus our preparation for testing is complete. Now lets dive into testing.Testing For writing test cases, we need to create some files inside the app. Lets say, we have created test_blog.py and inside it, the code should look like this:from rest_framework.test import APITestCase class BlogTest(APITestCase): # some pieces of code Here, we will be subclassing BlogTest from APITestCase because it comes built in with self.client which can be used for calling the API. Now lets setup a test:from django.urls import reverse # https://docs.djangoproject.com/en/2.1/ref/urlresolvers/#reverse class BlogTest(APITestCase): def test_blog_not_found(self): data = {\u0026#39;slug\u0026#39;: \u0026#39;random\u0026#39;} response = self.client.get(reverse(\u0026#39;blog-get\u0026#39;), data=data) self.assertEqual(response.status_code, status.HTTP_400_NOT_FOUND) Here we can see, if we call the API without creating a blog instance, it will return 404. Now lets create one:def test_blog_found(self): name = \u0026#34;TEST\u0026#34; slug = \u0026#34;test\u0026#34; body = \u0026#34;TEST TWO\u0026#34; blog = Blog.objects.create(name=name, body=body, slug=slug) ata = {\u0026#39;slug\u0026#39;: slug} response = self.client.get(reverse(\u0026#39;blog-get\u0026#39;), data=data) self.assertEqual(response.status_code, status.HTTP_200_OK) self.assertEqual(response.json().get(\u0026#39;slug\u0026#39;), slug) # and so on In this case, we should get 200 status and response slug should match our provided slug. As we are testing only one object, creating one should not be a problem. But how about a list objects(regarding our second view). Can we write the name, slug, body variables for 100 times if we need to check for 100 objects?Writing Factory To overcome this, we can setup a library called factory boy which will generate Blog objects randomly. To install factory boy, we need to type:pip install factory_boy Now, to generate 100 Blog objects, we can setup a Factory Class which has Blog in Meta class:class BlogFactory(factory.Factory): class Meta: model = Blog To ensure each object is random, we can put some providers in attributes(named same as model fields):class BlogFactory(factory.Factory): class Meta: model = Blog name = factory.Faker(\u0026#39;name\u0026#39;) slug = factory.Faker(\u0026#39;slug\u0026#39;) body = factory.Faker(\u0026#39;text\u0026#39;) Now we can use this factory in our test:def test_hundrand_blogs(self): for i in range(100): BlogFactory() response = self.client.get(reverse(\u0026#39;blog-list\u0026#39;)) self.assertEqual(response.status_code, status.HTTP_200_OK) self.assertEqual(len(response.data), 100) # and so on Pretty cool, right!! Now, let\u0026rsquo;s say we want to create a BlogFactory instance which has title X. We can do that by passing title keyword argument through BlogFactory instance call:def test_blog_X(self): blog = BlogFactory(title=\u0026#39;X\u0026#39;) response = self.client.get(reverse(\u0026#39;blog-get\u0026#39;), data={\u0026#39;slug\u0026#39;: blog.slug}) self.assertEqual(response.status_code, status.HTTP_200_OK) self.assertEqual(response.json().get(\u0026#39;title\u0026#39;), \u0026#34;X\u0026#34;) # and so on Finally, one more test case, where you want the X to be a random value. Its also doable using faker. We can install it using:pip install Faker And we can use it like this:from faker import Faker def test_blog_fake_name(self): fake = Faker() name = fake.name() blog = BlogFactory(title=name) response = self.client.get(reverse(\u0026#39;blog-get\u0026#39;), data={\u0026#39;slug\u0026#39;: blog.slug}) self.assertEqual(response.status_code, status.HTTP_200_OK) self.assertEqual(response.json().get(\u0026#39;title\u0026#39;), name) Conclusion Testing is made easier by libraries like Factory boy, Faker etc. So we should use them write test cases faster.Thats it. Thank you for reading. Let me know if you have any questions in comments section below.Cheers!!","date":"December 22, 2018","link":"/posts/tdd-drf-factory-boy-faker/","readTime":4,"title":"Testing for Django Rest Framework with Factory Boy and Faker"},{"content":" The Only Thing That Is Constant Is Change -  Heraclitus This blog site has changed quite a lot since the begining. I initially developed it as a Django application. It was part of my learning process, and I hosted it in a private shared server.But unfortunatly, good days came to an end, I had to give up that hosting for costing. So I was looking for a solution which will allow me to host it for free(as I already purchased the domain). Then I found out about github static page hosting.Initially I tried with Octopress. It was an awesome, till it wasn\u0026rsquo;t anymore. I had really hard time using it with builds and configuration. Once my PC crashed, and it felt like the end of the world to fix the octopress.Then, to resolve that issue, I started using ghost. Up till now, I have been using it. Its a fantastic tool for blogging with superb interface, markdown support and so on. Initially I used buster for generating the static site. I was stuck with ghost 0.11 for a long time, because upgrade was a hassel. But I finally did that using docker.But, again I started to face troubles with this setup. As buster is not activly maintained, and ghost was evolving, it was no longer competible with latest ghost. So I started to use a custom fork of buster. It worked fine with some manual tweaking. Here is the tweaked version. I had to rename every reference from localhost to ruddra.com via IDE. And some links were still ended with index.html. Which I had to fix time to time.But there was a huge struggle which came in, when I tried to migrate ghost from 1.11 to 2. All my route paths changed, and it was a disaster. I put a hack on the 404 page to redirect older urls to newer ones. But user experince or SEO was pretty bad.Thing is that, ghost isn\u0026rsquo;t built for static site, buster is itself a hack tool to get ghost\u0026rsquo;s contents. Still, ghost is good for personal blogging or if you host it yourself. I have a docker version of ghost to make it usable in any platform/nodejs version.Finally, by going through several blog sites, I found Hugo. Its really fast and most importantly it has static page generation support. Also, it fixed my url issues with aliases. I was able to map my old urls to new one.So far so good. I am really enjoying Hugo, and transition from Ghost to Hugo was very smooth, thanks to this post. I am using Hermit theme, which looks really cool.Finally, thank you Ghost, I will miss you. But I am looking forward to Hugo for upcoming future.BTW I have moved out of using Disqus. Although it is fine commenting mechanism(still recommended) but using a opensource solution is always tempting. Also in https://utteranc.es/ provides tracking free commenting mechanism and stores comments in github issues.","date":"December 21, 2018","link":"/posts/moving-to-hugo/","readTime":3,"title":"Bye Bye Ghost, Hello Hugo"},{"content":"While developing application in ReactJs with Redux and React Router 4, what I felt is that, there is no direct relationship between application\u0026rsquo;s state and routing. We needed to create those connections inside Components. In each component, we needed to check application state and based on that we made decisions where to go next(if needed). Table of contents    Scenario Approaches to solve the issue Even better solution Pros and cons  Pros Cons   Is there any better solution   Scenario Let\u0026rsquo;s think of a scenario, where we need to call an API, which returns some data or 404. If it returns 404, we will redirect the user to a 404 page. So the traditional way of doing this is:import React, { Component } from \u0026#34;react\u0026#34;; import * as actions from \u0026#34;./actions\u0026#34;; class HelloWorld extends Component { componentWillRecieveProps(nextProps) { const { apiStatus, history } = nextProps; if (apiStatus === \u0026#34;FAILED\u0026#34;) { history.push(\u0026#34;/not-found\u0026#34;); } } componentWillMount() { this.props.fetchAPI(this.props.match.params.id); // declaired in routes  } render() { return ( \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;{this.props.apiStatus.data}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ); } } function mapToStateProps(state) { return { apiStatus: state.apiStatus, }; } export default connect(mapToStateProps, actions)(HelloWorld); IMHO, this approach is bit messy, because in each component there is some codes regarding application flow, and needs to deal with history object to change routes. Let\u0026rsquo;s try to get rid of this code.Even if we want to remove this connection(between state and routing) from Component, we need to put it back somewhere else, and the best place is redux middleware. Because, it is able to intercept all the actions fired and access their payloads. It can even access redux store to fire actions and use reducers. But, there is a catch. We can\u0026rsquo;t manipulate routing from there.Approaches to solve the issue To overcome this hurdle, we need to use a library named connected-react-router. We can run npm install ---save connected-react-router or yarn add connected-react-router to install it. Also, need to follow their usage instructions for integrating this library with our react application.Basically, we will be using the push method for updating our routing. We can fire actions with push method from middleware.FYI: For the following example code, we are going to use redux-thunk, but if you are comfortable using redux-saga, then please checkout connected-react-router\u0026lsquo;s example based on react-saga.Now, let\u0026rsquo;s write the middleware for routing:import { push } from \u0026#34;connected-react-router\u0026#34;; export const routingMiddleware = (store) =\u0026gt; (next) =\u0026gt; (action) =\u0026gt; { if (action.type === API_FAILED) { store.dispatch(push(\u0026#34;/not-found\u0026#34;)); // assuming we are using redux thunk  } return next(action); }; And use it inside redux store:import { routingMiddleware } from \u0026#34;./middlewares\u0026#34;; const store = createStore( connectRouter(history)(rootReducer), initialState, compose( applyMiddleware( routerMiddleware(history), routingMiddleware // other middlewares  ) ) ); Also, let\u0026rsquo;s not forget to remove routing related codes from Component. After that, we are ready to go!!But, let us go one more step further, that is controlling the whole flow from middleware. For that, we need to create some actions:export const GO_TO_PAGE_ONE = \u0026#34;GO_TO_PAGE_ONE\u0026#34;; export const GO_TO_PAGE_TWO = \u0026#34;GO_TO_PAGE_TWO\u0026#34;; export function goToPageOne() { return { type: GO_TO_PAGE_ONE, }; } export function goToPageTwo() { return { type: GO_TO_PAGE_TWO, }; } Add them in the Component:import React, { Component } from \u0026#34;react\u0026#34;; import * as actions from \u0026#34;./actions\u0026#34;; class HelloWorldAgain extends Component { render() { return ( \u0026lt;div\u0026gt; \u0026lt;button onClick={this.props.goToPageOne()}\u0026gt;Go to Page One\u0026lt;/button\u0026gt; \u0026lt;button onClick={this.props.goToPageTwo()}\u0026gt;Go to Page Two\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); } } export default connect(null, actions)(HelloWorldAgain); Update the middleware, so it will catch any new actions when fired:export const routingMiddleware = (store) =\u0026gt; (next) =\u0026gt; (action) =\u0026gt; { if (action.type === API_FAILED) { store.dispatch(push(\u0026#34;/not-found\u0026#34;)); } else if (action.type === GO_TO_PAGE_ONE) { store.dispatch(push(\u0026#34;/page-one\u0026#34;)); } else if (action.type === GOTO_PAGE_TWO) { store.dispatch(push(\u0026#34;/page-two\u0026#34;)); } return next(action); }; Fantastic!! Now we are controlling our routing from middleware. We don\u0026rsquo;t need to put any routing related code in the Component.Even better solution Wait, wait \u0026hellip; There is more to that. There is one more step we need to do, so that we can control the application flow in much better way. That step is using reducers in middleware. Let\u0026rsquo;s think of an example: before a user can go to Page Two, he/she needs to visit Page One first. Let\u0026rsquo;s set up a reducer for accessing page one data:import { GO_TO_PAGE_ONE } from \u0026#34;../actions\u0026#34;; import { combineReducers } from \u0026#34;redux\u0026#34;; function pageOne(state = false, action) { switch (action.type) { case GO_TO_PAGE_ONE: return true; default: return state; } return state; } export default combineReducer({ pageOne: pageOne, }); Then, we can restrict the flow in middleware:export const routingMiddleware = (store) =\u0026gt; (next) =\u0026gt; (action) =\u0026gt; { if (action.type === API_FAILED) { store.dispatch(push(\u0026#34;/not-found\u0026#34;)); } else if (action.type === GO_TO_PAGE_ONE) { store.dispatch(push(\u0026#34;/page-one\u0026#34;)); } else if (action.type === GOTO_PAGE_TWO) { if (store.getState().pageOne == true) { store.dispatch(push(\u0026#34;/page-two\u0026#34;)); } else { store.dispatch(push(\u0026#34;/not-found\u0026#34;)); } } return next(action); }; Thus, we can control our application flow from middleware!!Pros and cons So far so good, right!! but there are some good side and bad side of this approach:Pros  All the flow restrictions and redirections in one place(One ring to rule them all !! ;) ). So change it in one place, reflect it everywhere else. All the routing related codes are in middleware. Means application state will be reflected directly in routing without involving any Component. No direct relation between Component and Routing (other than route declaration in store). Component will not have any routing related codes. Means they will be less messy. Means we can detouch Components from Routing if we want. Components can be more reusable.  Cons  Performance could be slower than native implementation. Bit of overengineering to acheive something simple.  Is there any better solution Well, when we were experimenting with this solution, we were almost at the end of our project, where we were heavily dependent on react router 4. But if you are only at the beginning or don\u0026rsquo;t have that much regard for react router 4, then consider using redux first router.","date":"October 12, 2018","link":"/posts/control-application-flow-from-redux-middleware/","readTime":5,"title":"Control Application Flow from Redux Middleware using React Router 4"},{"content":" This post is no longer valid as I have moved to Hugo  Table of contents    Verdict    Verdict I have just migrated the site from Ghost 1.25.2 to 2.2. As every growth comes with some inevitable pain, thus you can expect a lot of things are broken.Here all the URLS have been updated. So you have an old URL, for example: https://ruddra.com/2015/09/18/working-with-formset/ then it will be accessible at: https://ruddra.com/working-with-formset/.Feel free to comment here you if are looking for something and still could not find it in the site. I will be more than happy to help.FYI: I have changed the theme again!! This theme is called nomore. ","date":"October 11, 2018","link":"/posts/migration-from-ghost-1-2-to-2-2/","readTime":1,"title":"Migration From Ghost 1.25.2 to 2.2"},{"content":"The last post was about defining the pipelines. Now it is time to execute them. Also, at the end, we are going to show how to integrate webhook in your repository, so that for a specific event(like Push, Pull request merge etc) it will trigger the pipelines to automatically deploy the latest code to servers. Table of contents    Creating python jenkins slave Create build config Running pipeline Outcome  Before STAGE deployment prompt Full pipeline execution Blue Ocean pipeline Python+NGINX running in DEV and STAGE project   Automated deployment  BitBucket Github   Working example Useful resources   Creating python jenkins slave Most important step before we start using pipeline is to create a Jenkins Slave. For our python execution, we need a python slave to execute step 1-4 described in previous post. There is two ways to do that: Way One:  Go to Manage Jenkins(URL: Jenkins_Url/manage) Click on Configure System   Then go to bottom of settings, under Kubernetes Pod Template. Then click on Add Pod Template.   Now add configuration given as the following image:  If the image is not clear enough then please use the following dictionary:  { Name: python Labels: python // click on add container Container Template: { Name: jnlp Docker Image: idealo/jenkins-slave-python-centos7:latest Always pull image: True Working directory: /tmp Arguments to pass to the command: ${computer.jnlpmac}${computer.name} } } Let us save this configuration by clicking on Save button.   Way Two: Use this command from idealo\u0026rsquo;s repository: oc create -f https://raw.githubusercontent.com/idealo/jenkins-ci/master/config-map.yaml Or check here and follow their instructions.  By the way, there are other jenkins python slaves available in github or other opensource repository. Feel free to use any of your choice. I am using this repository, for this demo purpose.Create build config We need to import our pipeline to openshift. For that we need to create a Build Config for that. Let\u0026rsquo;s do that:apiVersion:v1kind:BuildConfigmetadata:labels:app:python-nginx-pipelinename:python-nginx-pipelinespec:source:git:ref:masteruri:\u0026lt;yourgitrepositorylink\u0026gt; strategy:jenkinsPipelineStrategy:jenkinsfilePath:path/to/Jenkinsfiletype:JenkinsPipelinetriggers:- github:secret:secrettype:GitHub- generic:secret:secrettype:Generic- bitbucket:secret:secrettype:BitbucketWe can save this as jenkins-pipeline.yaml. You can import this using:oc project cicd oc create -f jenkins-pipeline.yaml Now, if you go to web interface of OpenShift and access in url path OPENSHIFT_URL \u0026gt; console \u0026gt; CI/CD project \u0026gt; browse \u0026gt; pipelines, you should see a new pipeline has been created. You can import the build config directly in openshift web interface as well.  Running pipeline You can run the pipeline using the Start Pipeline on top right corner of pipeline page. Or you can do that using command line:oc start-build python-nginx-pipeline Outcome Here comes the sweet results of your hard work. You should see the pipeline executions like this:Before STAGE deployment prompt  Full pipeline execution  Blue Ocean pipeline If you go to Blue Ocean(comes build-in) of Jenkins, then go to your latest build, you should see a beautiful pipeline looks like this:  Also if you click on tests, then you should see the recorded result of tests:  In Artifacts section, you will see pipeline logs and the artifact file which you have stored in Step 4 of previous post:  Python+NGINX running in DEV and STAGE project You should see your server is up and running in DEV and STAGE Project.  Automated deployment Last part of our implementation is automated deployment. For that we are going to use Webhook. It will allow us to start a build based on an event in git repository. For starting a build, we need to define triggers in Pipeline Build Config. We have defined triggers in our build config. Now we need to copy that trigger link from here:  You can paste this webhook in your repository\u0026rsquo;s webhook settings, for example like this:BitBucket  Github  Then for any event(as per your configuration, like push, pull request merge etc) it will trigger a new build. FYI: this functionality will not work for minishift.Working example You will find a working example available at hereUseful resources  If you want to implement Django Using Pipeline, checkout here. Read details about Jenkins Python Slave in here. Read about running NGINX on OpenShift here. Read about CI/CD demo by RedHat in here. For Python2.7 based deployment use this Jenkins Slave.  Thank you for reading. Please let me know your feedback by commenting below.Cheers!!","date":"August 12, 2018","link":"/posts/openshift-python-gunicorn-nginx-jenkins-pipelines-part-three/","readTime":4,"title":"Automated Deployment to OpenShift Using Jenkins and Webhook"},{"content":"Pipeline is a set of instructions, which will be executed as per given sequence and produce a output. Jenkins Pipeline is simply writing these instructions in Jenkins. These pipelines can be written in Groovy.In previous post we have defined deployment structure and made necessary preparations for deployment of a python+gunicorn+nginx+jenkins based project. In this post, we will discuss about the most important part of our CI/CD project, writing jenkins pipeline. This pipeline will provide continuous delivery(CD) for our deployment. Table of contents    Plan for pipeline Steps  Step 0: declaring agent and ENV variables Step 1: get latest code Step 2: install dependencies Step 3: run tests Step 4: storing artifacts Step 5: create build configuration in OpenShift Step 6: build image Step 7: deploy to OpenShift in DEV Step 8: promote image to STAGE Step 9: deploy to OpenShift in STAGE Step 10: scale in STAGE   Conclusion   Plan for pipeline Our plan is to execute the following stages in serial order: Get Latest Code Install Dependencies Run Tests(and store them) Store Artifacts Create Build Configuration in OpenShift(DEV PROJECT) Build in OpenShift(DEV PROJECT) Create Deploy Configuration in OpenShift(DEV PROJECT) Promote the image(to STAGE PROJECT) Deploy to OpenShift(STAGE PROJECT) Scale Up(in STAGE PROJECT)  Stage 1-4 will be executed in Jenkins.Stage 5-10 will be executed in OpenShift. If you are feeling confused about these stages, don\u0026rsquo;t worry. We will be explaining them one by one. Please keep in mind, with every stage there is a description of possible outcome of the execution of that stage. But in this article, our task is only to create a pipeline, not execute it.Steps Step 0: declaring agent and ENV variables Stage Zero? Yep, this step is very important for writing pipeline. We will declare agent and environment variables in this step. In Jenkins, we need to specify a agent, in which the pipeline will be executing. We will be using python jenkins slave to execute our pipeline. We will discuss more about jenkins python slave in next article. The python agent should look like this:pipeline { agent { node {label \u0026#39;python\u0026#39;} } } We can write down our constant values in environment variables. For example:environment { APPLICATION_NAME = \u0026#39;python-nginx\u0026#39; GIT_REPO=\u0026#34;http://github.com/ruddra/openshift-python-nginx.git\u0026#34; GIT_BRANCH=\u0026#34;master\u0026#34; STAGE_TAG = \u0026#34;promoteToQA\u0026#34; DEV_PROJECT = \u0026#34;dev\u0026#34; STAGE_PROJECT = \u0026#34;stage\u0026#34; TEMPLATE_NAME = \u0026#34;python-nginx\u0026#34; ARTIFACT_FOLDER = \u0026#34;target\u0026#34; PORT = 8081; } Some variable declaration may seem confusing, but we will use them in next steps.Step 1: get latest code Jenkins Pipeline execution is done in stages. All the stages of pipeline will be inside in one stages dictionary. Like:stages { stage(\u0026#34;One\u0026#34;){ // Do Something  } stage(\u0026#34;Two\u0026#34;){ // Do Something  } } In first stage, we will be using Git Plugin which comes as default with OpenShift Jenkins. We will be using following code for pulling the latest code:stage(\u0026#39;Get Latest Code\u0026#39;) { steps { git branch: \u0026#34;${GIT_BRANCH}\u0026#34;, url: \u0026#34;${GIT_REPO}\u0026#34; // declared in environment  } } Step 2: install dependencies In this stage, we will install python dependencies inside in a virtual environment. For that, we will first install virtualenv using pip install virtualenv. After activating the virtualenv, we will install the dependencies using pip install -r requirements.pip from dependencies defined in app\u0026gt;requirements.pip.stage(\u0026#34;Install Dependencies\u0026#34;) { steps { sh \u0026#34;\u0026#34;\u0026#34; pip install virtualenv virtualenv --no-site-packages . source bin/activate pip install -r app/requirements.pip deactivate \u0026#34;\u0026#34;\u0026#34; } } Here we are using steps to execute the commands for installing dependencies.Step 3: run tests In this stage, we will run tests, so that we can make sure if any test fails, pipeline does not execute any further. We will also store the test results using JUnit Plugin.First, we will activate our virtualenv(again!!) then run tests in app directory using nosetests. It will export test results in xml format. Then we will store the result using JUnit.stage(\u0026#39;Run Tests\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; source bin/activate nosetests app --with-xunit deactivate \u0026#39;\u0026#39;\u0026#39; junit \u0026#34;nosetests.xml\u0026#34; } } Step 4: storing artifacts Artifact may sound weird to you if you are familliar with Java. Because we are working with Python, not Java builds. Python does not require any builds, still we are storing a compressed file consists of Python codes as well as our Dockerfile and NGINX configurations(app,config,Dockerfile), and we are going to use this compressed file to deploy our application to openshift. You might think that storing that file is not necessary, but I feel that storing that might be necessary, so that in later you can find out what is being pushed to openshift or if there is any discrepancy between what you want to deploy and what you are deploying. Anyways, for this stage, we will make safe name for naming our compressed file. BUILD_NUMBER is an environment variable available in pipeline, which provides current build number which should be unique per build. We will be using APPLICATION_NAME + BUILD_NUMBER to make a safe build name. We will store the Artifacts in a special directory, for now lets use target folder inside workspace(Workspace is basically the place/path where the whole pipeline execution is happening).stage(\u0026#39;Store Artifact\u0026#39;){ steps{ script{ def safeBuildName = \u0026#34;${APPLICATION_NAME}_${BUILD_NUMBER}\u0026#34;, artifactFolder = \u0026#34;${ARTIFACT_FOLDER}\u0026#34;, fullFileName = \u0026#34;${safeBuildName}.tar.gz\u0026#34;, applicationZip = \u0026#34;${artifactFolder}/${fullFileName}\u0026#34; applicationDir = [\u0026#34;app\u0026#34;, \u0026#34;config\u0026#34;, \u0026#34;Dockerfile\u0026#34;, ].join(\u0026#34; \u0026#34;); def needTargetPath = !fileExists(\u0026#34;${artifactFolder}\u0026#34;) if (needTargetPath) { sh \u0026#34;mkdir ${artifactFolder}\u0026#34; } sh \u0026#34;tar -czvf ${applicationZip} ${applicationDir}\u0026#34; archiveArtifacts artifacts: \u0026#34;${applicationZip}\u0026#34;, excludes: null, onlyIfSuccessful: true } } } We will be using script to execute our instructions. Script console is basically for running arbitrary commands inside it.Step 5: create build configuration in OpenShift After Step 4, we will have a nice compressed file containing what we need for building our app in openshift. Before start building the app, we need to have a Build Configuration inside OpenShift App. Basically Build Configuration is sort of a skeleton which contains instructions on how your source will be build. There are many strategies for build config. We will be using Docker strategy for building our code. That is why we have put the Dockerfile inside our compressed file. In build config, you need to provide from which source you want to build your code from. We will be using binary. Meaning we will provide a compressed file to openshift so that it can start building the image from it.stage(\u0026#39;Create Image Builder\u0026#39;) { when { expression { openshift.withCluster() { openshift.withProject(DEV_PROJECT) { return !openshift.selector(\u0026#34;bc\u0026#34;, \u0026#34;${TEMPLATE_NAME}\u0026#34;).exists(); } } } } steps { script { openshift.withCluster() { openshift.withProject(DEV_PROJECT) { openshift.newBuild(\u0026#34;--name=${TEMPLATE_NAME}\u0026#34;, \u0026#34;--docker-image=docker.io/nginx:mainline-alpine\u0026#34;, \u0026#34;--binary=true\u0026#34;) } } } } } This stage of pipeline will only execute once when the pipeline is executed for first time. Later when pipeline is executed for second time, third time\u0026hellip; and so on, this stage will be skipped. Because build configuration is made only once. You can check if a build configuration is being created using oc get bc in terminal (in project dev) from your local machine. You can also check it using web interface of openshift in CI/CD projects \u0026gt; Builds. openshift.withCluster and openshift.withProject is basically openshift APIs open to Jenkins plugin. Using these APIs, you can execute commands inside openshift cluster and projects. Our new-build command is being executed inside DEV_PROJECT. Value of DEV_PROJECT is defined in environment variable, and the value is dev.NB: Build Config, Deployment Config, Service, Image, Storage etc are Kubernetes basics. These terms will come more frequently in next stages. If you have some idea about them, it might help you to understand this tutorial even more. Step 6: build image Outcome of a successful build is an Image. So after a successful build, you will see an image inside OpenShift in web interface(at OpenShift_Url \u0026gt; console \u0026gt; DEV Project \u0026gt; browse \u0026gt; images). The most recent build outcomes to image tagged by latest. This is an important information, which will come handy in our next steps. In our build configuration(defined in last step), we configured that the build will executed from binary. Meaning if will expect a archive file to start the build process. As we have a archive file from step 4, we will use that file for this purpose.stage(\u0026#39;Build Image\u0026#39;) { steps { script { openshift.withCluster() { openshift.withProject(env.DEV_PROJECT) { openshift.selector(\u0026#34;bc\u0026#34;, \u0026#34;$TEMPLATE_NAME\u0026#34;).startBuild(\u0026#34;--from-archive=${ARTIFACT_FOLDER}/${APPLICATION_NAME}_${BUILD_NUMBER}.tar.gz\u0026#34;, \u0026#34;--wait=true\u0026#34;) } } } } } openshift.selector selects the build config named python-nginx (this value is set in ${TEMPLATE_NAME}). Using that build config, it initiates a build from the tar.gz file. After the build is complete, you will see an image named python-nginx and it should be tagged as latest.Step 7: deploy to OpenShift in DEV In this stage, we will deploy the image to DEV_PROJECT. Basically a Deployment Configuration is created in this step. Deployment Configuration is kind of like Build Config, but this is related to deployment. We will be using new-app to create our project from image built in last stage.stage(\u0026#39;Deploy to DEV\u0026#39;) { when { expression { openshift.withCluster() { openshift.withProject(env.DEV_PROJECT) { return !openshift.selector(\u0026#39;dc\u0026#39;, \u0026#34;${TEMPLATE_NAME}\u0026#34;).exists() } } } } steps { script { openshift.withCluster() { openshift.withProject(env.DEV_PROJECT) { def app = openshift.newApp(\u0026#34;${TEMPLATE_NAME}:latest\u0026#34;) app.narrow(\u0026#34;svc\u0026#34;).expose(\u0026#34;--port=${PORT}\u0026#34;); def dc = openshift.selector(\u0026#34;dc\u0026#34;, \u0026#34;${TEMPLATE_NAME}\u0026#34;) while (dc.object().spec.replicas != dc.object().status.availableReplicas) { sleep 10 } } } } } } This step is executed only in first time execution of pipeline. In next pipeline builds, this stage is skipped. But still, a deployment will be automatically done after each build is completed successfully. Because of Image Change Trigger. Meaning, after each new image, an automated deployment will initiate. New deployment means, it will generate new pods and will destroy the old pods. There are two strategies for this change, rolling strategy and recreate strategy. app.narrow(\u0026quot;svc\u0026quot;).expose(\u0026quot;--port=${PORT}\u0026quot;); this command will use the service built by new-app; then expose its route to port 8081, this value is set in Environment variable as well. You can see the new service and ports as well using oc get svc and oc get routes in dev project from your local machine.Step 8: promote image to STAGE In this stage, jenkins will prompt you to promote or abort regarding if you want to continue deployment to stage project or not. If you click in promote(in Jenkins), then openshift promote the image created in step 6 to stage and tag it as promoteToQA(as per defined in environment).stage(\u0026#39;Promote to STAGE?\u0026#39;) { steps { timeout(time:15, unit:\u0026#39;MINUTES\u0026#39;) { input message: \u0026#34;Promote to STAGE?\u0026#34;, ok: \u0026#34;Promote\u0026#34; } script { openshift.withCluster() { openshift.tag(\u0026#34;${DEV_PROJECT}/${TEMPLATE_NAME}:latest\u0026#34;, \u0026#34;${STAGE_PROJECT}/${TEMPLATE_NAME}:${STAGE_TAG}\u0026#34;) } } } } Step 9: deploy to OpenShift in STAGE In this step, we will be looking if any previous deployment is done in STAGE PROJECT. If it exists, then we will delete service, route, deployment config of that deployment, and create a new app using the new image promoted in last step. Then will expose the route for this application in 8081 port.stage(\u0026#39;Rollout to STAGE\u0026#39;) { steps { script { openshift.withCluster() { openshift.withProject(STAGE_PROJECT) { if (openshift.selector(\u0026#39;dc\u0026#39;, \u0026#39;${TEMPLATE_NAME}\u0026#39;).exists()) { openshift.selector(\u0026#39;dc\u0026#39;, \u0026#39;${TEMPLATE_NAME}\u0026#39;).delete() openshift.selector(\u0026#39;svc\u0026#39;, \u0026#39;${TEMPLATE_NAME}\u0026#39;).delete() openshift.selector(\u0026#39;route\u0026#39;, \u0026#39;${TEMPLATE_NAME}\u0026#39;).delete() } openshift.newApp(\u0026#34;${TEMPLATE_NAME}:${STAGE_TAG}\u0026#34;).narrow(\u0026#34;svc\u0026#34;).expose(\u0026#34;--port=${PORT}\u0026#34;) } } } } } Step 10: scale in STAGE We will be using openshiftScale API provided by Jenkins OpenShift Plugin. What it does is that, it creates replication of pods according the number of replication information provided to it via parameter replicaCount.stage(\u0026#39;Scale in STAGE\u0026#39;) { steps { script { openshiftScale(namespace: \u0026#34;${STAGE_PROJECT}\u0026#34;, deploymentConfig: \u0026#34;${TEMPLATE_NAME}\u0026#34;, replicaCount: \u0026#39;3\u0026#39;) } } } FYI: This API will be deprecated from oc version 3.11 This is the last step of our pipeline. Lets save it in a file named Jenkinsfile. Please commit and push your changes to your Git repository.The Jenkins file should look like this.Conclusion Lets discuss more about how to deploy our application using this pipeline in next article.Feel free to comment if you find anything unclear. Thanks for reading. Cheers!!","date":"August 11, 2018","link":"/posts/openshift-python-gunicorn-nginx-jenkins-pipelines-part-two/","readTime":10,"title":"Writing Jenkins Pipeline For OpenShift Deployment"},{"content":"Deploying a Python application to OpenShift is fairly easy. Write a Dockerfile and run oc new-app /path/to/Dockerfile, that\u0026rsquo;s it!! But if you want implement a full fledged modern CI/CD using Jenkins and openshift, you need to do little more than that. So let\u0026rsquo;s dive into it.We will explain about the whole process in three articles: Part One: We will explain the deployment structure and preparations. Part Two: We will write the Jenkins Pipeline. Part Three: Deploy using Jenkins Pipeline to OpenShift and Webhook implementation.  In this article, we will explain the deployment structure and required preparations for this deployment. Table of contents    Deployment structure Preparations  A \u0026lsquo;Python\u0026rsquo; application Preparing NGINX Writing \u0026lsquo;Dockerfile\u0026rsquo; Preparing git repository Preparing OpenShift/Minishift Preparing jenkins   In conclusion   Deployment structure In this deployment, our python program will be running inside a container. That program will be served through a Gunicorn server. We will reverse proxy this server from NGINX, running in the same container. The following diagram shows our application structure:  The OpenShift Cluster will have three project spaces, CI/CD, DEV and STAGE. CI/CD will contain the Jenkins file, running pipelines inside it. DEV and STAGE projects will have applications running inside them. Next diagram will show what we intend to do in this CI/CD project:  Preparations For implementing this CI/CD project, we need to prepare few things beforehand. Let us go through them one by one.A \u0026lsquo;Python\u0026rsquo; application NB: if you have a python application, which runs through gunicorn and has tests runable through nosetests(or any xml test result exporter) then please skip this section.Writing a python application For this demo purpose, we will use a small Flask application, which exposes a simple REST api. We will be using the code below:from flask import Flask, jsonify app = Flask(__name__) tasks = [ { \u0026#39;id\u0026#39;: 1, \u0026#39;title\u0026#39;: \u0026#39;OpenShift Jenkins Pipeline Python/Nginx Implementation\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;Find the implementation at https://github.com/ruddra/openshift-python-nginx\u0026#39; }, { \u0026#39;id\u0026#39;: 2, \u0026#39;title\u0026#39;: \u0026#39;OpenShift Jenkins Pipeline Django Implementation\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;Find the implementation at https://github.com/ruddra/openshift-django\u0026#39; } ] @app.route(\u0026#39;/\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_tasks(): return jsonify({\u0026#39;tasks\u0026#39;: tasks}) if __name__ == \u0026#39;__main__\u0026#39;: app.run(host=\u0026#39;0.0.0.0\u0026#39;) First we should create a empty Project directory. Inside that, we will put another folder named app, which will contain the python application. Save the above flask application as flask_app.py. We need to install flask using pip install flask as well.Adding gunicorn We can run this application using python flask_app.py, but its not safe to use a development server in production. So we will use gunicorn to run our application. Let\u0026rsquo;s install that using pip install gunicorn, then write a wsgi.py inside the app directory.from flask_app import app as application if __name__ == \u0026#34;__main__\u0026#34;: application.run() If we run gunicorn --bind 0.0.0.0:5000 wsgi, we should be able to see our project up and running in 0.0.0.0:5000. The output should look like this:  Writing tests Writing tests are important, and we are going to store test results in Jenkins. For this purpose, let\u0026rsquo;s create some tests inside app directory.import unittest class MyTestClass(unittest.TestCase): @classmethod def setUpClass(cls): pass @classmethod def tearDownClass(cls): pass def setUp(self): pass def tearDown(self): pass def test_dummy_one(self): self.assertEqual(2, 2) def test_dummy_two(self): self.assertEqual(True, True) For running the tests, we are going to use nosetests, so let\u0026rsquo;s install that using pip install nose. We can run tests using nosetests --with-xunit, it will generate a xml file which will contain the test results.Our preparation is complete for the python application. Now let\u0026rsquo;s store our requirements using pip freeze \u0026gt; requirements.pip.Preparing NGINX In this step, we will write a NGINX configuration file which will proxy pass the gunicorn server.upstream web { ip_hash; server 0.0.0.0:5000; } server { location / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_pass http://web/; } listen 8081; server_name _; } In here, the NGINX will listen to 8081 port. It will proxy pass gunicorn at location \u0026ldquo;/\u0026quot;. Let\u0026rsquo;s save the above code in default.conf file and store in config/nginx directory of the project.Writing \u0026lsquo;Dockerfile\u0026rsquo; This is the most important preparation step of our project. For this demo purpose, we will be using a alpine based nginx Dockerfile.FROMnginx:mainline-alpine# --- Python Installation ---RUN apk add --no-cache python3 \u0026amp;\u0026amp; \\  python3 -m ensurepip \u0026amp;\u0026amp; \\  rm -r /usr/lib/python*/ensurepip \u0026amp;\u0026amp; \\  pip3 install --upgrade pip setuptools \u0026amp;\u0026amp; \\  if [ ! -e /usr/bin/pip ]; then ln -s pip3 /usr/bin/pip ; fi \u0026amp;\u0026amp; \\  if [[ ! -e /usr/bin/python ]]; then ln -sf /usr/bin/python3 /usr/bin/python; fi \u0026amp;\u0026amp; \\  rm -r /root/.cache# --- Work Directory ---WORKDIR/usr/src/app# --- Python Setup ---ADD . .RUN pip install -r app/requirements.pip# --- Nginx Setup ---COPY config/nginx/default.conf /etc/nginx/conf.d/RUN chmod g+rwx /var/cache/nginx /var/run /var/log/nginxRUN chgrp -R root /var/cache/nginxRUN sed -i.bak \u0026#39;s/^user/#user/\u0026#39; /etc/nginx/nginx.confRUN addgroup nginx root# --- Expose and CMD ---EXPOSE8081CMD gunicorn --bind 0.0.0.0:5000 wsgi --chdir /usr/src/app/app \u0026amp; nginx -g \u0026#34;daemon off;\u0026#34;Let\u0026rsquo;s store this Dockerfile in root directory of Project. In Python Setup section, we are using a minimal python setup with pip enabled. For our demo purpose, this is just enough. If your project has more dependencies, then install them using apk add python3-dev ... etc. In NGINX Setup section, we have add the nginx default.conf file to /etc/nginx/conf.d/. In Expose and CMD section, we have exposed 8081 port and we will be running this command when the docker is running: gunicorn --bind 0.0.0.0:5000 wsgi --chdir /usr/src/app/app \u0026amp; nginx -g \u0026quot;daemon off;\u0026quot;So far, the project directories should be looking like this:| - Project | -- app | |-- flask_app.py | |-- wsgi.py | |-- requirements.pip | -- config | |-- nginx | ||-- default.conf | - Dockerfile If you want to run this Dockerfile in project root directory, you can do this like:docker build -t flask-nginx . docker run -p 8081:8081 flask-nginx You can access the application in browser through localhost:8081.Preparing git repository Let\u0026rsquo;s create a repository at your github/bitbucket account. Then push this code to that repository.Preparing OpenShift/Minishift I would recommend using minishif for this demo purpose. I am not sure if this app will run in free RedHat\u0026rsquo;s openshift cloud services. If you own a private OpenShift Server, that is fine too. For using minishift, please follow this guideline.Preparing jenkins Once your openshift/minishift is ready, let\u0026rsquo;s log into that system using:oc login --server=server_name --username=your_username --password=your_password Now, we need to create three project spaces, cicd, dev, stage for this demo purpose.oc new-project dev --display-name=\u0026#34;Dev\u0026#34; oc new-project stage --display-name=\u0026#34;Stage\u0026#34; oc new-project cicd --display-name=\u0026#34;CI/CD\u0026#34; Very good. Now we are going to use project cicd and install Jenkins there:oc project cicd oc new-app jenkins-persistent Let\u0026rsquo;s expose jenkin\u0026rsquo;s url using:oc expose svc/jenkins-persistant FYI: You can do this step using OpenShift UI as well.  Now, we need to give jenkins permission to make changes in dev and stage. Let\u0026rsquo;s do that using:oc policy add-role-to-user edit system:serviceaccount:cicd:jenkins -n dev oc policy add-role-to-user edit system:serviceaccount:cicd:jenkins -n stage In conclusion Our preparation is completed. Let\u0026rsquo;s continue with Jenkins pipeline in next article.If you have any questions regarding this, feel free to comment. Thanks for reading.Cheers!!","date":"August 11, 2018","link":"/posts/openshift-python-gunicorn-nginx-jenkins-pipelines-part-one/","readTime":6,"title":"Deploy A Python App to OpenShift: Planning and Preparations"},{"content":"Using Jenkins pipeline, you can easily automate your deployments in openshift. In this post, we are going to use this powerful tool to deploy a Django project. But before we start, one thing I need to mention is that, this project has been tested on in Minishift 1.2.0, OpenShift v3.9.0 and Kubernetes v1.9.1. So without further adou, let us begin. Table of contents    Source code Pipeline excecusion steps Automated deploy on minishift Manual deploy on minishift Screenshots How to customize the pipeline to deploy your project  Pre-requisite Modify pipeline   Advanced implementation   Source code You can find the source code for a this project here: https://github.com/ruddra/openshift-djangoPipeline excecusion steps On every pipeline execution, the code goes through the following steps: First, jenkins will check if build configuration for https://github.com/ruddra/openshift-django exists in dev project. If not, then it will create new app using the templates provided in code. Then, it will build and test the code After successful build, it will deploy the image to dev project. Then it will prompt if it will deploy the code to stage. If yes, then it will proceed to next step, else it will abort. It will push the dev image to stage and tag it as promoteToQA. This image will be deployed to stage project and will be scaled to 3.  Automated deploy on minishift Go to scripts directory and run ./setup.shManual deploy on minishift First, run minishift using minishift start memory=4000. Then, run the following commands to create projects in OpenShift.# Create Projects oc new-project dev --display-name=\u0026#34;Tasks - Dev\u0026#34; oc new-project stage --display-name=\u0026#34;Tasks - Stage\u0026#34; oc new-project cicd --display-name=\u0026#34;CI/CD\u0026#34; Install Jenkins in cicd by runningoc project cicd oc new-app openshift/jenkins-2-centos7 Now, give permission to Jenkins to modify in dev and stage project.oc policy add-role-to-user edit system:serviceaccount:cicd:jenkins -n dev oc policy add-role-to-user edit system:serviceaccount:cicd:jenkins -n stage Deploy the MySQL in cicd project by loading MySQL template like this:oc new-app -f .openshift/templates/mysql-template.yaml It will create the mysql server. Please keep in mind that, this MySQL will be available in 172.30.0.30. This configurations in defined in Service configuration section of the templateSo, we are almost ready. Now please load the pipeline using:oc new-app -f .openshift/pipelines/openshift-django-pipeline.yaml Now you will be able to see the pipeline in Console \u0026gt; CI/CD \u0026gt; Build \u0026gt; Pipelines. You can run it from there or from CLI like:oc start-build djangopipeline To implement webhooks, you can look into this project.Screenshots Pipeline execustion looks like this in Blue Ocean:  Stage View in Jenkins looks like this:  In CI/CD project of OpenShift:  In Dev project of OpenShift:  In Stage project of OpenShift:  After Pipeline excecustion, it should look like this:  How to customize the pipeline to deploy your project Pre-requisite  Have a Django Project with Docker to deploy it. Have minishift installed in your local machine. Give it 4GB memory. If you can pull python, jenkins-2-centos7, mysql-55-centos7 images beforehand using docker pull \u0026lt;image name\u0026gt;, it would make the deployment much more faster.  Modify pipeline You can use this Pipeline to deploy your own project with this minimal changes: Go to Pipeline template at here: OR Take this pipeline:  def openshiftDjangoRepo=\u0026#34;https://raw.githubusercontent.com/ruddra/openshift-django/develop/.openshift/templates/openshift-django-template.yaml\u0026#34; def stageTag=\u0026#34;promoteToQA\u0026#34; def DEV_PROJECT=\u0026#34;dev\u0026#34; def STAGE_PROJECT=\u0026#34;stage\u0026#34; def templateName=\u0026#34;openshift-django\u0026#34; pipeline{ agent { label \u0026#34;\u0026#34;} stages{ stage(\u0026#39;Create in DEV\u0026#39;) { when { expression { openshift.withCluster() { openshift.withProject(DEV_PROJECT) { echo \u0026#34;checking openshift django exists in DEV\u0026#34; return !openshift.selector(\u0026#34;bc\u0026#34;, \u0026#34;${templateName}\u0026#34;).exists(); } } } } steps { script { openshift.withCluster() { openshift.withProject(DEV_PROJECT) { openshift.newApp(openshiftDjangoRepo).narrow(\u0026#34;svc\u0026#34;).expose(); } } } } } stage(\u0026#39;Build and Test in DEV\u0026#39;){ steps { script { openshiftBuild(namespace: \u0026#34;${DEV_PROJECT}\u0026#34;, buildConfig: \u0026#34;${templateName}\u0026#34;, showBuildLogs: \u0026#39;true\u0026#39;, waitTime: \u0026#34;600000\u0026#34;) } } } stage(\u0026#39;Rollout to DEV\u0026#39;) { steps { script { openshiftDeploy(namespace: \u0026#34;${DEV_PROJECT}\u0026#34;, deploymentConfig: \u0026#34;${templateName}\u0026#34;, waitTime: \u0026#34;600000\u0026#34;) } } } stage(\u0026#39;Scale in DEV\u0026#39;) { steps { script { openshiftScale(namespace: \u0026#34;${DEV_PROJECT}\u0026#34;, deploymentConfig: \u0026#34;${templateName}\u0026#34;, replicaCount: \u0026#39;1\u0026#39;) } } } stage(\u0026#39;Promote to STAGE?\u0026#39;) { steps { timeout(time:15, unit:\u0026#39;MINUTES\u0026#39;) { input message: \u0026#34;Promote to STAGE?\u0026#34;, ok: \u0026#34;Promote\u0026#34; } script { openshift.withCluster() { openshift.tag(\u0026#34;${DEV_PROJECT}/${templateName}:latest\u0026#34;, \u0026#34;${STAGE_PROJECT}/${templateName}:${stageTag}\u0026#34;) } } } } stage(\u0026#39;Rollout to STAGE\u0026#39;) { steps { script { openshift.withCluster() { openshift.withProject(STAGE_PROJECT) { if (openshift.selector(\u0026#39;dc\u0026#39;, \u0026#34;${templateName}\u0026#34;).exists()) { openshift.selector(\u0026#39;dc\u0026#39;, \u0026#34;${templateName}\u0026#34;).delete() openshift.selector(\u0026#39;svc\u0026#39;, \u0026#34;${templateName}\u0026#34;).delete() openshift.selector(\u0026#39;route\u0026#39;, \u0026#34;${templateName}\u0026#34;).delete() } openshift.newApp(\u0026#34;${templateName}:${stageTag}\u0026#34;).narrow(\u0026#34;svc\u0026#34;).expose() } } } } } stage(\u0026#39;Scale in STAGE\u0026#39;) { steps { script { openshiftScale(namespace: \u0026#34;${STAGE_PROJECT}\u0026#34;, deploymentConfig: \u0026#34;${templateName}\u0026#34;, replicaCount: \u0026#39;3\u0026#39;) } } } } } Edit this line in the template: def openshiftDjangoRepo=\u0026#34;https://raw.githubusercontent.com/ruddra/openshift-django/master/.openshift/templates/openshift-django.yaml\u0026#34; Change the url to either your repository link like https://github.com/ruddra/openshift-django Change the template name in there as well:  def templateName=\u0026#34;openshift-django\u0026#34; Advanced implementation In this post, we have deployed our application by using only OpenShift APIs in Jenkins Plugin. If you want more advanced implementation, like having Jenkins do build, testing, store test results etc with more advanced pipeline, then please look into these posts: Deploy A Python App to OpenShift: Planning and Preparations Writing Jenkins Pipeline For OpenShift Deployment Automated Deployment to OpenShift Using Jenkins and Webhook  Thanks for reading. Cheers!!","date":"July 22, 2018","link":"/posts/django-openshift-pipeline/","readTime":4,"title":"Deploy Django to OpenShift Using Jenkins Pipeline(CI/CD)"},{"content":" This post is deprecated. Use it at your own risk Simple and clean boilerplate for using django and angular 1.x togather, with basic structure comes built-in. Table of contents    Tech Stack Prerequisite Setup frontend Setup docker Setup backend Usage Local settings sample Screenshots  Front page at: http://localhost:8000/#!/ Rest API at: http://localhost:8000/api/movies/ Admin Site at: http://localhost:8000/admin/movies/movie/     Tech Stack  Python3 Django 1.11.6, Django Rest Framework Webpack 2 JavaScript(ES6)/AngularJS 1.x Yarn  Prerequisite  Require Yarn. How to install Yarn: https://yarnpkg.com/lang/en/docs/install/ Require Docker if you want to use this boiler plate with docker. How to install Docker: https://docs.docker.com/engine/installation/ ** If you don\u0026rsquo;t want docker, you can still use this boilerplate. Look herefor details  Setup frontend  Run make install-frontend to install frontend dependencies Run make build-frontend to build frontend. Run make watch to watch over the file changes  Setup docker  Run make docker-build to build up docker Run make docker-up to startup docker apps for first time. To start again make docker-start, stop make docker-stop, restart make docker-restart For running the migrations: make docker-migrate *How to use django with docker: ruddra.com/2016/08/14/docker-django-nginx-postgres/ *For more commands, go inside docker folder.  Setup backend If you are not using docker, then you can go to backend folder and do following steps: Create Virtual Environment using: virtualenv -p python3 /path/to/venv Activate it: source /path/to/venv bin activate(Its different in Windows, see virtual environment documentation) Run pip install -r requirements.txt Put your local settings in local_settings.py and place it in \u0026ldquo;backend/movie_app/\u0026rdquo; directory to override current settings. FYI You need to put your DB settings in local_settings.py Now run ./manage.py migrate to migrate Database Now run ./manage.py collectstatic to run collectstatic Run: ./manage.py runserver to run the django application  Usage  Create super user and access the adminsite. In there you can create movies instances. Or use API host:port/api/movies to create Movies Access host:port to see the movies.  Local settings sample Lets say you want to use sqllite instead of psql. You can try like this:# Will reside in ./backend/movie_app/ import os BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.sqlite3\u0026#39;, \u0026#39;NAME\u0026#39;: os.path.join(BASE_DIR, \u0026#39;db.sqlite3\u0026#39;), } } Screenshots Front page at: http://localhost:8000/#!/  Rest API at: http://localhost:8000/api/movies/  Admin Site at: http://localhost:8000/admin/movies/movie/  ","date":"April 29, 2018","link":"/posts/boilerplate-django-with-angularjs-powered-by-webpack2-yarn-docker-drf/","readTime":2,"title":"Boilerplate For Angular 1.X and Django(Powered by Webpack2, Docker, Yarn, DRF)"},{"content":"Ghost had formulated lots changes when they released version 1.XX. So, when you try to upgrade from 0.11 to 1.XX, you can follow the following steps. Table of contents    Steps  Export backup Rename old content Copy to a seperate directory Create cocker compose file Start docker containers Setup ghost admin Import file Change Theme Regarding updating handlebar files   In conclusion   Steps FYI, you can also use the following steps if you want run Ghost in Docker and import your existing settings. Just Skip Step 1, 2, 7, 8.Export backup Run your Ghost server and go to adminsite by localhost:2368/admin or localhost:2368/ghost (assuming its running in 2368 port). Now go to Labs_\u0026gt;_Export Your Content. Click there and export your backup json file.  Rename old content Renamed your content directory as old content and copy your exiting images, themes from that directory and put them in new folder named content.Copy to a seperate directory Copy content directory to a seperate folder where your dockerized ghost files will exist.Create cocker compose file Create a docker-compose.yml file like this:version:\u0026#34;2\u0026#34;services:ghost:image:ghost:latestcontainer_name:g01ports:- \u0026#34;2368:2368\u0026#34;volumes:- ./content/images:/var/lib/ghost/content/images- ./content/themes:/var/lib/ghost/content/themes- ./content/apps:/var/lib/ghost/content/apps- ./content/data:/var/lib/ghost/content/dataHere you can see that your content directory is syncing with Ghost container\u0026rsquo;s content directory through volume sharing. BTW you need to have python and docker-compose in your local machine. Install docker compose by pip install docker-composeStart docker containers Now, run docker-compose up. It will start up your ghost server. Access it via localhost:2368.Setup ghost admin For new Ghost users and those whom are migrating, you need to go adminsite using localhost:2368/admin and setup your ghost admin.Import file This step is also for migration. Go to Labs_\u0026gt;_Import Your Content, then import the json file from Step 2.  Change Theme This step is also for migration only. Go to Design section in admin site. You can see the themes which can be used in here. FYI, if your theme does not appear, don\u0026rsquo;t worry.   Just create a zip file of your particular theme in themes folder. In adminsite\u0026gt;Design section, click on upload a theme. Then try to upload the zip file. It will show errors where the changes needs, fix the errors, and import again.  Regarding updating handlebar files One important thing to mention, if you update any handlebar files in the theme, then you need to restart the docker container to see the change in the frontend. Just run docker-compose stop and docker-compose start.In conclusion Thats it. You have a docker powered ghost blog .You can find a working example here.","date":"April 29, 2018","link":"/posts/migrating-from-ghost-11-to-1-xx-using-docker/","readTime":2,"title":"Migration from Ghost 0.11 to 1.XX Using Docker"},{"content":"Bandarban is a heaven for trekking for Bangladeshi people. There are lots of hills around the district, and there are some amazing waterfall as well. I visited three amazing waterfalls named Nafakhum, AmiaKhum, Satbhaikhum. Khum means waterfall in Marma language. Table of contents    Thanchi Remakri Nafakhum Amiakhum Padma Jhiri Tindu   Thanchi   _captured on the way to Thanchi*Again we went to Bandarban town and from there we took a Chander Gari(A four wheel drive car) to Thanchi Upazila. On the way, you will cross Nilgiri and Peak69, they are two of most popular places in Bandarban.Remakri   _Shangu river at Thanchi Upazila*We hired guide from there, as you can\u0026rsquo;t go beyond this place unless you have a guide. Also to mention, we lost cellphone signal after leaving Thanchi, but it was long time ago.As we went there on an almost rainy season, the Shangu river was full and we took a boat to Remakri. On the way, we saw Boro Pathor(means big stones).  _Tindu*We spent the night in Remakri at a cottage. But you can stay in tent if you want, also you need to bring it yourself.Nafakhum Next day moring, we started our trekking to Nafakhum. We walked through Shangu river bank, crossed many Jhiri poth(the path has been created for regular water flow). Finally we arrived at Nafakhum in 3 hours. If you are on a day trip, then you need to return to Remakri from here.  _Nafakhum*We did not stay long at Nafakhum. So we started our trekking towards Khiyong Para. Its a village where Khiyong people lives. It took us about 4-5 hours to arrive there. Its in top of the hill, have an amazing view. We stayed with a Khiyong family, and enjoyed their food, company and hospitality.Amiakhum Next day moring, we took a local guide with us to go to Amiakhum. We climbed 3 hills and took us about 3 hours to arrive at Amiakhum.  _View from some para which was a pitstop to Amiakhum trekking*Amiakhum is locally known as Hamiakhum. Satbhaikhum is also close to the Amiakhum. You need to cross a small canal to arrive at Satbhaikhum. You will need a Basher Bhela(bamboo boat). Sometimes you can\u0026rsquo;t find bhela, so you can either make a new one or swim across the canal. I crossed the icy cold canal that day, and on the other side, saw amazing view of Satbhaikhum.Also on top of Amiakhum, there is a small lake which sources all the water. We rode around it with a bhela.  _Amiakhum*We returned via Debotar Pahar(now a days people ofter use this route). When we went, it was full of jungle, as we went through, we got stuck by lots of leeches ;). It took us about 2 hours to get back.  _View from the bottom of Amiakhum*Padma Jhiri We stayed the night at Khiyong para, next morning, we started our return journey by Padma Jhiri. On the way, we saw some amazing wild waterfalls. It was raining hard that day, so we walked through water a lot of time. It took us about 5 hours to arrive at Tindu.  _one snap from Padma Jhiri*Tindu From Tindu, we went to Thanchi via a boat. From Thanchi we went to Bandarban via Bus(fare was 300 tk per head that time).So thats it\u0026hellip;. Cheers!!!","date":"April 27, 2018","link":"/posts/bandarban-nafakhum-amiakhum-satbhaikhum/","readTime":3,"title":"[Travel] Bandarban: In Search of Waterfalls"},{"content":"Bandarban is a beautiful district of Bangladesh. You will see lots of amazing mountains which are part of Arakan range. Keokradong is the 3rd highest pick**(after Tajingdong and Saka Haphong, references are here) of Bangladesh, which is situated in Bandarbans. So today I am going to share my experience visiting there. But beware, your experience might differ from mine as I went there a long time ago. Table of contents    Journey begins In Bandarban At Boga lake To Keokaradong   Journey begins So, me and some of my friends went there from Dhaka. We got to Bandarban via bus, but there are other means of transport. Like you can go to Chittagong via train/bus/plane, and catch a bus to Bandarban from there.In Bandarban After arriving to Bandarban, next we had breakfast, then took a 4 wheel drive Toyota car(its also known as Chander Gari) to Ruma. It costs about 5K tk per car. You can also take bus to go to there, costs 300 tk per-head.  _Took this photo on the way to Ruma*By car it takes about 3 hours to arrive at Ruma. From there we took another Chander Gari to arrive at bottom of Boga Lake. You can buy necessary stuff from Ruma Bazar, like masala(which we will mention later), shoes for climbing, anklet, medicine etc.In winter, you can take the Chander Gari to lake\u0026rsquo;s bank directly, but in rainy season, you can\u0026rsquo;t get that close due to bad road condition. So, you might need to start trekking long before.After we arrived at bottom of the hill which surrounds the Boga lake, we purchased some bamboo sticks and started climbing the hill.It took us around 60 minutes.At Boga lake Then we saw a mesmerising beauty, the lake itself. It took away our tiredness of climbing, cheer us up!!  We stayed at one of the cottages near the lake for 100 tk per night. We also ate our dinner there for 100 tk per meal. We also had BBQ chicken, we brought BBQ masala from Ruma Bazar, and chicken from some house in Boga lake. But, if you arrive early in Boga lake, then you can start climbing the Keokradong(which I did in my second trip). But its not recommended if its your first time in there.To Keokaradong Keokradong stands just beside the Boga lake. We started climbing the hill in early morning. We bought some sugar canes before we started our journey. It gave us lots of glucose, which helped us stay fresh for the trekking. Be aware, there are lots leeches in rainy season, so bring salt/tobacco. You can climb at night as well, its super fun and less tiring. It took us about 4 hours to climb up. It may differ due to weather conditions. It was comfortable trekking. On top, it was amazing to see the surrounding. After spending sometime at top, we started the return journey, it took us about 1 hour. Then, we used the same vehicles same as our coming journey.But I spent the night at the rest house on top of the mountain for my second trip. Morning on top of Keokradong is amazing.  _Morning view from Keokradong*So, Thats it\u0026hellip;. Stay tuned for more travel blogs ;).","date":"April 26, 2018","link":"/posts/bandarban-journey-to-keokaradong/","readTime":3,"title":"[Travel] Bandarban: Journey to Keokradong"},{"content":"I went to Bhutan on September, 2015. It was my best trip ever, really ever. From Dhaka, we went there by DRUK AIR. We(me and two of my friends) landed in Paro Airport. It was really wonderful airport, it was small but clean and organised. After getting out of the plan, a chilling air struck us, and instantly we knew, we gonna like this country. Table of contents    Journey starts At Thimpu Towards Punakha Phobjika valley At Bumtang Chelala Pass Again at Thimpu and then to Phuntsholing Back again at Thimpu and then to Paro Itinerary Tips Food People In conclusion    Journey starts After completing immigration, we went out and bought sim cards. Before coming to Bhutan, we contacted a Taxi driver named Mr. Darji. He picked us up from outside of airport and escorted to his car. From there, we went to Thimpu. On the way, we really enjoyed the surroundings. It was fantastic.At Thimpu After arriving Thimpu, we went to Police Station to pick up permission for rest of the journey. It took us about 2-3 hours to get the permission. To me, Thimpu is the cleanest and most beautiful city in South Asia.We were backpackers and wanted a long road journey, so we decided to go to Bumthang by road. That is why we did not stay Thimpu for long that day. Towards Punakha From there we went to Punakha at the evening. On the way, we stopped briefly at Dochula pass. At night, we stayed at Punakha.Next morning, we went to Punakha Dzong. It was a majestic place, really enjoyed looking around. But it would be better if you get a guided tour. Phobjika valley From there, we went to Phobjikha Valley, there we went to see Gangteng Monastery. We took a pit stop at Trongsa. Had lunch, then began our journey again. At night we arrived at Bumthang.At Bumtang  In the morning, we went sight seeing around in the town and outside, visited some Dzong. At late morning, we started our return journey\u0026hellip;  Chelala Pass We also saw Chelala pass on the way. Again at Thimpu and then to Phuntsholing At night we arrived in Thimpu. Next day, we began our journey to Phuntsholing. Phuntsholing is the border city of Bhutan. This city is divided into two, the part in Bhutan is called Phuntsholing, in India, its called Joygaon. We arrived there at afternoon, so got into a hotel, left our bags, and started strolling around the city. Back again at Thimpu and then to Paro Next day, we returned to Thimpu. The day after, we went to Paro. We climbed the hill and visited the famous temple Tiger\u0026rsquo;s Nest in our last day of Bhutan.  In afternoon, we strolled around the city with bicycle with our Bhutanese friend Ugen. We met ugen at bikeshop. He is a cool person, and we had a lot in common. We had a lot of fun that day. And next day, we went back to Bangladesh from Paro via Druk Air. Itinerary  Paro -\u0026gt; Thimpu Thimpu -\u0026gt; Punakha Punakha -\u0026gt; Bumthang Bhumthang -\u0026gt; Thimpu Thimpu -\u0026gt; Phuntsholing -\u0026gt; Thimpu Thimpu -\u0026gt; Paro  Tips  Taxi can be costly in Bhutan. Try to ride via bus. Don\u0026rsquo;t miss Momo(dumplings). Masala Tea is fantastic in Bhutan. Chilly is not that hot. Food is fantastic Its really safe. Don\u0026rsquo;t miss Bumthang. You can go there by Air if you want. Best time to go: April, September, October. But Bhutan is enjoyable all around the year.  I know that, we rushed a lot, missed a lot of good spots, maybe spend some more time in places like Punakha, Bumthang, Thimpu etc.Food We ate a lot of Kawa datshi, Ema datshi, Chilly rice, noodles, Momo, Masala Tea, Masala Omlet etc.People People in Bhutan are really friendly. We found help from random strangers everywhere. We even made friends with them.An example of their helpfulness: On our last day, we had trouble arranging Taxi, as our flight was in early morning. So, at 5am, we went out, there was no taxi in the road. We tried to hitch-hike to airport, but no car was stopping for us. All of a sudden, a car stopped, the door opened, the guy behind the wheel asked if we are going to airport. We said yes. He told us to hop in, and we did(in fear, we are not used to hitch-hike). On the way, he said nothing much. When we arrived at airport, he dropped us. He did not ask for anything, and we were really grateful.In conclusion Bhutan trip was fantastic, we covered 600KM in 3 days ;). Made friends with local people, was amazed by their friendly and helpful behaviour. Majmorized by the natural beauty. Hoping to visit Bhutan again really soon.","date":"April 25, 2018","link":"/posts/journey-to-bhutan/","readTime":4,"title":"[Travel] Journey to the Land of Thunder Dragon"},{"content":"If you want to use OpenShift for deploying Django, you can follow this post and simply do that. You don\u0026rsquo;t need to learn Kubernetes. We will use docker file only. No other fancy stuff.Also, before starting, I am also hoping you are little bit familiar with OpenShift 3 and oc tools. Download oc clients from hereSo let\u0026rsquo;s get started. Table of contents    Contents of this post Deploying database Making django project Creating \u0026lsquo;Dockerfile\u0026rsquo; for django Serving static and media contents Deploying django to OpenShift Allowed hosts in django Uploading images in persistent volume   Contents of this post In the blog, we are going to discuss about how we can deploy production grade django server, using mysql as Database. We are also going to use Gunicorn to deploy Django. We will be using Whitenoise to serve static files. We will use Persistent Volume provided by OpenShift to store media contents. A working example for deploying can be found here: Deploying Django in OpenShift Github RepoNow let\u0026rsquo;s discuss part by part about the deployment.Deploying database First we need to deploy our mysql server to OpenShift. We are going to do that using this command:oc new-app openshift/mysql-55-centos7 It will start building the deployment for mysql. But it will throw error as there is no MYSQL_USER, MYSQL_PASSWORD, MYSQL_DATABASE defined in environment variable. So we can update the deployment by this commands:\u0026gt; oc env dc/mysql-55-centos7 MYSQL_USER=myroot \u0026gt; oc env dc/mysql-55-centos7 MYSQL_PASSWORD=myroot123 \u0026gt; oc env dc/mysql-55-centos7 MYSQL_DATABASE=mydjango You can check the deployment name using the command oc status.Now our MySQL server is ready inside OpenShift and it should look like this: In our django application, we can use internal service name(FYI:It did not work for my local machine) mysql-55-centos7 in DATABASE_SETTINGS or we can use internal IP which we can find in application\u0026gt;pods. Checkout the list of pods: Find out the running Pod of Mysql and when you click on the name, you will go the details page. There you can find the IP like given image given below:  You can update the settings of the Django application like this:DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.mysql\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;mydjango\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;rootadmin\u0026#39;, \u0026#39;PASSWORD\u0026#39;: \u0026#39;rootadmin123\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;172.17.0.4\u0026#39;, # The IP we get from Pods \u0026#39;PORT\u0026#39;: \u0026#39;3306\u0026#39;, } } That\u0026rsquo;s it, our mysql configuration is done.Making django project We need a django application for this deployment right!! So let\u0026rsquo;s create one or if you have one, you can use it(lets hope its not too much complicated). If you have one, then please skip to next section.django-admin.py startproject django-openshift Now inside the project, lets create an app:django-admin.py startapp pictures and add this app to django project:#inside django settings(django-openshift/django-openshift/settings.py) INSTALLED_APPS += [ \u0026#39;pictures\u0026#39; ] Lets update the models.py of pictures:from django.db import models class Pictures(models.Model): image = models.ImageField() and update the admin.pyfrom django.contrib import admin from .models import Picture admin.site.register(Picture) and add a requirements.pip file in the root directory with the given entries:django\u0026gt;=2.0 mysqlclient==1.3.12 pillow==5.0.0 gunicorn==19.7.1 whitenoise==3.3.1 We have created a really simple django application where we can upload photos.Creating \u0026lsquo;Dockerfile\u0026rsquo; for django Now lets create the Dockerfile for django:FROMpython:3.6ENV PYTHONUNBUFFERED 1RUN mkdir /srcWORKDIR/srcADD . /srcRUN pip install -r requirements.pipRUN ./manage.py migrateRUN ./manage.py collectstatic --no-inputCMD gunicorn openshift_django.wsgi -b 0.0.0.0:3013EXPOSE3013In this dockerfile, we are going to do the following stuffs:  We will be using python:3.6 official image.  We are going to install dependencies from requirements.pip.  We are going to run database migrations.  We are going to put gunicorn command which will make the application running in CMD.  We are going to expose 3013 port.  We are going to expose 3013 because it\u0026rsquo;s one of the default port for openshift, we can also use 8080 as well.Serving static and media contents For serving static files, we are going to use whitenoise. Its pretty simple, please go their documentation and check how they configure it in Django.For Media content, we are using django\u0026rsquo;s own media serving mechanism. You can update the urls.py like this:from django.conf.urls import url, re_path from django.views.static import serve from django.conf import settings urlpatterns += [ re_path(r\u0026#39;^media/(?P\u0026lt;path\u0026gt;.*)$\u0026#39;, serve, { \u0026#39;document_root\u0026#39;: settings.MEDIA_ROOT, }), ] So media will be served from url path /media/*Now we need to configure media settings in settings.py.MEDIA_ROOT = \u0026#39;/data/\u0026#39; # Why data? explained in below MEDIA_URL=\u0026#39;/media/\u0026#39; So, it will store media contents in /data directory whenever its uploaded through django.Deploying django to OpenShift There are many ways to deploy to openshift. You can check here for more details. But I can share my ways.Lets say, you have your application in git. So you can deploy it usingoc new-app /path/to/src Then it will push the git configurations to openshift. Then openshift will pull code from git, build the image and start deploying it.Or you can useoc new-build /path/to/src It will push binary to openshift, and openshift will build image from that and start deployment.If you want to rebuild the app, you can useoc start-build ./path/to/src It will start new build.Now, if you want to see status of your deployment from console, you can use oc status command. If you want to expose your app(it will create url and you can access it through that url), then use this command:oc expose svc/\u0026lt;your-service-name\u0026gt; Allowed hosts in django Once deployed, if you try to access the application, you might face errors. And one of the most common error is related ALLOWED_HOSTS. There is several ways to avoid this, like you can putALLOWED_HOSTS=[\u0026#39;*\u0026#39;] It will allow all hosts. OrALLOWED_HOSTS=[ os.environ.get(\u0026#39;OPENSHIFT_DNS\u0026#39;, \u0026#39;*\u0026#39;) ] And you can configure the deployment by adding environment variable like this:oc env svc/\u0026lt;your-service\u0026gt; OPENSHIFT_DNS=\u0026#39;YOUR OPENSHIFT DNS\u0026#39; Uploading images in persistent volume When you try to upload images, You will face permission errors from Django. To avoid that, you can attach storage(Persistent Volume) to your deployment. To do that, create a storage from console like this given image:  Put any name you want, access mode can RWO, size 1 GB or anything you wish.After creating the storage, go to deployment configuration:  In the bottom of the page, there is a button called add storage, click on that. While adding the storage, mount path should be the path where you want to access your media files from in file storage. In our application, its /data like image below: That\u0026rsquo;s about it, you should be able to access your application in exposed url.For more advanced stuff, please checkout OpenShift\u0026rsquo;s Blog about how they deploy Django.Thanks for reading. Cheers!!","date":"February 24, 2018","link":"/posts/deploy-django-to-openshift-3/","readTime":6,"title":"Deploy Django to OpenShift 3 Powered by MySQL and Gunicorn"},{"content":"Visual Studio Code is an editor developed by Microsoft. I have been using this editor for Python development for sometime now. Previously I have been using PyCharm Community Edition for development, but I had to switch to an editor which was less resource consuming than PyCharm, since I have been using VS Code.It was initially suggested to me by one of my colleagues. My first impression was, what is this? Is it really usable? Is it as bad as Atom?(I have a dreadful experience with Atom, although Atom is maybe as good as VS Code). But instead, I found that it is really useful, user-friendly and has lots of useful features.Let us check out how we can use VS Code to develop Production grade Python applications. Table of contents    Go to settings Configuring python  Install python plugin Add virtual environment Using PEP8 and lint   Generic configurations  Format on save Add ruler in editor Ignoring unnecessary files Disable preview Increase zoom level Customizing sidebar and topbar(Or any other visual customization) Themes   Debugging Useful plugins In conclusion   Go to settings You can check VS code\u0026rsquo;s documentation on how to go to User and Workspace Settings. I prefer to use Command Palette for them.To go to Workspace Settings, press CTRL+SHIFT+P(CMD+SHIFT+P for MacOS), it will open up Command Palette, then type Preferences: Open Workspace Settings. After pressing Enter(or click), you will go into Workspace settings(JSON).And to go to User settings,press CTRL+SHIFT+P(CMD+SHIFT+P for MacOS) and type Preferences: Open Settings(JSON).Configuring python For python related configurations, we are going to use Workspace Settings.Install python plugin For using pythonic features, you need to install plugin Python VS Code To do that, go to Extensions Section of VS Code(marked blue in the image given below), in search section, type python; and install the red marked package called Python by Don Jayamanne(Now Maintained by Microsoft). Add virtual environment Here is how to configure the virtual environment:{ \u0026#34;python.pythonPath\u0026#34;: \u0026#34;/path/to/virtualenv/bin/python\u0026#34; } That should be enough to let you use the virtual environment for development. If you want to add your own modules, then add this settings:{ \u0026#34;python.autoComplete.extraPaths\u0026#34;: [\u0026#34;./path-to-your-code\u0026#34;] } After that, let us add some more features which are useful to develop python codes.Using PEP8 and lint To add them to vs code, add the following key values to above dictionary:{ \u0026#34;python.linting.pep8Enabled\u0026#34;: true, \u0026#34;python.linting.pylintPath\u0026#34;: \u0026#34;/path/to/virtualenv/bin/pylint\u0026#34;, \u0026#34;python.linting.pylintArgs\u0026#34;: [\u0026#34;--load-plugins\u0026#34;, \u0026#34;pylint_django\u0026#34;], \u0026#34;python.linting.pylintEnabled\u0026#34;: true } To use the above features, editor will prompt you to install pylint and autopep8, or you can install them directly in virtual environment:pip install autopep8 pip install pylint Generic configurations For generic related configurations, we are going to use User Settings.Format on save Add this like in dict: \u0026quot;editor.formatOnSave\u0026quot;: true It will auto format code (language does not matter).Add ruler in editor Adding rulers in the editor gives you a better idea of how many words you will put on a single line, in Pep8 Standard, it\u0026rsquo;s 79. So let\u0026rsquo;s add the following key and values in the settings dictionary:{ \u0026#34;editor.rulers\u0026#34;: [80, 120] } Ignoring unnecessary files To ignore unnecessary files, add this following lines:{ \u0026#34;files.exclude\u0026#34;: { \u0026#34;**/.git\u0026#34;: true, \u0026#34;**/.svn\u0026#34;: true, \u0026#34;**/.hg\u0026#34;: true, \u0026#34;**/CVS\u0026#34;: true, \u0026#34;**/.DS_Store\u0026#34;: true, \u0026#34;.vscode\u0026#34;: true, \u0026#34;**/*.pyc\u0026#34;: true, \u0026#34;**/__pycache__/\u0026#34;: true } } Disable preview When you open a file using an import file or try to go back to the declaration of the code, vs code intends to open it in a preview window, which sometimes is annoying when you want to do it multiple steps/times. To disable it, add this:{ \u0026#34;workbench.editor.enablePreview\u0026#34;: false } Increase zoom level You can increase the zoom level of the IDE via adding the following settings:{ \u0026#34;window.zoomLevel\u0026#34;: 0.15 } Customizing sidebar and topbar(Or any other visual customization) By default the font size for sidebar and top bar for vscode is not that big. But there are two plugins to handle it.Custom CSS and JS Loader You can use Custom CSS and JS Loader, which allow you to override css classes for VS code itself. For example, I use the following styling:.monaco-tree-row.has-children { color: #eee; font-size: 15px; } .monaco-tree-row { font-weight: 100; font-size: 13px; color: rgb(223, 223, 223); } .monaco-workbench .tabs-container .tab * { font-size: 15px; } .explorer-viewlet { font-size: 15px; color: rgb(223, 223, 223); overflow: auto; } And to add this css in Custom CSS and JS, you need to add the following settings:{ \u0026#34;vscode_custom_css.imports\u0026#34;: [\u0026#34;file:///path/to/css/styling.css\u0026#34;] } Please check their documentation on how to add CSS and JS paths to settings in detail.CustomizeUI CustomizeUI relies on the Monkey Patch Extension to inject custom javascript in VSCode. Here is the settings I use (in settings.json) for my Mac:\u0026#34;customizeUI.stylesheet\u0026#34;: { \u0026#34;.explorer-viewlet .mac\u0026#34;: \u0026#34;font-size: 1.2em !important; overflow: auto; border-left:none!important\u0026#34;, }, It has some minor advantages over Custom CSS and JS Loader. Like:  It won\u0026rsquo;t show Unsupported on the title-bar or won\u0026rsquo;t show any error message like Your installation is corrupted.  You do not have to reload settings every time VS Code is updated.  Themes You can download any theme from the marketplace and install them. Currently I am using the Material theme.Debugging It is really cool to have debugging feature built-in VS Code. Although as far as I tested, it works perfectly fine on Ubuntu, but not in OSX. So please check before you configure it.Anyways, the best way to configure it for Django is to add the following lines in launch.js:{ \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Django\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;stopOnEntry\u0026#34;: true, \u0026#34;pythonPath\u0026#34;: \u0026#34;${config:python.pythonPath}\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceRoot}/manage.py\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceRoot}\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;runserver\u0026#34;, \u0026#34;--noreload\u0026#34;, \u0026#34;--nothreading\u0026#34;], \u0026#34;env\u0026#34;: {}, \u0026#34;envFile\u0026#34;: \u0026#34;${workspaceRoot}/path/to/virtualenv\u0026#34;, \u0026#34;debugOptions\u0026#34;: [ \u0026#34;WaitOnAbnormalExit\u0026#34;, \u0026#34;WaitOnNormalExit\u0026#34;, \u0026#34;RedirectOutput\u0026#34;, \u0026#34;DjangoDebugging\u0026#34; ] } ] } Or go to the Debug section(Marked green in the screenshot), click the section marked as yellow and then click add configuration. Then, click on Django settings to add new Django settings for debugging. Useful plugins VS Code Marketplace is really rich with lots of themes, plugins and so on. You can download and use the following plugins from marketplace to enrich your coding experience. Use Sync to synchronize your settings in between VS Code instances over multiple machines. You can use Git Blame to see Git blames. You can use Git LENS to supercharge git functionality. IntellijIdea Keybindings allows you to use Idea\u0026rsquo;s shortcut keys in VSCode. Use Icons to beautify VS Code\u0026rsquo;s icons. Use Material Icons for using material icons on the sidebar. Use Code Spell Checker to check spells in code.  In conclusion It goes without saying that PyCharm is the best IDE for Python development. It supports refactoring, which makes life a lot easier; has advanced debugging, and its easy to configure. Still, VS code has its own charm. I prefer VS code\u0026rsquo;s UI, its configurations(which is really dynamic and lots of options) and most importantly, I can do front end development better in VS Code, as PyCharm community edition does not support JS. Also its lightweight(not resource hungry as PyCharm).Thanks for reading. Cheers!!","date":"July 26, 2017","link":"/posts/vs-code-for-python-development/","readTime":6,"title":"Use VS Code for Python and Django Development"},{"content":"Chrome apps are really handy when you don\u0026rsquo;t want to overhaul your system with apps for every little purpose. Starting those apps at startup can be used instead of manually starting them every time you want to use them. Like starting the messenger at startup of the system. To do that. Table of contents    Steps In conclusion   Steps  First, go to Ubuntu\u0026rsquo;s search menu and search for startup applications.   Then open that application preference and add new startup application.    Put any name you want in there. Also, in command section, write:  google-chrome --app-id=[app_id] Now you need to have app_id which you can easily find like this:For example, we want to add All in One Messenger, and when you look for it in chrome store, the URL will appear like this: https://chrome.google.com/webstore/detail/all-in-one-messenger/lainlkmlgipednloilifbppmhdocjbda?hl=en So the lainlkmlgipednloilifbppmhdocjbda is the app_id here.Finally, if we want to add this app at startup, first we need to add it to our chrome, then like the previous steps mentioned above, add new startup application with the command: google-chrome --app-id=lainlkmlgipednloilifbppmhdocjbda. That should do the trick and you will see all in one messenger starting up whenever you initially login into the system.In conclusion Its pretty simple and straight forward proccess, and very useful when you have bunch of chrome apps.","date":"July 19, 2017","link":"/posts/ubuntu-protip-launching-chrome-apps-at-startup/","readTime":2,"title":"Ubuntu Hacks: Launching Chrome Apps at Startup"},{"content":"Who hasn\u0026rsquo;t heard PIL? It\u0026rsquo;s an image processing library made by python (Python Image Library). Pillow is an extension of it. Table of contents    Installation Resize image Crop image in box shape Resize image and crop in center to convert it to a fixed size Paste image over a background image Draw text over an image Adjast brightness of an image Adjast color of an image Save the PIL image in django models or serve as django file Convert to webp Convert to JPG/JPEG And Many more   Installation So installing pillow is really easy:pip install pillow Now we are going to do some image processing cool stuff:First Load Image:from PIL import Image, ImageOps img = Image.open(\u0026#39;image.png\u0026#39;) Resize image Suppose We want to resize an image and maintain its aspect ratio. Here we need to get the aspect ratio after resizing. Here is a snippet for it:def get_resize_image_size(image, baseheight=None): \u0026#34;\u0026#34;\u0026#34; Get resize image size \u0026#34;\u0026#34;\u0026#34; size = 1200, 1200 print(\u0026#34;Getting resize image size\u0026#34;) if not baseheight: baseheight = size[1] hpercent = (baseheight / float(image.size[1])) wsize = int((float(image.size[0]) * float(hpercent))) return wsize, baseheight Or we may want to our desired image size.After we get the desired aspect ratio, now we resize it with the following code:def resize_image(img, size=None): \u0026#34;\u0026#34;\u0026#34; Resize Image img: Image file opened by PIL size: if Size not given, it will be calculated \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Resizing image\u0026#34;) if not size: size = get_resize_image_size(img) img = img.resize(size, Image.ANTIALIAS) return img Crop image in box shape When we try to crop, we can either give it positions for cropping or we can calculate the points, which will be used in cropping. Suppose we have a random image, but we want to crop it in middle position and we want to crop in such way that, we will only take out 1200x1200 size out of the sample image, the following is the code:def crop_position(image): \u0026#34;\u0026#34;\u0026#34; Get Crop Image Positions \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Getting Crop Image Positions\u0026#34;) width, height = image.size new_width, new_height = 1200, 1200 left = (width - new_width) / 2 top = (height - new_height) / 2 right = (width + new_width) / 2 bottom = (height + new_height) / 2 return left, top, right, bottom Now we got the position, lets do cropping:def crop_image(img, positions=None): \u0026#34;\u0026#34;\u0026#34; Crop Image in Box Shape \u0026#34;\u0026#34;\u0026#34; if not positions: positions = crop_position(img) return img.crop(box=positions) Resize image and crop in center to convert it to a fixed size Let\u0026rsquo;s take an image in size and convert it to 1200x1200 image:def convert_image(pil_image): \u0026#34;\u0026#34;\u0026#34; Resize Image and Crop in Center to Convert it to a Fixed Size \u0026#34;\u0026#34;\u0026#34; try: if pil_image.size == self.size: print(\u0026#34;Nothing to change, returning image\u0026#34;) return image # Resize to size given in settings pil_image = resize_image(pil_image) # Cropping to adjust size given in settings pil_image = crop_image(pil_image) pil_image.save(path) except (KeyError, Exception) as exp: print(str(exp)) raise exp Paste image over a background image The following code will do the trick:def add_overlay_over_background(background_image, overlay_image, offset=(0, 0)): \u0026#34;\u0026#34;\u0026#34; Add overlay image over background image \u0026#34;\u0026#34;\u0026#34; background_image.paste(overlay_image, offset, mask=overlay_image) return background_image Draw text over an image For that, we first we need fonts:path = \u0026#39;path/to/font\u0026#39; def get_font(size): \u0026#34;\u0026#34;\u0026#34; Get Font Object \u0026#34;\u0026#34;\u0026#34; return ImageFont.truetype(path, size=size) Then we need to co-ordinates where the texts will be written:Here are two scenarios: You provide the exact positions. You can provide the height of the text position, we will calculate where the texts will be printed in the image. Here we are assuming that texts will be aligned in middle horizontally. It will be symmetric in perspective middle vertical line.  def process_text_coordination(position, font=None, vertical_only=False, text=None): \u0026#34;\u0026#34;\u0026#34; Process Co-Ordination of the position \u0026#34;\u0026#34;\u0026#34; if vertical_only: # Case Two size = font.getsize(text) width = (self.size[0] - size[0]) / 2 return width, position # Case One return position Now we got everything, so let\u0026rsquo;s do writing text on image:def write_text_on_image(image, font, text,position, color, size, vertical_only=False): \u0026#34;\u0026#34;\u0026#34; Write Text over Image \u0026#34;\u0026#34;\u0026#34; draw = ImageDraw.Draw(image) font = get_font(size) position = process_text_coordination( position, font, vertical_only, text ) draw.text(position, text, font=font, fill=color) return image BTW, here color can be hexa value of the color you want to use. For white, you need use: #fffffAdjast brightness of an image Let\u0026rsquo;s lighten or darken image based on amount:def darken_or_lighten_pixels(image, amount=0.5): \u0026#34;\u0026#34;\u0026#34; Enhance Image \u0026#34;\u0026#34;\u0026#34; converter = ImageEnhance.Brightness(image) return converter.enhance(amount) Adjast color of an image Let\u0026rsquo;s adjust color of image based on amountdef add_color_saturation(image, amount=0.5): \u0026#34;\u0026#34;\u0026#34; Color saturation \u0026#34;\u0026#34;\u0026#34; converter = ImageEnhance.Color(image) return converter.enhance(amount) Save the PIL image in django models or serve as django file This code will convert PIL Images to Content file:def process_django_file(pil_image, name, format=\u0026#39;png\u0026#39;): \u0026#34;\u0026#34;\u0026#34; Process the PIL file to Django File \u0026#34;\u0026#34;\u0026#34; file_object = BytesIO() pil_image.save(file_object, format=format) content = file_object.getvalue() return ContentFile(content, name=name) And saving image will be damn easy:pil_image = Image.open(\u0026#39;image.png\u0026#39;) content_file = process_django_file(pil_image, name=\u0026#34;some name\u0026#34;, format=\u0026#39;png\u0026#39;) my_model.image = content_file my_model.save() Put watermark on your image by PILService object\u0026rsquo;s water_mark_on_image method:img= Image.open(\u0026#39;foobar.jpg\u0026#39;) pil_service = PILService() pil_service.water_mark_on_image( img, \u0026#34;Text\u0026#34;, \u0026#34;20\u0026#34;, # percent of height \u0026#34;font/path\u0026#34;, opacity=50, margin=10 ) Convert to webp For this feature you need to install.jpg driver in your machine. For mac use you need to run the following(using homebrew):brew install webp Or you can install libwebp in Unix/Linux.Then you need to re-install pillow. Now, run this command to convert all the files to *.webp format in a certain directory.DIR = \u0026#34;Path/To/Images\u0026#34; for root, dirs, files in os.walk(DIR): for file in files: infile = os.path.join(root, file) name, ext = os.path.splitext(file) try: im = Image.open(infile) im.save(os.path.join(root, name) + \u0026#34;.webp\u0026#34;, \u0026#34;WEBP\u0026#34;) os.remove(infile) # comment out this line if you want to keep original images except Exception as e: print(e) Convert to JPG/JPEG Before converting to JPG/JPEG, you need to know that JPG is a lossy compression, and it can\u0026rsquo;t convert RGBA(Red, Green, Blue, Alpha). Meaning it can\u0026rsquo;t convert Alpha. So you can try like this:im = Image.open(infile) im = im.convert(\u0026#39;RGB\u0026#39;) # convert RGB im.save(os.path.join(root, name) + \u0026#34;.jpg\u0026#34;, \u0026#34;JPEG\u0026#34;) But above code will convert any transparent background to black. So instead you can use a white background to replace transparent part and then convert to RGB:im = Image.open(infile) new_image = Image.new(\u0026#34;RGBA\u0026#34;, im.size, \u0026#34;WHITE\u0026#34;) # White Image new_image.paste(im, (0, 0), im) # Setting as background im = new_image.convert(\u0026#39;RGB\u0026#39;) im.save(os.path.join(root, name) + \u0026#34;.jpg\u0026#34;, \u0026#34;JPEG\u0026#34;) And Many more Many more methods are given in this Repo: https://github.com/ruddra/play-with-pillowHope it was helpful. Cheers!!!","date":"May 28, 2017","link":"/posts/play-with-pillow/","readTime":5,"title":"Play With Pillow"},{"content":" This article is deprecated as PhantomJs has been deprecated from Selenium as driver. PhantomJS is a headless WebKit script-able with a JavaScript API. It has fast and native support for various web standards: DOM handling, CSS selector, JSON, Canvas, and SVG. And Selenium is a portable software-testing framework for web applications. Selenium provides a record/playback tool for authoring tests without the need to learn a test scripting language (Selenium IDE)Using the combination of selenium and PhantomJs can give you a way to capture screenshots and use it in your choices. Table of contents    Installation of PhantomJs Installation of Selenium Using with Django   Installation of PhantomJs For that let\u0026rsquo;s install PhantomJs in your computer. For Ubuntu/Debian platform, you can use like this:\u0026gt;sudo apt-get install phantomjs Or,\u0026gt;sudo apt-get update \u0026gt;sudo apt-get install build-essential chrpath libssl-dev libxft-dev \u0026gt;sudo apt-get install libfreetype6 libfreetype6-dev \u0026gt;sudo apt-get install libfontconfig1 libfontconfig1-dev \u0026gt;cd ~ \u0026gt;wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2 \u0026gt;sudo tar xvjf phantomjs-2.1.1-linux-x86_64.tar.bz2 \u0026gt;sudo mv phantomjs-2.1.1-linux-x86_64 /usr/local/share \u0026gt;sudo ln -sf /usr/local/share/phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin Or use npm:\u0026gt;npm install phantomjs2 Installation of Selenium Install selenium by using pip install seleniumLet\u0026rsquo;s go to coding:from selenium import webdriver # Import selenium web driver driver = webdriver.PhantomJs() # PhantomJs webdriver driver.get(\u0026#39;https://google.com\u0026#39;) driver.save_screenshot(\u0026#39;google.jpg\u0026#39;) Done! your image will be saved as google.jpg.Using with Django Suppose you want to store the image in Django ImageFile, you can use the following code:screenshot = driver.screenshot_as_base64() # binary image my_model = MyModel() my_model.image_field = ContentFile(screenshot, \u0026#39;google.jpg\u0026#39;) MyModel.save() That\u0026rsquo;s all for now. Cheers!!","date":"March 21, 2017","link":"/posts/python-selenium-with-phantomjs-to-capture-screenshots/","readTime":2,"title":"Python: Selenium with PhantomJs to capture Screenshots"},{"content":" This is no longer maintained. Use it at your own risk. Django Encrypt File is a simple Library which can be used to encrypt uploaded files and store them. Table of contents    Installation Basic usage Using in the view Advance example(with models)  Model defination Encrypt view Decrypt view   Notes   Installation Use pip to install it:pip install djangoencryptfile Orpip install https://github.com/ruddra/django-encrypt-file/tarball/0.7 Basic usage from djangoencryptfile import EncryptionService from django.core.files import File password = \u0026#39;1234\u0026#39; service = EncryptionService(raise_exception=False) open(\u0026#39;readme.md\u0026#39;, \u0026#39;rb\u0026#39;) as inputfile: usefile = File(inputfile, name=\u0026#39;readme.md\u0026#39;) encrypted_file = service.encrypt_file(useFile, password, extension=\u0026#39;enc\u0026#39;) # it will save readme.md.enc decrypt_file = service.decrypt_file(encrypted_file, password, extension=\u0026#39;.enc\u0026#39;) # it will remove .enc extension Using in the view from django_encrypt_file import EncryptionService, ValidationError def some_view(request): try: myfile = request.FILES.get(\u0026#39;myfile\u0026#39;, None) password = request.POST.get(\u0026#39;password\u0026#39;, None) encrypted_file = EncryptionService().encrypt_file(myfile, password, extension=\u0026#39;enc\u0026#39;) decrypt_file = service.decrypt_file(encrypted_file, password, extension=\u0026#39;enc\u0026#39;) # it will remove .enc extension except ValidationError as e: print(e) Advance example(with models) Model defination class MyModel(models.Model): upload_file = models.FileField( upload_to=\u0026#39;tuploaded_file/%Y/%m/%d\u0026#39; ) Encrypt view from django_encrypt_file import EncryptionService, ValidationError def encrypt_view(request): try: myfile = request.FILES.get(\u0026#39;myfile\u0026#39;, None) password = request.POST.get(\u0026#39;password\u0026#39;, None) encrypted_file = EncryptionService().encrypt_file(myfile, password, extension=\u0026#39;enc\u0026#39;) mymodel = MyModel.objects.create(uploaded_file=encrypted_file) except ValidationError as e: print(e) Decrypt view from django_encrypt_file import EncryptionService, ValidationError def decrypt_view(request): try: my_object = MyModel.objects.get(pk=1) myfile = my_object.uploaded_file password = request.POST.get(\u0026#39;password\u0026#39;, None) decrypt_file = service.decrypt_file(encrypted_file, password, extension=\u0026#39;enc\u0026#39;) my_object.uploaded_file = decrypt_file my_object.save() except ValidationError as e: print(e) Notes Input file here can be any kind of Django File Object like models.FileField or forms.FileFiled. raise_exception = True will throw ValidationError error which can be imported from django_encrypt_file import ValidationError.","date":"February 24, 2017","link":"/posts/documentation-of-django-encrypt-file/","readTime":2,"title":"Documentation Of Django Encrypt File"},{"content":"In previous two posts, we have deployed Django with Postgres, Nginx, now its time to do some async stuff using Celery. In this post, I will do the magic tricks first, explain them later. Table of contents    Add \u0026lsquo;Celery\u0026rsquo; to django Make a simple async task Explanations Need proof that this works   Add \u0026lsquo;Celery\u0026rsquo; to django To add celery, we need make a container for celery. We can re-use the Dockerfile of django for making celery\u0026rsquo;s container like this:FROMpython:3.6ENV PYTHONUNBUFFERED 1ENV C_FORCE_ROOT trueRUN mkdir /srcRUN mkdir /staticWORKDIR/srcADD ./src /srcRUN pip install -r requirements.pipCMD python manage.py collectstatic --no-input;python manage.py migrate; gunicorn mydjango.wsgi -b 0.0.0.0:8000 \u0026amp; celery worker --app=myapp.tasks** FYI We are avoiding Python 3.7 or **latest** docker image, because celery conflicts with Python\u0026rsquo;s async api(reference).Now lets update the docker-compose.yml file to link django with celery, also link redis container to celery.version:\u0026#34;2\u0026#34;services:nginx:image:nginx:latestcontainer_name:nz01ports:- \u0026#34;8000:8000\u0026#34;volumes:- ./src:/src- ./config/nginx:/etc/nginx/conf.ddepends_on:- webweb:build:.container_name:dz01depends_on:- dbvolumes:- ./src:/srcexpose:- \u0026#34;8000\u0026#34;links:- redisdb:image:postgres:latestcontainer_name:pz01redis:image:redis:latestcontainer_name:rz01ports:- \u0026#34;6379:6379\u0026#34;Now in django project, lets add broker url for celery in settings.py:CELERY_BROKER_URL = \u0026#39;redis://redis:6379/0\u0026#39; CELERY_RESULT_BACKEND = \u0026#39;redis://redis:6379/0\u0026#39; Integration with celery from django is complete. You don\u0026rsquo;t need to read the next section if you already have celery tasks.Make a simple async task How to use celery from django? There is an amazing documentation about that on celery\u0026rsquo;s own documentation page.To make this post post short, I will not get to details, but do basic stuff for making a simple async task using celery.Lets add a celery.py inside mydjango\u0026gt;mydjango directory.from __future__ import absolute_import, unicode_literals import os from celery import Celery import logging logger = logging.getLogger(\u0026#34;Celery\u0026#34;) os.environ.setdefault(\u0026#39;DJANGO_SETTINGS_MODULE\u0026#39;, \u0026#39;mydjango.settings\u0026#39;) app = Celery(\u0026#39;mydjango\u0026#39;) app.config_from_object(\u0026#39;django.conf:settings\u0026#39;, namespace=\u0026#39;CELERY\u0026#39;) app.autodiscover_tasks() @app.task(bind=True) def debug_task(self): print(\u0026#39;Request: {0!r}\u0026#39;.format(self.request)) And create a new app named myapp, add it to django settings:INSTALLED_APPS = [ \u0026#39;django.contrib.admin\u0026#39;, \u0026#39;django.contrib.auth\u0026#39;, \u0026#39;django.contrib.contenttypes\u0026#39;, \u0026#39;django.contrib.sessions\u0026#39;, \u0026#39;django.contrib.messages\u0026#39;, \u0026#39;django.contrib.staticfiles\u0026#39;, \u0026#39;myapp.apps.MyappConfig\u0026#39;, \u0026#39;celery\u0026#39;, # Don\u0026#39;t forget to add celery ] Now we need to install celery and redis by using:pip install celery pip install redis or we can add them to config\u0026gt;requirements.pip.Now lets add a simple email sending task in src\u0026gt;mydjango\u0026gt;myapp\u0026gt;tasks.pyfrom __future__ import absolute_import, unicode_literals import logging from django.conf import settings from mydjango.celery import app logger = logging.getLogger(\u0026#34;celery\u0026#34;) @app.task def show_hello_world(): logger.info(\u0026#34;-\u0026#34;*25) logger.info(\u0026#34;Printing Hello from Celery\u0026#34;) logger.info(\u0026#34;-\u0026#34;*25) Thats it, that should do the trick. Now just run docker-compose build and docker-compose run to make the project running. If you do then you should see an output like this: Explanations In first step, we have updated Dockerfile which was responsible for building django application\u0026rsquo;s environment.FROMpython:3.6ENV PYTHONUNBUFFERED 1ENV C_FORCE_ROOT trueRUN mkdir /srcRUN mkdir /staticWORKDIR/srcADD ./src /srcRUN pip install -r requirements.pipCMD python manage.py collectstatic --no-input;python manage.py migrate; gunicorn mydjango.wsgi -b 0.0.0.0:8000 \u0026amp; celery worker --app=myapp.tasksHere we are using python 3.6 image and inside it creating src \u0026amp; static directory. After that, we have added local src directory to docker\u0026rsquo;s src, and made it working directory(whenever you go into docker, you will be directly inside src folder). Then we installed the requirements. Finally, the CMD command runs collect static, migration, gunicorn and in the end creates celery workers.In docker-compose.yml, we have are adding nothing new from last step.Now the new celery will be running in the old django container.Celery will run this command: celery worker --app=myapp.tasks, which will execute tasks within an app named myapp.Need proof that this works Go to this github link and pull and build. Don\u0026rsquo;t forget to update email configurations inside the settings of django.Checkout previous posts about docker: Deploy Django, Gunicorn, NGINX, Postgresql using Docker Serve Static Files by Nginx from Django using Docker  ","date":"November 14, 2016","link":"/posts/docker-do-stuff-using-celery-using-redis-as-broker/","readTime":3,"title":"Docker: Use Celery in Django(Redis as Broker)"},{"content":"This is more of a follow up post of my previous blog.Before I start, I am assuming you have successfully deployed django using docker and nginx, but having some problems serving static files. Table of contents    Steps Update   Steps No worries, it is easy. Just follow these steps:1. In your django settings.py file, add static file directory i.e. STATIC_ROOT=/static. So what it will do is, when you run collectstatic command(python manage.py collectstatic), it will store the static files in your /static directory of OS.2. Now in docker-compose.yml folder, lets add a configuration like this:version:\u0026#34;2\u0026#34;services:nginx:image:nginx:latestcontainer_name:NGINXDOCKERNAMEports:- \u0026#34;8000:8000\u0026#34;volumes:- ./src:/src- ./config/nginx:/etc/nginx/conf.d- /static:/static\u0026lt;--- HEREdepends_on:- webweb:build:.container_name:DJANGOXDOCKERNAMEcommand:bash-c\u0026#34;python manage.py makemigrations \u0026amp;\u0026amp; python manage.py migrate \u0026amp;\u0026amp; gunicorn mydjango.wsgi -b 0.0.0.0:8000\u0026#34;depends_on:- dbvolumes:- ./src:/src- /static:/static\u0026lt;---- HEREexpose:- \u0026#34;8000\u0026#34;db:image:postgres:latestcontainer_name:PSQLDOCKERNAMEFYI the command argument of the docker compose is same as CMD of Dockerfile. So we can move this inside Dockerfile if we want to(i.e.: CMD python manage.py makemigrations;python manage.py migrate;gunicorn mydjango.wsgi -b 0.0.0.0:8000).What it will do is that, two containers web and nginx will share a directory named /static.3. Now lets add few lines in nginx\u0026rsquo;s config file, i.e mydjango.conf:upstream web { ip_hash; server web:8000; } server { location /static/ { autoindex on; alias /static/; } location / { proxy_pass http://web/; } listen 8000; server_name localhost; } So what it will do is, any request to url like yourhost:yourport/static/* this comes to nginx, it will serve data from /static directory.4. Now lets run the following command:docker exec DOCKERNAME /bin/sh -c \u0026#34;python manage.py collectstatic --noinput\u0026#34; Or update the command key\u0026rsquo;s value in compose to:command:bash-c\u0026#34;python manage.py collectstatic --no-input \u0026amp;\u0026amp; python manage.py makemigrations \u0026amp;\u0026amp; python manage.py migrate \u0026amp;\u0026amp; gunicorn mydjango.wsgi -b 0.0.0.0:8000\u0026#34;It will put static files in /static directory and thats should do the trick. Whenever you hit url with /static will serve static files from that folder. Similarly you can serve media files too. Codes have been updated here at: https://github.com/ruddra/docker-django Cheers!! Update If you are interested to run distributed tasks using celery in Docker with Django, then please read this post.","date":"November 2, 2016","link":"/posts/serve-static-files-by-nginx-from-django-using-docker/","readTime":2,"title":"Serve Static Files by Nginx from Django using Docker"},{"content":"This post mainly based on this blog: https://docs.docker.com/compose/django/.I will be extending this post by serving django+gunicorn using Nginx, also I will using Postgresql docker container to use it as database. Table of contents    Steps  Create django project Add \u0026lsquo;requirement.pip\u0026rsquo; Create \u0026lsquo;Dockerfile\u0026rsquo; Install \u0026lsquo;docker-compose\u0026rsquo; Add configuration to \u0026lsquo;docker-compose.yml\u0026rsquo; Create NGINX config Database configuration Almost done Start and stop docker compose Shell access   Example source code In conclusion Update   Steps Let us checkout to the following steps.Create django project Let\u0026rsquo;s make an empty directory named myproject and add another folder inside name it src. src should contain the django project. For testing purpose lets put a simple django project inside named mydjango.Add \u0026lsquo;requirement.pip\u0026rsquo; Let\u0026rsquo;s create a subdirectory inside myproject and name it config. Lets put a requirement.pip file inside config and write these line in it:Django==1.10 gunicorn==19.6.0 psycopg2==2.6.2 Create \u0026lsquo;Dockerfile\u0026rsquo; Now let\u0026rsquo;s make a Dockerfile inside the myproject. This should contain the following lines:FROMpython:3.6ENV PYTHONUNBUFFERED 1RUN mkdir /configADD /config/requirements.pip /config/RUN pip install -r /config/requirements.pipRUN mkdir /src;WORKDIR/srcSo this Dockerfile starts with a Python 3.5 based image. Then the container is modified by adding the requirement.pip file in /config directory within the container and installing the packages from it.Install \u0026lsquo;docker-compose\u0026rsquo; Let\u0026rsquo;s create a file called docker-compose.yml in myproject directory.The docker-compose.yml file describes the services that make your app. Here we need a web service(Django+Gunicorn), A database(Postgres), and Proxy Server(Nginx). It also describes which Docker images these services will use, how they will link together, any volumes they might need mounted inside the containers. Finally, the docker-compose.yml file describes which ports these services expose. See the docker-compose.yml reference for more information on how this file works. Don\u0026rsquo;t forget to add docker-compose to your python environment by running pip install docker-compose.Add configuration to \u0026lsquo;docker-compose.yml\u0026rsquo; Let\u0026rsquo;s add the following configuration to the docker-compose.yml file:version:\u0026#34;2\u0026#34;services:nginx:image:nginx:latestcontainer_name:ng01ports:- \u0026#34;8000:8000\u0026#34;volumes:- ./src:/src- ./config/nginx:/etc/nginx/conf.ddepends_on:- webweb:build:.container_name:dg01command:bash-c\u0026#34;python manage.py makemigrations \u0026amp;\u0026amp; python manage.py migrate \u0026amp;\u0026amp; gunicorn mydjango.wsgi -b 0.0.0.0:8000\u0026#34;depends_on:- dbvolumes:- ./src:/srcexpose:- \u0026#34;8000\u0026#34;db:image:postgres:latestcontainer_name:ps01It says that there are three services for this project: nginx, web, db. nginx depends on web, web depends on db. db container uses postgres\u0026rsquo;s latest image from dockerhub. Default username for db is postgres and password is postgres web container is build using project\u0026rsquo;s Dockerfile. It mounts src directory into it and exposes port 8000. version is being used for which format to use to compose the docker file.nginx uses nginx\u0026rsquo;s latest image from dockerhub. This proxy server is accessible from port 8000. It mounts src and config directory.Create NGINX config Now let\u0026rsquo;s write a nginx configuration config file named mydjango.conf inside myproject\u0026lsquo;s config folder and put it in a subdirectory named nginx.upstream web { ip_hash; server web:8000; } # portal server { location / { proxy_pass http://web/; } listen 8000; server_name localhost; } So what it does that, nginx acts as a reverse proxy for any connections going to django server and all connections goes through nginx to reach django server.Project Directory should look like this: myproject  src   mydjango   manage.py  config   requirements.pip   nginx   mydjango.conf  Dockerfile  docker-compose.yml Database configuration To communicate from django to postgres, we need to put database configuration in django applications settings file. It should look like this:DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.postgresql_psycopg2\u0026#39;, \u0026#39;NAME\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;USER\u0026#39;: \u0026#39;postgres\u0026#39;, \u0026#39;HOST\u0026#39;: \u0026#39;db\u0026#39;, \u0026#39;PORT\u0026#39;: 5432, } } Almost done All is done. Now lets run docker-compose build in terminal within the project directory. It will build/rebuild(if necessary) all the containers. For first time running the containers, run docker-compose up -d. Lets go to browser and type: localhost:8000. We should see the django application up and running.Start and stop docker compose For stopping the docker, run docker-compose stop. Re-running docker, use docker-compose start.Shell access For shell accessing.#Nginx docker exec -ti nginx bash #Web docker exec -ti web bash #Database docker exec -ti db bash For logs:#Nginx docker-compose logs nginx #Web docker-compose logs web #DB docker-compose logs db Thats it.Example source code You can see an working example here in my repo: https://github.com/ruddra/docker-djangoAlso another deployment example for Ruby on rails here: https://github.com/ruddra/deploy-notebook (Thanks to Akimul Islam for the source)In conclusion Thats all for now. Thanks for reading. If you have any question, please post it in comment section below.Cheers!! Update Serving django with gunicorn won\u0026rsquo;t allow you to serve static files with it. You need to serve static files seperately. You can follow this post for how to do serve static files using Nginx from docker.","date":"August 9, 2016","link":"/posts/docker-django-nginx-postgres/","readTime":4,"title":"Deploy Django, Gunicorn, NGINX, Postgresql using Docker"},{"content":"Today we are going to see how make OSX notifications from RSS Feed of any website using Python.So first of all, let us see how to make a simple notification. We are going to use AppleScript for this. Table of contents    Command for notification in AppleScript Start coding for application Integrate AppleScript in application In conclusion   Command for notification in AppleScript Go to terminal and Type:\u0026gt;\u0026gt;osascript -e \u0026#39;display notification \u0026#34;Lorem ipsum dolor sit amet\u0026#34; with title \u0026#34;Title\u0026#34;\u0026#39; We shall see a notification popping up in right up corner like this: Now, we will use this command to pop up notifications using python from rss feeds.Start coding for application We are going to use feedparser library to get feed from a url. For example:import feedparser feeds = feedparser.parse(\u0026#39;https://feedity.com/ruddra-com/VFZQWlFU.rss\u0026#39;) print([(x.title, x.id) for x in feeds.entries]) It will return a list of tuples containing urls and titles from the feed.Now we will make an infinite loop and inside the loop we will continuously call the rss feed (will also pause for few moments, no worries) and show notifications:while True: d = feedparser.parse(rss_url) for ds in d.entries: print (d.id, d.title) time.sleep(5) Integrate AppleScript in application Now lets build the command for applescript which will put pop ups.apple_cmd = \u0026#34;osascript -e \u0026#39;{0}\u0026#39;\u0026#34; for ds in d.entries: base_cmd = \u0026#39;display notification \u0026#34;{0}\u0026#34; with title \u0026#34;{1}\u0026#34;\u0026#39;.format(ds.title, \u0026#34;Foobar\u0026#34;) apple_cmd.format(base_cmd) Thats all we need. Now if we print the apple_cmd, we shall see commands like this:osascript -e \u0026#39;display notification \u0026#34;Foobar Title\u0026#34; with title \u0026#34;Foobar\u0026#34;\u0026#39; Now using python\u0026rsquo;s subprocessor module, we will call these commands:import subprocessor subprocessor.Popen([apple_cmd], shell=True) Also there is one more thing, we need to remove duplicate entries from feed, we don\u0026rsquo;t want to see same notifications twice, or see old notifications on and on and on. So, we will check if the feed is updated like this:update_time = d.feed.updated We will verify if update_time is greater than previous iteration in while loop. Also, we will store feed id in a list so that we can check if new feed has ids which was already stored in previous iteration.Finally, the code will look like this:last_updated_time = None while True: d = feedparser.parse(rss_url) updated_time = d.feed.updated if updated_time == last_updated_time: print(\u0026#39;No new feed\u0026#39;) else: last_updated_time = updated_time for entry in d.entries: _id = entry.id if _id in dup_ids: print(\u0026#39;Entry already exists\u0026#39;) else: dup_ids.append(_id) base_cmd = \u0026#39;display notification \u0026#34;{0}\u0026#34; with title \u0026#34;{1}\u0026#34;\u0026#39;.format(_notification, rss_title) cmd = apple_cmd.format(base_cmd) subprocess.Popen([cmd], shell=True) time.sleep(5) time.sleep(5) In conclusion Thats it, we will see popups of new notifications if feed is updated.The full working code is here: https://github.com/ruddra/AppleFeedNotifierCheers!","date":"January 26, 2016","link":"/posts/make-apple-notifications-from-rss-feed-using-python/","readTime":3,"title":"Make MacOS Notifications From RSS Feed Using Python"},{"content":"In this blog, I am going to show how to develop an application by which we can make .csv files from MySQL database using Python. Table of contents    Steps SQL command Run SQL command in python To csv In conclusion   Steps SQL command Now, let us start by getting data from Database. Lets say we have a DB names csv_test and it contains a table named test with fields id and name, and for testing purpose let us have some values inserted into it like this:+------+--------+ | id | name | +------+--------+ | 1 | test_1 | | 2 | test_2 | | 3 | test_3 | +------+--------+ Now, to get all data from this table, the SQL command should be:SELECT * FROM csv_test.test; Run SQL command in python We shall run this vary command using python like this:import subprocess try: sql_cmd = \u0026#34;mysql -uroot -proot -e SELECT * FROM csv_test.test;\u0026#34; results = subprocess.check_output( [sql_cmd], shell=True) print(results) except Exception as e: print(e) If we run the above command, we shell find a binary string like this:id\tname 1\ttest_1 2\ttest_2 3\ttest_3 Actual output will be something like this:b\u0026#39;id\\tname\\n1\\t\\test_1\\n2\\ttest_2\\n3\\ttest_3\u0026#39; Now we will convert this binary string to unicode string by running this command:data = results.decode(\u0026#39;utf-8\u0026#39;) And the data will be like this:\u0026#39;id\\tname\\n1\\t\\test_1\\n2\\ttest_2\\n3\\ttest_3\u0026#39; So, now we have the MySQL data as string. Now we shall start making .csv file out of it.To csv First, lets make an empty .csv file, ie:filename = \u0026#39;report_{0}.csv\u0026#39;.format(datetime.datetime.now().strftime(\u0026#39;%d-%m-%Y:%H:%M\u0026#39;)) csv_out = open(filename, \u0026#39;w\u0026#39;) From the data string, we will start making .csv file\u0026rsquo;s contents. Let us split the string in new lines(\\n) and loop through that list, also replace tabs(\\t) with comma(,):csv_data = \u0026#39;\u0026#39; for item in data.split(`\\n`): csv_data += \u0026#39;{0}\\r\\n\u0026#39;.format(item.replace(\u0026#39;\\t\u0026#39;, \u0026#39;,\u0026#39;)) Now we write that csv_data variable to csv_out(empty .csv file) like this:csv_out.write(csv_data) csv_out.close() Thats it, our work is done. We shall see a .csv file named REPORT_DD-MM-YYYY:HH:MM:SS.csv in the project directory.In conclusion Special Thanks to this post: https://redmoses.me/flask-and-shell/ about using MySQL and Flask togatherSource is here: https://github.com/ruddra/mysql-python-csv","date":"January 10, 2016","link":"/posts/make-csv-file-using-mysql-and-python/","readTime":2,"title":"Make .CSV file using MySQL and Python"},{"content":" This post is deprecated. Use it at your own risk. Today I am going to share how to use Scrapy and Django together to crawl to a website and store scraped data to Database using Django. Table of contents    Project setup  Django Scrapy   Screenshots   Project setup Django First, let us build a Django application using the following commands.pip install django==1.7 django-admin.py startproject example_project cd example_project Inside the example_project directory, we will create a django app named app:python manage.py startapp app Then, we will update the models.py like this:from django.db import models class ExampleDotCom(models.Model): title = models.CharField(max_length=255) description = models.CharField(max_length=255) def __str__(self): return self.title Now we shall update the admin.py inside the app directory:from django.contrib import admin from app.models import ExampleDotCom admin.site.register(ExampleDotCom) Update INSTALLED_APPS of settings.py like:INSTALLED_APPS += (\u0026#39;app\u0026#39;,) Now, we will run the following commands in project directory:python manage.py makemigrations python manage.py migrate python manage.py createsuperuser The last command will prompt to create a super user for the application. Now we will run the following command:python manage.py runserver It will start the django application.Django part is complete for now. Lets start the scrapy project.Scrapy In separate directory, we will create a scrapy project using the following commands:pip install Scrapy==1.0.3 scrapy startproject example_bot To use with Django application from scrapy application, we shall update its settings.py inside example_bot project directory:import os import sys DJANGO_PROJECT_PATH = \u0026#39;YOUR/PATH/TO/DJANGO/PROJECT\u0026#39; DJANGO_SETTINGS_MODULE = \u0026#39;example_project.settings\u0026#39; sys.path.insert(0, DJANGO_PROJECT_PATH) os.environ[\u0026#39;DJANGO_SETTINGS_MODULE\u0026#39;] = DJANGO_SETTINGS_MODULE BOT_NAME = \u0026#39;example_bot\u0026#39; SPIDER_MODULES = [\u0026#39;example_bot.spiders\u0026#39;] To connect with django model, we need to install DjangoItem like this:pip install scrapy-djangoitem==1.0.0 Inside example_bot directory, we will update the items.py file like this:from scrapy_djangoitem import DjangoItem from app.models import ExampleDotCom class ExampleDotComItem(DjangoItem): django_model = ExampleDotCom Now we will create a crawl spider named example.py inside spiders directory:from scrapy.spiders import BaseSpider from example_bot.items import ExampleDotComItem class ExampleSpider(BaseSpider): name = \u0026#34;example\u0026#34; allowed_domains = [\u0026#34;example.com\u0026#34;] start_urls = [\u0026#39;http://www.example.com/\u0026#39;] def parse(self, response): title = response.xpath(\u0026#39;//title/text()\u0026#39;).extract()[0] description = response.xpath(\u0026#39;//body/div/p/text()\u0026#39;).extract()[0] return ExampleDotComItem(title=title, description=description) Now we shall create an pipeline class like this inside pipelines.py:class ExPipeline(object): def process_item(self, item, spider): item.save() return item Now we need to update the settings.py with this:ITEM_PIPELINES = { \u0026#39;example_bot.pipelines.ExPipeline\u0026#39;: 1000, } Project structure will be like this: django1.7+scrapy   example_bot    __init__.py    items.py    pipelines.py    settings.py    spiders    __init__.py    example.py   scrapy.cfg  example_project  manage.py  app   __init__.py   models.py   admin.py   views.py  example_project  __init__.py  settings.py  urls.py Now we shall run the application using the following command:scrapy crawl example Now let us return to the running django application. If everything above is done correctly, then we shall see an object of ExampleDotCom class has been created like the below screenshot in this url http://localhost:8000/admin/app/exampledotcom/:Screenshots   Thats all. Up and running django 1.7 + Scrapy project.Drawbacks: Only implemented using django 1.7Source: https://github.com/ruddra/django1.7-scrapy1.0.3Got help and clues from this SO link.","date":"January 3, 2016","link":"/posts/django-1-7-scrapy/","readTime":3,"title":"Django 1.7 and Scrapy"},{"content":" This post is now deprecated. Please follow the official tutorial for creating your first django application. From previous post, you have configured and ran django, also added admin site to the django. To view the working source of this tutorial, check here at: https://github.com/ruddra/myblog/ Table of contents    Create entry in database Templates Separating views    Create entry in database Now click on the myblog section and click add to add new blog. You can create new tags using Tags section of the admin page or clicking the (+) button right beside the Tags section on the new blog creation page, marked with blue circle in the previous image. After successfully adding a new blog, you can see it in list view page.Creating new tags is easy, just click on the Tags section in the admin page and press add tags button.Templates Now you have created new blogs and tags. Its time for showing them in templates.For making data visible in templates, you need to use views to send data to them. lets use Class Based View(CBV) for that. ListView is most appropriate for viewing all blogs in one page as it renders a page representing a list of objects. You can directly use this generic CBV in urls like:urlpatterns = patterns(\u0026#39;\u0026#39;, url(r\u0026#39;^admin/\u0026#39;, include(admin.site.urls), name=\u0026#39;admin-site\u0026#39;), url(r\u0026#39;^$\u0026#39;, ListView.as_view(model = MyBlog, template_name = \u0026#39;blog_list.html\u0026#39;), name=\u0026#39;blog_list\u0026#39;), ) Here, you need to create a template as well to view the data sent from this view:\u0026lt;ul\u0026gt; {% for blog in object_list %}\u0026lt;li\u0026gt; {{ blog.title }}\u0026lt;br/\u0026gt; \u0026lt;p\u0026gt; {{ blog.body }}\u0026lt;/p\u0026gt; {% endfor %}\u0026lt;/ul\u0026gt; So now if you go to URL: 127.0.0.1:8000, you will see a list of Blogs in there.Voila!! You can see the posts you are creating in admin site on this page.Separating views Now, for accessing each blog post separately, you can use Class Based View (CBV) for that. You can use DetailView for viewing content of one myblog object. For that, you can directly use it in urls like:# ------------- Models --------------- from myblog.models import Tag, MyBlog # ------------- Generic Views -------- from django.views.generic.list import ListView from django.views.generic.detail import DetailView urlpatterns = patterns(\u0026#39;\u0026#39;, url(r\u0026#39;^admin/\u0026#39;, include(admin.site.urls), name=\u0026#39;admin-site\u0026#39;), url(r\u0026#39;^$\u0026#39;, ListView.as_view(model = MyBlog, template_name = \u0026#39;blog_list.html\u0026#39;), name=\u0026#39;blog_list\u0026#39;), url(r\u0026#39;^details/(?P\u0026lt;pk\u0026gt;[0-9]+)/\u0026#39;, DetailView.as_view(model = MyBlog, template_name = \u0026#39;blog_details.html\u0026#39;), name=\u0026#39;blog_details\u0026#39;), # Why naming the urls? Check below for usage of named urls ) And update the template:\u0026lt;h2\u0026gt;{{ object.title }}\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;{{ object.body }}\u0026lt;/p\u0026gt; \u0026lt;b\u0026gt;Tags:\u0026lt;/b\u0026gt; \u0026lt;p\u0026gt; {% for item in object.tags.all %}{{ item }}{% endfor %}\u0026lt;/p\u0026gt; Blog details can visible to this url: 127.0.0.1:8000/details/1/ . There you should see the details of the Blog you have written before.Now you can see all the posts and tags separately. Will do some tune-ups in next post. Cheers!!!","date":"September 17, 2015","link":"/posts/make-a-blog-using-django-part-2/","readTime":3,"title":"Make a Blog using Django Part 2"},{"content":" This post is now deprecated. Please follow the official tutorial for creating your first django application. \u0026ldquo;Making a blog using django\u0026rdquo; - is probably the most made tutorial of Django. This post is no different. But I made this in my own way using some of django\u0026rsquo;s built-in features so that less coding is required and making it more understandable with minimum packages to use. To view the working source of this tutorial, check here at: https://github.com/ruddra/myblog Table of contents    Requirements intro on django framework Project setup Create models Database migration and more Admin site In conclusion    Requirements Before jumping to the main event, see if you have these installed in your pc: Python 3 installed in the computer. (Or python 2.7 if you prefer) Django 1.7 installed in the computer.  intro on django framework Django appears to be a MVC framework, but instead of using the name \u0026lsquo;Controller\u0026rsquo;, we call it as \u0026lsquo;View\u0026rsquo; and \u0026lsquo;View\u0026rsquo; as \u0026lsquo;Template\u0026rsquo;, Also Django is not a CMS. Its a Web framework; its a programming tool that lets you build Web sites. Check here for details: SOURCE.So as we stated before, django appears to be a MVC framework. MVC is a framework for building web applications using a MVC (Model View Controller) design: The Model represents the application core (for instance a list of database records). The View displays the data (the database records).(Here it is called \u0026ldquo;Template\u0026rdquo;) The Controller handles the input (to the database records).(Here it is called \u0026ldquo;View\u0026rdquo;) **(copied from here http://www.w3schools.com/aspnet/mvc_intro.asp)  Django manipulates data in the database using ORM(Object Relational Model). ORM saves you a lot of time by making the structure of the database, running CRUD(Create Read Update Delete) operations etc. Django ORM builds the structure of the database using the structure of the model. It means, the way you define the model, the way your database structure will be. A model is the single, definitive source of data about your data. It contains the essential fields and behaviors of the data youre storing. Generally, each model maps to a single database table(more).Project setup Now lets start a project named myproject in desired directory using this command: django-admin.py startproject myproject. Then, create an app inside the myproject directory using python manage.py startapp myblog. So the structure should look like this:myproject/ manage.py myproject/ __init__.py settings.py urls.py wsgi.py myblog/ __init__.py admin.py migrations/ __init__.py models.py tests.py views.py This reusable app is going to be used for making the blog. More about reusable apps.Append \u0026lsquo;myblog\u0026rsquo; to myproject\u0026gt;myproject\u0026gt;settings.py\u0026lsquo;s INSTALLED_APP like:INSTALLED_APPS += ( \u0026#39;myblog\u0026#39;, ) Create models Now we start by making a blog by making models. In this project, we are going to display Title, Body, Tags in each post. So for each content in a post, database\u0026rsquo;s table is going to need a field. So in our model, we are going to add those fields to myproject\u0026gt;myblog\u0026gt;models.py like:from django.db import models class Tag(models.Model): name = models.CharField(max_length=255) description = models.CharField(max_length=255, null=True, default=\u0026#39;\u0026#39;) def __str__(self): return self.name class MyBlog(models.Model): title = models.CharField(max_length=255) body = models.CharField(max_length=20000) tags = models.ManyToManyField(Tag) def __str__(self): return self.title The reason for making these structure is that:  title: It is a CharField(Character Field) which can take any kind of input.  body: It is a CharField](https://docs.djangoproject.com/en/1.7/ref/models/fields/#django.db.models.CharField)(Character Field) which can take any kind of input.  tags: A ManyToMany relation with Model Tag, because a blog can be related to multiple tags simillarly a tag can be used to different blogs, hence many to many relation.  Database migration and more The model class Tag is going to be used for making/displaying tags. Now we have made model for blog, need to use ORM for making Database structure and add aditional data(Why migration is necessary? See here. So for that, go to myproject directory where manage.py resides and run:$python manage.py makemigrations $python manage.py migrate $python manage.py createsuperuser --username=admin --email=me@ruddra.com #it will ask for setting a password $python manage.py runserver # for running the server The third command for making a superuser in the system. The fourth command will run your project in this url: 127.0.0.1:8000(if you don\u0026rsquo;t provide any specific ip/port). Or you can run like this python manage.py runserver 0.0.0.0:8000 and it will make your project run in 0.0.0.0:8000 and this is accessible from browser. The webpage will look like this when the project runs successfully: So you have successfully ran the django site.Admin site Now the database has been made and superuser has been created, so we go the next step, creating blogs. We are going to use django\u0026rsquo;s one of the most powerful and popular feature, django\u0026rsquo;s admin site. For making admin site visible and accessible, you need to add this lines to your urls.py (myproject\u0026gt;myproject\u0026gt;urls.py):from django.conf.urls import patterns, include, url from django.contrib import admin admin.autodiscover() #this line is for making model visible in admin site urlpatterns = patterns(\u0026#39;\u0026#39;, url(r\u0026#39;^admin/\u0026#39;, include(admin.site.urls), name=\u0026#39;admin-site\u0026#39;), ) This lines will let you access the django\u0026rsquo;s admin site using this url: 127.0.0.1:8000/admin (if you are running this project in localhost).Now we need to modify the admin.py in myblog\u0026lsquo;s directory to register the app to admin site.# Location myproject\u0026gt;myblog\u0026gt;admin.py from django.contrib import admin from django import forms from myblog.models import MyBlog, Tag admin.site.register(MyBlog) admin.site.register(Tag) Now your admin site will look like this: In conclusion So up-to admin site, the making of blog is complete. Next part will be provided in next post. Cheers!!","date":"September 17, 2015","link":"/posts/make-a-blog-using-django-part-1/","readTime":5,"title":"Make a Blog using Django Part 1"},{"content":"Django Tables2 is a package which displays table directly from queryset. It shows column header based on object\u0026rsquo;s attribute name. But if someone wants to override it, how can he/she do that? Here is a easy solution. Table of contents    Model class Table class View class   Model class Suppose we have a model class like this:class SomeModel(models.Model): somevalue = models.CharField() And we want to show table column somevalue to overridenvalueTable class class SomeTable(tables.Table): def __init__(self, *args, _overriden_value=\u0026#34;\u0026#34;,**kwargs): super().__init__(*args, **kwargs) self.base_columns[\u0026#39;somevalue\u0026#39;].verbose_name = _overriden_value class Meta: model = models.SomeModel fields = \u0026#39;__all__\u0026#39; And the Class Based View:View class class SomeView(ListView): def get_context_data(self, **kwargs): context = super().get_context_data(**kwargs) context[\u0026#39;sometable\u0026#39;] = SomeTable(SomeModel.objects.all(), _overriden_value=\u0026#34;overriden value\u0026#34;) return context And template should render that table like this:{% load render_table from django_tables2 %}{% render_table sometable %}Thats it, we shall be able to see our override table column header.","date":"September 17, 2015","link":"/posts/change-column-headers-django-tables2/","readTime":1,"title":"Change Column Headers in Django Tables 2"},{"content":"Here I am going to write a dynamic filter. This filter is made for python 3. It will take query or model class and filter condition as input, It will return filtered query based on those filter condition.This is constructed using this SO answer. Table of contents    Function Usage   Function from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker class ModelHelper(object): query = None model_class = None _session = None @property def session(self): return self.get_session() @session.setter def session(self, session=None): self._session = session def get_model_class(self): return self.model_class def get_session(self): \u0026#34;\u0026#34;\u0026#34; Override this method to assign customized session or you can add your own session by DynamicFilter(m_class, filter_cond).session = session Returns: session object \u0026#34;\u0026#34;\u0026#34; if self._session: return self._session some_engine = create_engine(\u0026#39;postgresql://scott:tiger@localhost/\u0026#39;) Session = sessionmaker(bind=some_engine) return Session() class DynamicFilter(ModelHelper): def __init__(self, model_class, filter_condition, query=None): super().__init__(*args, **kwargs) self.query = query self.model_class = model_class self.filter_condition = filter_condition def get_query(self): \u0026#39;\u0026#39;\u0026#39; Returns query with all the objects :return: \u0026#39;\u0026#39;\u0026#39; if not self.query: self.query = self.session.query(self.model_class) return self.query def filter_query(self, query, filter_condition): \u0026#39;\u0026#39;\u0026#39; Return filtered queryset based on condition. :param query: takes query :param filter_condition: Its a list, ie: [(key,operator,value)] operator list: eq for == lt for \u0026lt; ge for \u0026gt;= in for in_ like for like value could be list or a string :return: queryset \u0026#39;\u0026#39;\u0026#39; if query is None: query = self.get_query() model_class = self.get_model_class() # returns the query\u0026#39;s Model for raw in filter_condition: try: key, op, value = raw except ValueError: raise Exception(\u0026#39;Invalid filter: %s\u0026#39; % raw) column = getattr(model_class, key, None) if not column: raise Exception(\u0026#39;Invalid filter column: %s\u0026#39; % key) if op == \u0026#39;in\u0026#39;: if isinstance(value, list): filt = column.in_(value) else: filt = column.in_(value.split(\u0026#39;,\u0026#39;)) else: try: attr = list(filter( lambda e: hasattr(column, e % op), [\u0026#39;%s\u0026#39;, \u0026#39;%s_\u0026#39;, \u0026#39;__%s__\u0026#39;] ))[0] % op except IndexError: raise Exception(\u0026#39;Invalid filter operator: %s\u0026#39; % op) if value == \u0026#39;null\u0026#39;: value = None filt = getattr(column, attr)(value) query = query.filter(filt) return query def return_query(self): return self.filter_query(self.get_query(), self.filter_condition) Usage _filter_condition = [(\u0026#39;has_attribute\u0026#39;, \u0026#39;eq\u0026#39;, \u0026#39;attribute_value\u0026#39;)] dynamic_filtered_query_class = DynamicFilter( model_class=models.user.User, filter_condition=_filter_condition, query=None, ) dynamic_filtered_query = dynamic_filtered_query_class.return_query() \u0026ldquo;model_class\u0026rdquo; is the model class you want to run the filter upon.\u0026ldquo;filter_condition\u0026rdquo; the conditon you want to implement here. This is based on the following operator list: eq for == lt for \u0026lt; ge for \u0026gt;= in for in_ like for like value could be list or a string  ","date":"September 17, 2015","link":"/posts/dynamically-constructing-filters-based-on-string-input-using-sqlalchemy/","readTime":2,"title":"Dynamically constructing filters based on string input using SQLAlchemy"},{"content":"I think, Pycharm is THE best IDE for developing python. But unfortunately, the professional edition is not free. But community edition is good enough for doing debugging, integrating GIT etc.Normally its easy to use the community edition for django and tornado\u0026rsquo;s debugging/running if you know how to configure. Table of contents    Django configuration Tornado configuration  PyCharm 2016     Django configuration For django\u0026rsquo;s configuration, there is 5 easy steps:First: Go to edit configuration and click on it(like the below pictures).  Second: Click on the (+) mark in top-left corner and add python configuration.   Third: Click on the Script, and for django select the manage.py which resides on the project directory. Fourth: Add runserver as Scripts parameter or any other django commands. Fifth: Click Apply and if your python interpreter is correctly configured, then clicking on the run command should run the project, and debugging will work as well. Tornado configuration There is only 1 Step:Just follow the first and second step from above configuration and click the *.py file you want to run for tornado project in the Script, i.e. this file should contain lines like below:if __name__ == \u0026#34;__main__\u0026#34;: application.listen(8888) tornado.ioloop.IOLoop.current().start() And save and run.PyCharm 2016 In 2016 edition, to add an existing virtual environment to the list of available interpreters  In the Project Interpreter page, click  icon.  In the drop-down list, choose Add local.    In the Select Python Interpreter dialog box that opens, choose the desired Python executable, located inside the virtual environment folder, and click OK.  Go to this link for more information.","date":"September 17, 2015","link":"/posts/how-to-configure-django-tornado-in-pycharm-community-edition/","readTime":2,"title":"How to configure Django and Tornado in PyCharm Community Edition"},{"content":" This post is deprecated as its based on Django 1.6 mostly. I am going to share some useful Django tools/functions which are very useful(were for me atleast) to get things done. Table of contents    Return any model class and its properties Distance calculator Dynamic relational operations Get week list Get month list   Return any model class and its properties This method will return any model class if you have the name of the class.from django.db import models def get_model_description(model_name=None, return_property_list=True): for item in models.get_models(include_auto_created=True): if item.__name__ == model_name: if return_property_list is True: return item.get_trigger_properties() else: return item return [] For usage, let us think of an example. Let us think, we have a class name \u0026lsquo;X\u0026rsquo;, we will get the class instance using it like this:from usefultools import get_model_descriptor model_x = get_model_descriptor(model_name=\u0026#39;X\u0026#39;) #will get class model_x_objects = get_model_descriptor(model_name=\u0026#39;X\u0026#39;).objects.all() #will get all the objects of this class And for its property:from usefultools import get_model_descriptor model_x = get_model_descriptor(model_name=\u0026#39;X\u0026#39;, return_property_list=True) #will get a list of properties like [\u0026#39;a_property\u0026#39;,\u0026#39;b_property\u0026#39;] Distance calculator If you input latitude and longitude of two places, this function will return the distance in between them. Got help from here: http://code.activestate.com/recipes/576779-calculating-distance-between-two-geographic-points/import math def distance_calculator(lat1, long1, lat2, long2): lat1, long1, lat2, long2 = float(lat1), float(long1), float(lat2), float(long2) degrees_to_radians = math.pi/180.0 phi1 = (90.0 - lat1)*degrees_to_radians phi2 = (90.0 - lat2)*degrees_to_radians theta1 = long1*degrees_to_radians theta2 = long2*degrees_to_radians cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) + math.cos(phi1)*math.cos(phi2)) arc = math.acos( cos ) distance = arc*6378.1 return distance It will return the distance in KM.Dynamic relational operations Suppose we have a sentence like: '5 is greater than 9' and check if its true. We could use eval to dynamically converty string to python but its highly not recommended. So I tried like this:def calculate_relational_operation(lhs, rhs, operator): get_type = type(lhs).__name__ if get_type == \u0026#39;str\u0026#39;: rhs = str(rhs) elif get_type == \u0026#39;float\u0026#39;: rhs = float(rhs) elif get_type == \u0026#39;int\u0026#39;: rhs = int(rhs) if operator == \u0026#34;==\u0026#34;: if lhs == rhs: return True return False elif operator == \u0026#34;!=\u0026#34;: if lhs != rhs: return True return False elif operator == \u0026#34;\u0026gt;\u0026#34;: if lhs \u0026gt; rhs: return True return False elif operator == \u0026#34;\u0026lt;\u0026#34;: if lhs \u0026lt; rhs: return True return False elif operator == \u0026#34;\u0026gt;=\u0026#34;: if lhs \u0026gt;= rhs: return True return False elif operator == \u0026#34;\u0026lt;=\u0026#34;: if lhs == rhs: return True return False elif operator == \u0026#34;Is\u0026#34;: if lhs is rhs: return True return False return False It will return True or False depending on the statement/input.Get week list It will return all the weeks list from last 1 year (extendable).from isoweek import Week def generate_week(): max_week = datetime.datetime.combine(Week.thisweek().thursday(), datetime.time(0,0)) min_week = max_week - datetime.timedelta(days=365) _weeks = list() while True: _weeks.append(\u0026#39;Week\u0026#39;+str(max_week.isocalendar()[1])+ \u0026#39; \u0026#39; +str(max_week.isocalendar()[0]))) max_week -= datetime.timedelta(days=7) if max_week \u0026lt;= min_week: break return _weeks #Output\u0026gt;\u0026gt; [\u0026#39;Week2 2015\u0026#39;, \u0026#39;Week1 2015\u0026#39;, \u0026#39;Week52 2014\u0026#39; ....] Get month list It will return last 12 month\u0026rsquo;s year and month number. Constructed using this SO answer: http://stackoverflow.com/a/6576603/2696165x = 12 now = time.localtime() print([time.localtime(time.mktime((now.tm_year, now.tm_mon - n, 1, 0, 0, 0, 0, 0, 0)))[:2] for n in range(x)]) #Output\u0026gt;\u0026gt; [(2015, 2), (2015, 1), (2014, 12), (2014, 11), (2014, 10), (2014, 9), (2014, 8), (2014, 7), (2014, 6), (2014, 5), (2014, 4), (2014, 3)] Thats it. Thanks for reading.","date":"September 17, 2015","link":"/posts/some-useful-tools-function/","readTime":3,"title":"Some Useful Tools/Function for Django"},{"content":" This post is deprecated and may not be useful. As documentation says: A formset is a layer of abstraction to work with multiple forms on the same page. It can be best compared to a data grid. So here I am going to show a very simple django formset implementation example.Here we are going to use the following model, form, template, view: Table of contents    Model Form Template View   Model class Product(models.Model): name = models.CharField(max_length=50) quantity = models.IntegerField() price = models.IntegerField() class Distributor(models.Model): name = models.CharField(max_length=100) products= models.ManyToManyField(Product) These fairly simple models, where product is related to distributor model by a many-to-many relation.Form First we declare productform, then using formset factory helps to create multiple instances of product. Then we add this to distributor form like below:from django import forms from django.forms.formsets import formset_factory class ProductForm(forms.Form): name = forms.CharField() quantity = forms.IntegerField() price = forms.IntegerField() ProductFormset= formset_factory(ProductForm) class DistributorForm(forms.Form): name= forms.CharField() products= ProductFormset() Now we use this form in template.Template \u0026lt;form action=\u0026#34;\u0026#34; method=\u0026#34;post\u0026#34; class=\u0026#34;\u0026#34;\u0026gt; {% csrf_token %}\u0026lt;h2\u0026gt; Distributors :\u0026lt;/h2\u0026gt; {% for field in form %}{{ field.errors }}{{ field.label_tag }}: {{ field }}{{ form.products.management_form }}\u0026lt;h3\u0026gt; Product Instance(s)\u0026lt;/h3\u0026gt; \u0026lt;table id=\u0026#34;table-product\u0026#34;\u0026gt; {% form.products.all %}\u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;name\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;quantity\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;price\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody class=\u0026#34;product-instances\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;{{ form.product }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ form.product }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{{ form.product }}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input id=\u0026#34;input_add\u0026#34; type=\u0026#34;button\u0026#34; name=\u0026#34;add\u0026#34; value=\u0026#34; Add More \u0026#34; class=\u0026#34;tr_clone_add btn data_input\u0026#34;\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; {% endfor %}\u0026lt;/table\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; name=\u0026#34;submit\u0026#34; class=\u0026#34;button\u0026#34; value=\u0026#34;Save\u0026#34;/\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;script\u0026gt; var i = 1; $(\u0026#34;#input_add\u0026#34;).click(function() { $(\u0026#34;tbody tr:first\u0026#34;).clone().find(\u0026#34;.data_input\u0026#34;).each(function() { if ($(this).attr(\u0026#39;class\u0026#39;)== \u0026#39;tr_clone_add btn data_input\u0026#39;){ $(this).attr({ \u0026#39;id\u0026#39;: function(_, id) { return \u0026#34;remove_button\u0026#34; }, \u0026#39;name\u0026#39;: function(_, name) { return \u0026#34;name_remove\u0026#34; +i }, \u0026#39;value\u0026#39;: \u0026#39;Remove\u0026#39; }).on(\u0026#34;click\u0026#34;, function(){ var a = $(this).parent(); var b= a.parent(); i=i-1 $(\u0026#39;#id_form-TOTAL_FORMS\u0026#39;).val(i); b.remove(); $(\u0026#39;.product-instances tr\u0026#39;).each(function(index, value){ $(this).find(\u0026#39;.data_input\u0026#39;).each(function(){ $(this).attr({ \u0026#39;id\u0026#39;: function (_, id) { var idData= id; var splitV= String(idData).split(\u0026#39;-\u0026#39;); var fData= splitV[0]; var tData= splitV[2]; return fData+ \u0026#34;-\u0026#34; +index + \u0026#34;-\u0026#34; + tData }, \u0026#39;name\u0026#39;: function (_, name) { var nameData= name; var splitV= String(nameData).split(\u0026#39;-\u0026#39;); var fData= splitV[0]; var tData= splitV[2]; return fData+ \u0026#34;-\u0026#34; +index + \u0026#34;-\u0026#34; + tData } }); }) }) }) } else{ $(this).attr({ \u0026#39;id\u0026#39;: function (_, id) { var idData= id; var splitV= String(idData).split(\u0026#39;-\u0026#39;); var fData= splitV[0]; var tData= splitV[2]; return fData+ \u0026#34;-\u0026#34; +i + \u0026#34;-\u0026#34; + tData }, \u0026#39;name\u0026#39;: function (_, name) { var nameData= name; var splitV= String(nameData).split(\u0026#39;-\u0026#39;); var fData= splitV[0]; var tData= splitV[2]; return fData+ \u0026#34;-\u0026#34; +i + \u0026#34;-\u0026#34; + tData } }); } }).end().appendTo(\u0026#34;tbody\u0026#34;); $(\u0026#39;#id_form-TOTAL_FORMS\u0026#39;).val(1+i); i++; }); \u0026lt;/script\u0026gt; The html part is fairly simple, like using form in template. Then the JS is being used so that multiple instances of product form can be generated like:\u0026lt;!-- First row of the table --\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;form-0-name\u0026#34; id=\u0026#34;id_form-0-name\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input type=\u0026#34;number\u0026#34; name=\u0026#34;form-0-quantity\u0026#34; id=\u0026#34;id_form-0-quantity\u0026#34; /\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;input type=\u0026#34;number\u0026#34; name=\u0026#34;form-0-price\u0026#34; id=\u0026#34;id_form-0-price\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input id=\u0026#34;input_add\u0026#34; type=\u0026#34;button\u0026#34; name=\u0026#34;add\u0026#34; value=\u0026#34; Add More \u0026#34; class=\u0026#34;tr_clone_add btn data_input\u0026#34; /\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;!-- Second row of the table --\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;form-1-name\u0026#34; id=\u0026#34;id_form-1-name\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input type=\u0026#34;number\u0026#34; name=\u0026#34;form-1-quantity\u0026#34; id=\u0026#34;id_form-1-quantity\u0026#34; /\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;input type=\u0026#34;number\u0026#34; name=\u0026#34;form-1-price\u0026#34; id=\u0026#34;id_form-1-price\u0026#34; /\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; \u0026lt;input id=\u0026#34;remove_button\u0026#34; type=\u0026#34;button\u0026#34; name=\u0026#34;remove_button1\u0026#34; value=\u0026#34; Remove \u0026#34; class=\u0026#34;tr_clone_add btn data_input\u0026#34; /\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;!-- more inline formset are going to rendered here --\u0026gt; View Here values from the form are being saved to database.def post(request): form = DistributorForm(request.POST) form.product_instances = ProductFormset(request.POST) if form.is_valid(): distributor= Distributor() #model class distributor.name= form.cleaned_data(\u0026#39;name\u0026#39;) distributor.save() if form.product_instances.cleaned_data is not None: for items in form.product_instances.cleaned_data: product = Product() #Product model class product.name= item[\u0026#39;name\u0026#39;] product.quantity= item[\u0026#39;quantity\u0026#39;] product.price= item[\u0026#39;price\u0026#39;] product.save() distributor.products.add(product) return redirect(\u0026#39;/success\u0026#39;) return redirect(\u0026#39;/failure\u0026#39;) Notes to keep in mind:First, need to be careful about things like:\u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;form-TOTAL_FORMS\u0026#34; value=\u0026#34;1\u0026#34; id=\u0026#34;id_form-TOTAL_FORMS\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;form-INITIAL_FORMS\u0026#34; value=\u0026#34;0\u0026#34; id=\u0026#34;id_form-INITIAL_FORMS\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;hidden\u0026#34; name=\u0026#34;form-MAX_NUM_FORMS\u0026#34; id=\u0026#34;id_form-MAX_NUM_FORMS\u0026#34; /\u0026gt; Here form-TOTAL_FORMS \u0026lsquo;s value should be equal to number of rows in table. The code above must exist in order to formset to work.Second, in views.py, formset form class needs to be called, else cleaned data within the formset can\u0026rsquo;t be found.form.product_instances = ProductFormset(request.POST) Thats all.","date":"September 17, 2015","link":"/posts/working-with-formset/","readTime":4,"title":"Working with Formsets"},{"content":"Let us make a test scenario here: A dropdown field which on change we are going to send a Get/Post request to Django and return response.Let us start coding\u0026hellip;. Table of contents    HTML code Create an Ajax request Handle AJAX request in django view In conclusion   HTML code \u0026lt;select id=\u0026#34;select_dropdown\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;joshua\u0026#34;\u0026gt;joshua\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;peter\u0026#34;\u0026gt;peter\u0026lt;/option\u0026gt; .... .... \u0026lt;/select\u0026gt; Create an Ajax request Let\u0026rsquo;s make an Ajax request after the change in the dropdown field.$(document).ready(function () { $(\u0026#34;#select_dropdown\u0026#34;).change(function () { var e = document.getElementById(\u0026#34;select_dropdown\u0026#34;); var value = e.options[e.selectedIndex].value; $.ajax({ url: \u0026#34;your-url\u0026#34;, type: \u0026#34;post\u0026#34;, // or \u0026#34;get\u0026#34;  data: value, headers: { \u0026#34;X-CSRFToken\u0026#34;: \u0026#34;{{ csrf_token }}\u0026#34; }, // for csrf token  success: function (data) { alert(data.result); }, }); }); }); Handle AJAX request in django view Here on change of a post request is called. Now let\u0026rsquo;s handle the view.from django.http import JsonResponse def post(request): if request.method == \u0026#34;POST\u0026#34;: #os request.GET() get_value= request.body # Do your logic here coz you got data in `get_value` data = {} data[\u0026#39;result\u0026#39;] = \u0026#39;you made a request\u0026#39; return HttpResponse(json.dumps(data), content_type=\u0026#34;application/json\u0026#34;) Thats all.In conclusion Above code should work both GET and POST methods. If you have any questions, please share in comments section below.","date":"September 17, 2015","link":"/posts/sample-ajax-get-post-request-in-django/","readTime":1,"title":"Sample Ajax GET/POST Request in Django"},{"content":"To add a virtual environment to Intellij Idea, You have added Python from virtualenv folder to project SDK. Which means that virtualenv directory\u0026rsquo;s python( for example venv/bin/python2.7) needs to be added to Intellij Idea project path. Table of contents    For PyCharm \u0026gt;= 2018 For Pycharm 2016 and later For older versions   For PyCharm \u0026gt;= 2018 As per documentation from Pycharm:  In the Project Interpreter page, click the cog icon and select Add.  In the left-hand pane of the Add Python Interpreter dialog box, select Virtualenv Environment. The following actions depend on whether the virtual environment existed before.  If Existing environment is selected: Specify the required interpreter: use the drop-down list, or click Select an interpreter and find one in your file system. Select the check-box Make available to all projects, if needed.    Click OK to complete the task.   For Pycharm 2016 and later To add an existing virtual environment to the list of available interpretersIn the Project Interpreter page, In the drop-down list, choose to Add local. add environment optionIn the Select Python Interpreter dialog box that opens, choose the desired Python executable, located inside the virtual environment folder, and click OK. Go to this link for more information. Now need to run the project. For older versions The virtualenv directory\u0026rsquo;s python( for example venv/bin/python2.7) needs to be added to IntelliJ Idea project path. Example: need to go to file\u0026gt;project structure (IntelliJ Idea) press (new) in Project SDK, and add new path to virtualenv\u0026rsquo;s python directory like this: Go to Modules\u0026gt;Dependencies and set your module SDK to Python SDK which is marked on this picture: Click on Django (option marked in next the image) and set Django project root, Settings, Manage Script like this: Now press ok and final look of the Project settings: Now need to run the project.My StackOverflow Answer is here: http://stackoverflow.com/questions/20877106/using-intellijidea-within-an-existing-virtualenv/20879661#20879661","date":"September 17, 2015","link":"/posts/using-intellijidea-within-an-exisiting-virtualenv/","readTime":2,"title":"Using IntellijIdea/Pycharm Within An Exisiting Virtualenv"},{"content":" This post is deprecated in favour of django authentication views Django has its own implementation for reset/forgot password for its admin site. We are going to use that piece of code as reference to implement similar feature for a non admin-site authentication page. Although there are tons of good packages which will allow user to use their password reseting system. But if the system isn\u0026rsquo;t too complex and doesn\u0026rsquo;t need such authentication plugins, then reusing the django\u0026rsquo;s very own implementation can be a good option.Class based view is going to be used instead of method based view(for no particular reason, so using either of them is alright.). And please read the comments of the example codes for better understanding of implementation.This implementation is going to divided into two parts. First part is sending an email with reset url, and the Second part is clicking the reset url attached in email and entering new password for reset completion.Before starting anything, lets look at the django\u0026rsquo;s reset/forgot password\u0026rsquo;s implementation in django/contrib/auth/forms.py (source) and django/contrib/auth/views.py (source). Table of contents    Implementation of sending an email for forgot password with reset url  Screenshots   Implemetation of clicking the reset url and entering new password for reset completation  Screenshots     Implementation of sending an email for forgot password with reset url First need to configure smtp/email configuration so the system can send email. Gmail\u0026rsquo;s SMTP service is going to be used here.EMAIL_USE_TLS = True DEFAULT_FROM_EMAIL = \u0026#39;test@gmail.com\u0026#39; SERVER_EMAIL = \u0026#39;test@gmail.com\u0026#39; EMAIL_HOST = \u0026#39;smtp.gmail.com\u0026#39; EMAIL_PORT = 587 EMAIL_HOST_USER = \u0026#39;test@gmail.com\u0026#39; EMAIL_HOST_PASSWORD = \u0026#39;test123##\u0026#39; EMAIL_BACKEND = \u0026#39;django.core.mail.backends.smtp.EmailBackend\u0026#39; Now we are going to make a reset password form where we are going to add an text field which will take either username or email address associated with the corresponding user.from django import forms class PasswordResetRequestForm(forms.Form): email_or_username = forms.CharField(label=(\u0026#34;Email Or Username\u0026#34;), max_length=254) We are going to make a view which will check the input email/username and send an email to user\u0026rsquo;s email address(implementation reference: source).from django.contrib.auth.tokens import default_token_generator from django.utils.encoding import force_bytes from django.utils.http import urlsafe_base64_encode, urlsafe_base64_decode from django.template import loader from django.core.validators import validate_email from django.core.exceptions import ValidationError from django.core.mail import send_mail from settings import DEFAULT_FROM_EMAIL from django.views.generic import * from utils.forms.reset_password_form import PasswordResetRequestForm from django.contrib import messages from django.contrib.auth.models import User from django.db.models.query_utils import Q class ResetPasswordRequestView(FormView): template_name = \u0026#34;account/test_template.html\u0026#34; #code for template is given below the view\u0026#39;s code success_url = \u0026#39;/account/login\u0026#39; form_class = PasswordResetRequestForm @staticmethod def validate_email_address(email): \u0026#39;\u0026#39;\u0026#39; This method here validates the if the input is an email address or not. Its return type is boolean, True if the input is a email address or False if its not. \u0026#39;\u0026#39;\u0026#39; try: validate_email(email) return True except ValidationError: return False def post(self, request, *args, **kwargs): \u0026#39;\u0026#39;\u0026#39; A normal post request which takes input from field \u0026#34;email_or_username\u0026#34; (in ResetPasswordRequestForm). \u0026#39;\u0026#39;\u0026#39; form = self.form_class(request.POST) if form.is_valid(): data= form.cleaned_data[\u0026#34;email_or_username\u0026#34;] if self.validate_email_address(data) is True: #uses the method written above \u0026#39;\u0026#39;\u0026#39; If the input is an valid email address, then the following code will lookup for users associated with that email address. If found then an email will be sent to the address, else an error message will be printed on the screen. \u0026#39;\u0026#39;\u0026#39; associated_users= User.objects.filter(Q(email=data)|Q(username=data)) if associated_users.exists(): for user in associated_users: c = { \u0026#39;email\u0026#39;: user.email, \u0026#39;domain\u0026#39;: request.META[\u0026#39;HTTP_HOST\u0026#39;], \u0026#39;site_name\u0026#39;: \u0026#39;your site\u0026#39;, \u0026#39;uid\u0026#39;: urlsafe_base64_encode(force_bytes(user.pk)), \u0026#39;user\u0026#39;: user, \u0026#39;token\u0026#39;: default_token_generator.make_token(user), \u0026#39;protocol\u0026#39;: \u0026#39;http\u0026#39;, } subject_template_name=\u0026#39;registration/password_reset_subject.txt\u0026#39; # copied from django/contrib/admin/templates/registration/password_reset_subject.txt to templates directory email_template_name=\u0026#39;registration/password_reset_email.html\u0026#39; # copied from django/contrib/admin/templates/registration/password_reset_email.html to templates directory subject = loader.render_to_string(subject_template_name, c) # Email subject *must not* contain newlines subject = \u0026#39;\u0026#39;.join(subject.splitlines()) email = loader.render_to_string(email_template_name, c) send_mail(subject, email, DEFAULT_FROM_EMAIL , [user.email], fail_silently=False) result = self.form_valid(form) messages.success(request, \u0026#39;An email has been sent to \u0026#39; + data +\u0026#34;. Please check its inbox to continue reseting password.\u0026#34;) return result result = self.form_invalid(form) messages.error(request, \u0026#39;No user is associated with this email address\u0026#39;) return result else: \u0026#39;\u0026#39;\u0026#39; If the input is an username, then the following code will lookup for users associated with that user. If found then an email will be sent to the user\u0026#39;s address, else an error message will be printed on the screen. \u0026#39;\u0026#39;\u0026#39; associated_users= User.objects.filter(username=data) if associated_users.exists(): for user in associated_users: c = { \u0026#39;email\u0026#39;: user.email, \u0026#39;domain\u0026#39;: \u0026#39;example.com\u0026#39;, #or your domain \u0026#39;site_name\u0026#39;: \u0026#39;example\u0026#39;, \u0026#39;uid\u0026#39;: urlsafe_base64_encode(force_bytes(user.pk)), \u0026#39;user\u0026#39;: user, \u0026#39;token\u0026#39;: default_token_generator.make_token(user), \u0026#39;protocol\u0026#39;: \u0026#39;http\u0026#39;, } subject_template_name=\u0026#39;registration/password_reset_subject.txt\u0026#39; email_template_name=\u0026#39;registration/password_reset_email.html\u0026#39; subject = loader.render_to_string(subject_template_name, c) # Email subject *must not* contain newlines subject = \u0026#39;\u0026#39;.join(subject.splitlines()) email = loader.render_to_string(email_template_name, c) send_mail(subject, email, DEFAULT_FROM_EMAIL , [user.email], fail_silently=False) result = self.form_valid(form) messages.success(request, \u0026#39;Email has been sent to \u0026#39; + data +\u0026#34;\u0026#39;s email address. Please check its inbox to continue reseting password.\u0026#34;) return result result = self.form_invalid(form) messages.error(request, \u0026#39;This username does not exist in the system.\u0026#39;) return result messages.error(request, \u0026#39;Invalid Input\u0026#39;) return self.form_invalid(form) As you see above, the code is fairly simple(although it looks long). An encoded user id has been generated here using urlsafebase64_encode(force___bytes(user.pk)) and a token by using defaulttokengenerator.make___token(user). This user id is going to be used later to get the user, the token will be used for checking validity of the url for that user and both the token and the user id is going to be used as unique reference for reset password url. c is a dictionary which has user id, token and other related data etc. This dictionary is going to be blent with the template registration/password_reset_email.html and send to the user\u0026rsquo;s email address.For displaying messages(if you are using messages framework of django-1.7, details: source), add this piece of code in your template:{# test template #}\u0026lt;!-- code for displaying success or error message in template --\u0026gt; {% if messages %}\u0026lt;ul class=\u0026#34;messages\u0026#34;\u0026gt; {% for message in messages %}\u0026lt;li\u0026gt;{% if message.tags %}class=\u0026#34;{{ message.tags }}\u0026#34;{% endif %}\u0026gt;{{ message }}\u0026lt;/li\u0026gt; {% endfor %}{% endif %}\u0026lt;/ul\u0026gt; \u0026lt;!-- Form rendering code for template --\u0026gt; \u0026lt;form action=\u0026#34;\u0026#34; method=\u0026#34;post\u0026#34;\u0026gt; {% csrf_token %}{{ form.as_p }}\u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Submit\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; Two more things before wrapping up sending email part. One, making a urlfor using this view.urlpatterns = patterns(\u0026#39;\u0026#39;, url(r\u0026#39;^admin/\u0026#39;, include(admin.site.urls)), # url(r\u0026#39;^account/reset_password_confirm/(?P\u0026lt;uidb64\u0026gt;[0-9A-Za-z]+)-(?P\u0026lt;token\u0026gt;.+)/$\u0026#39;, PasswordResetConfirmView.as_view(),name=\u0026#39;reset_password_confirm\u0026#39;), # PS: url above is going to used for next section of implementation. url(r\u0026#39;^account/reset_password\u0026#39;, ResetPasswordRequestView.as_view(), name=\u0026#34;reset_password\u0026#34;), ) Two, editing the template of registration/password_reset_email.html or else you will get errors.{% load i18n %}{% autoescape off %}{% blocktrans %}You\u0026#39;re receiving this email because you requested a password reset for your user account at {{ site_name }}.{% endblocktrans %}{% trans \u0026#34;Please go to the following page and choose a new password:\u0026#34; %}{% block reset_link %}{{ domain }}{% url \u0026#39;reset_password_confirm\u0026#39; uidb64=uid token=token %}\u0026lt;!--This is the only change from ` django/contrib/admin/templates/registration/password_reset_subject.html`. the url name is commented out in urls.py section. The view associated with the url is going to described later in this post. --\u0026gt; {% endblock %}{% trans \u0026#34;Your username, in case you\u0026#39;ve forgotten:\u0026#34; %}{{ user.get_username }}{% trans \u0026#34;Thanks for using our site!\u0026#34; %}{% blocktrans %}The {{ site_name }}team{% endblocktrans %}{% endautoescape %}Now run the server and you will see forms like the screen shots below: (This screenshots look cool because django adminsite\u0026rsquo;s js/css have been used here.) (**Image sequence is according the implementation flow)Screenshots Rendered template from PasswordResetRequestForm form  Rendered template from PasswordResetRequestForm form with error messages  Rendered template of login form with sent email confirmation message  Sent Email look  Implemetation of clicking the reset url and entering new password for reset completation First, lets write a form which will have two fields new password and retype password field.class SetPasswordForm(forms.Form): \u0026#34;\u0026#34;\u0026#34; A form that lets a user change set their password without entering the old password \u0026#34;\u0026#34;\u0026#34; error_messages = { \u0026#39;password_mismatch\u0026#39;: (\u0026#34;The two password fields didn\u0026#39;t match.\u0026#34;), } new_password1 = forms.CharField(label=(\u0026#34;New password\u0026#34;), widget=forms.PasswordInput) new_password2 = forms.CharField(label=(\u0026#34;New password confirmation\u0026#34;), widget=forms.PasswordInput) def clean_new_password2(self): password1 = self.cleaned_data.get(\u0026#39;new_password1\u0026#39;) password2 = self.cleaned_data.get(\u0026#39;new_password2\u0026#39;) if password1 and password2: if password1 != password2: raise forms.ValidationError( self.error_messages[\u0026#39;password_mismatch\u0026#39;], code=\u0026#39;password_mismatch\u0026#39;, ) return password2 It will take two password input and verify if they match, if those inputs match(in clean method), it will return password. Now using that form, we are going to write a view(reference for implementation:(source).class PasswordResetConfirmView(FormView): template_name = \u0026#34;account/test_template.html\u0026#34; success_url = \u0026#39;/admin/\u0026#39; form_class = SetPasswordForm def post(self, request, uidb64=None, token=None, *arg, **kwargs): \u0026#34;\u0026#34;\u0026#34; View that checks the hash in a password reset link and presents a form for entering a new password. \u0026#34;\u0026#34;\u0026#34; UserModel = get_user_model() form = self.form_class(request.POST) assert uidb64 is not None and token is not None # checked by URLconf try: uid = urlsafe_base64_decode(uidb64) user = UserModel._default_manager.get(pk=uid) except (TypeError, ValueError, OverflowError, UserModel.DoesNotExist): user = None if user is not None and default_token_generator.check_token(user, token): if form.is_valid(): new_password= form.cleaned_data[\u0026#39;new_password2\u0026#39;] user.set_password(new_password) user.save() messages.success(request, \u0026#39;Password has been reset.\u0026#39;) return self.form_valid(form) else: messages.error(request, \u0026#39;Password reset has not been unsuccessful.\u0026#39;) return self.form_invalid(form) else: messages.error(request,\u0026#39;The reset password link is no longer valid.\u0026#39;) return self.form_invalid(form) URL for this view:urlpatterns += patterns(\u0026#39;\u0026#39;, url(r\u0026#39;^admin/\u0026#39;, include(admin.site.urls)), url(r\u0026#39;^account/reset_password_confirm/(?P\u0026lt;uidb64\u0026gt;[0-9A-Za-z]+)-(?P\u0026lt;token\u0026gt;.+)/$\u0026#39;, PasswordResetConfirmView.as_view(),name=\u0026#39;reset_password_confirm\u0026#39;), ) Well PasswordResetConfirmView takes two parameter from urls, uidb64 and token, those were sent within email generated by ResetPasswordRequestView. We got user id hence the user by decoding uid64 by using urlsafebase64decode, and function defaulttokengenerator.check__token checks the token against the user. If they are valid and the form is valid, we set new password for the user using .set__password(\u0026lsquo;password\u0026rsquo;) function. If they are not valid, it will show an error message saying the url is no longer valid.More screenshots:(sequencial to implementation)Screenshots Rendered template for SetPasswordForm form  Reset successful  Thus you implement your very own forgot or reset the password.For full project/implementation, please check/fork this repository. This code has been tested for Python3 and django 1.7/1.10 . ","date":"October 21, 2014","link":"/posts/implementation-of-forgot-reset-password-feature-in-django/","readTime":8,"title":"Implementation of Forgot/Reset Password Feature in Django"},{"content":" This post is deprecated. It is no longer compatible with latest Celery versions. I am assuming you have read celery docs from Celery Documentation.As we know, celery can be used as a scheduler for executing asynchronous tasks in periodic cycles. Here I am going to share to do that with a code example. But I am going to avoid theoretical knowledge here because you can read them in celery documentation. Table of contents    Install celery Configuring brokers Making periodic task   Install celery First install celery: pip install django-celery.Configuring brokers We are using django database as Broker. For detail understanding, check here: http://celery.readthedocs.org/en/latest/getting-started/brokers/django.htmlMaking periodic task Here is the project structure we are going to use:-project -settings.py -manage.py -app1 -views.py -models.py -app2 -views.py -models.py Lets say, we want to add periodic task to app1. So structure of the project will be like this:-project -settings.py -manage.py -app1 -__init__py -celery.py -tasks.py -views.py -models.py -app2 -views.py -models.py No need to panic to see two new .py files. They will be created in time. Now, we need to add celery configuration in:settings.py:-from __future__ import absolute_import BROKER_URL = \u0026#39;pyamqp://guest:guest@wlocalhost:5672//\u0026#39; #read docs CELERY_IMPORTS = (\u0026#39;app1.tasks\u0026#39;, ) from celery.schedules import crontab from datetime import timedelta CELERYBEAT_SCHEDULE = { \u0026#39;schedule-name\u0026#39;: { \u0026#39;task\u0026#39;: \u0026#39;app1.tasks.email_sending_method\u0026#39;, # We are going to create a email_sending_method later in this post. \u0026#39;schedule\u0026#39;: timedelta(seconds=30), }, } As you can see, task in CELERYBEAT__SCHEDULE is name as app1.tasks.emailsendingmethod because in next section we are making a email_sendingmethod method which going to send an email every 30 seconds.And in installed apps, we need to add djcelery :-INSTALLED_APPS = ( ... \u0026#39;djcelery\u0026#39; ) Now we shall add a celery.py file in app1 directory:-from __future__ import absolute_import import os from celery import Celery import django from django.conf import settings os.environ.setdefault(\u0026#39;DJANGO_SETTINGS_MODULE\u0026#39;, \u0026#39;settings\u0026#39;) app = Celery(\u0026#39;app1.email_sending_method\u0026#39;) app.config_from_object(\u0026#39;django.conf:settings\u0026#39;) app.autodiscover_tasks(lambda: settings.INSTALLED_APPS) app.conf.update( CELERY_RESULT_BACKEND=\u0026#39;djcelery.backends.database:DatabaseBackend\u0026#39;, ) @app.task(bind=True) def debug_task(self): print(\u0026#39;Request: {0!r}\u0026#39;.format(self.request)) and update the __init__.py file within the directory:-from __future__ import absolute_import from celery import app as celery_app Now we are going to add a tasks.py which is actually going to be executed while running celery.from __future__ import absolute_import import datetime from celery.task.base import periodic_task from django.core.mail import send_mail @periodic_task(run_every=datetime.timedelta(seconds=30)) def email_sending_method(): send_mail(\u0026#39;subject\u0026#39;, \u0026#39;body\u0026#39;, \u0026#39;from_me@admin.com\u0026#39; , [\u0026#39;to_me@admin.com\u0026#39;], fail_silently=False) Add respective credentials/configurations for sending mail, and then run this piece of code in command prompt:-celery -A app1 worker -B -l info And that should do the trick, we will get mails after every 30 seconds.PS: Although there might a key-error, but it won\u0026rsquo;t occur any problems.","date":"September 1, 2014","link":"/posts/perodic-tasks-by-celery-3-1-example/","readTime":2,"title":"Perodic Tasks By Celery 3.1 Example"},{"content":"While surfing through Stackoverflow, I find a common question among Django users that, database not working properly; fields attribute changed, yet not working etc. Clearly because most of them used syncdb after altering fields. Well, lets make some things clear here about django syncdb and migration. Table of contents    What is \u0026lsquo;syncdb\u0026rsquo; What is \u0026lsquo;migration\u0026rsquo; What if you mess-up in production server with \u0026lsquo;syncdb\u0026rsquo; and \u0026lsquo;migration\u0026rsquo;   What is \u0026lsquo;syncdb\u0026rsquo; syncdb is a command which is executed in django shell to create tables for first time for apps which are added to INSTALLED_APPS of settings.py. Need to keep in mind about two key words: \u0026lsquo;First Time\u0026rsquo; and \u0026lsquo;Newly Added Apps\u0026rsquo;. Because syncdb only works on models of those apps for first time to create initial tables in database. So once syncdb is executed, not model field altering, because if anyone does that, it will not work. Its clearly mentioned in documentation: syncdb will only create tables for models which have not yet been installed. It will never issue ALTER TABLE statements to match changes made to a model class after installation. Changes to model classes and database schemas often involve some form of ambiguity and, in those cases, Django would have to guess at the correct changes to make. There is a risk that critical data would be lost in the process. If you have made changes to a model and wish to alter the database tables to match, use the sql command to display the new SQL structure and compare that to your existing table schema to work out the changes. So what if you need to change model field? No worries, migration is here to save you. What is \u0026lsquo;migration\u0026rsquo; Migration is a process to reconstruct database schema according to altered model fields. From Django 1.7(under development) documentation:So, after using syncdb, if you need to alter model fields, then go ahead, and after that you have to migrate database. If you are using django \u0026lt;=1.6, then you can use South. If django is above 1.6, it has its own migration process.And of course, if you use South to migrate, you have to use syncdb before executing migration, because if you don\u0026rsquo;t, initial database tables(including auth, auth_group_permission, django_admin_log etc) will not be created.What if you mess-up in production server with \u0026lsquo;syncdb\u0026rsquo; and \u0026lsquo;migration\u0026rsquo; If you end up doing syncdb initially but need to change database, what should be done?Easy solution, keep the models in same state as it was during initial syncdb command. Then run python manage.py schemamigration your_app_label --initial**(in django \u0026lt;=1.6)** or python manage.py makemigration your_app_label**(django \u0026gt;=1.7)**.After that, run python manage.py migrate your_app_label --fake**(in django \u0026lt;=1.6)** or python manage.py migrate your_app_label --fake-initial **(django \u0026gt;=1.7)**. It will put a fake migration in the database which will occur no change in tables.Then change models and run python manage.py migrate your_app_labelPS: syncdb is deprecated from django 1.7, which will reduce the hassle of using syncdb and migration separately.","date":"July 4, 2014","link":"/posts/syncdb-vs-migration/","readTime":3,"title":"Syncdb vs Migration"},{"content":"I wanted to add a rich text editor within django administrator. It is not that hard to add a rich text editor, as there are editors like CKeditor, Tinymce. Table of contents    Download ckeditor file Writing forms.py Generated text   Download ckeditor file There are multiple plugins for django like django-ckeditor or django-tinymce etc. It seemed very complicated to use for me. So what I did here is that I have downloaded ckeditor standard edition and extracted it in my Static folder and loaded the js file within templates\u0026gt;admin\u0026gt;base.html.Now, using firebug, I retrieved the textarea name/id/class in which I wanted to add ckeditor using firebug (or from chrome/firefox: inspect elements). This process is simple, just load the page where your textarea(or any type of field) resides, open firebug and inspect that place.For example: lets say the model field I want to modify is named blogbody. So the element\u0026rsquo;s name in admin site was id_blogbody(auto generated). In case of using a form, the input will be like following:Writing forms.py blogbody= forms.CharField( widget=forms.TextInput(attrs={\u0026#39;id\u0026#39;: \u0026#39;id_blogbody\u0026#39;}) ) Generated text \u0026lt;input id=\u0026#34;id_blogbody\u0026#34; ... /\u0026gt; Then go to base.html and add this script:\u0026lt;script\u0026gt; CKEDITOR.replace(\u0026#34;name_or_id_or_class_of_the_textfield\u0026#34;); //in this example CKEDITOR.replace( \u0026#39;#id_blogbody\u0026#39; ) \u0026lt;/script\u0026gt; Now reload the page from admin site and a textfield with rich text editor will be generated!","date":"April 29, 2014","link":"/posts/richtext-editor-in-django-admin-site/","readTime":2,"title":"RichText Editor in Django Admin Site"},{"content":" This post is deprecated. Please follow the official documentation. When comes to using multiple languages in one single site, Django is very handy. You can use .po file to do your translation for you. Table of contents    How to do that   How to do that Process is very simple: First create .po file. To make .po file I would suggest to use poedit or Rosetta. Here is another option that is using django\u0026rsquo;s very own Localisation. Second create a folder name locale within tour django project and add the language named (for example: ru__RU for Russian language) within locale. Within ru___RU folder, create another folder named \u0026lsquo;LC__MESSAGES\u0026rsquo;. There save the .po file you have created. Save the .po file in name django.po. File Map:Project  locale   ru_RU    LC_MESSAGES    django.po   en_GB   LC_MESSAGES   django.po Now run this command: django-admin.py compilemessages to generate .mo file(django.mo). Third comes to final touch. in Language settings in your settings.py add ru_RU like this:LANGUAGES = ( (\u0026#39;en-us\u0026#39;, \u0026#39;English\u0026#39;), (\u0026#39;ru_RU\u0026#39;, \u0026#39;Russian\u0026#39;), ) LANGUAGE_CODE = \u0026#39;en-us\u0026#39; \u0026#39;ru_RU\u0026#39; Add locale path :LOCALE_PATHS = ( os.path.join(PROJECT_PATH, \u0026#39;../locale\u0026#39;), ) and finally add a middleware in in MIDDLEWARE_CLASSES or MIDDLEWARES.\u0026#39;django.middleware.locale.LocaleMiddleware\u0026#39; That should the trick.","date":"December 16, 2013","link":"/posts/django-translation-using-po-file/","readTime":1,"title":"Django Translation Using .PO File"},{"content":"Hi, This is Arnab Kumar Shil, a full stack developer from Bangladesh. I specialize in Python/Django/React based web application development. Currently working as a senior software engineer at Cefalo Bangladesh Ltd. Table of contents    Introduction Skills Certifications Stackoverflow Open Source Contributions Papers Programming Hobbies Resume Contact   Introduction I have working professionally for more than six years. Apart from being a fullstack developer, I have also worked as DevOps and contributed to different projects like building CI/CD pipeline, documentation generation, automation by Ansible/Jenkins, Docker based software distribution etc. I am an open-source enthusiast, passionate about software development, team player, self motivated person who loves writing codes professionally, as well as a hobby. Prefer standard practices like writing test cases, use PR reviews, git rebase, pep8 standard for Python coding, using design patterns, following agile development process and so on.My latest interest is in Data Science. I am an IBM certified Data Science Professional.Skills I am familiar with many technologies but I have worked most on:- Languages: Python, JavaScript, Java- Python Frameworks/Packages: Django, DRF, Flask, Tornado, Scrapy, Celery- JS Frameworks/Packages: React, Redux, ReactNative, AngularJs, Express- Virtualization tools: Docker, Vagrant- Caching: Varnish, Redis- Blogging: Hugo, Jekyll, Octopress, Ghost- Container Orchestration: Kubernetes, OpenShift- Big Data: Apache Kafka, Apache Spark- DevOps: Jenkins, AWS, OpenShift, NGINX, Docker Compose, Pipeline, CI/CD- SEO: Google Analytics, Google Tag Manager, Adobe Analytics- Data Science: Jupyter Notebook, Zeppelin, Sklearn, Pandas, Numpy, Folium- Testing Tools/Framework - PyUnit, PyTest, Nose, Jest, Enzyme, Postman BDD- Design/Styling - CSS, LESS, SASSHere are scores from Pluralsight SkillIQ regarding some of my skills. Links can be found in my developer story. Certifications   ReactJS and Redux: Completed 26.5 hours course at Udemy.  React Native: Completed 12 week course regarding react native at Cefalo School.  Effective Object Oriented Programming: Completed the course with distinction marks at Cefalo School. Used Java for coursework.  Apache Kafka: Completed 7.5 hours course at Udemy.  Python Core and Advanced: Completed 8 hours course at Udemy.  Getting Started With Varnish: Completed the course at Varnish Software for learning about varnish basics. Earned the certificate with 100% score.  Taming Big Data with Apache Spark and Python - Hands On!: Completed 5 hours course at Udemy.  IBM Data Science Professional Certificate: Specialization certificate by IBM at Coursera. Completed 9 courses as part of this specialization.  To see all of my certifications, please visit my LinkedIn profile.Stackoverflow I also contribute to StackOverflow and I have more than 30,000 reputations. Currently ranked 8th in Bangladesh. Open Source Contributions  Github Bitbucket  Papers  GDPR Compliance: Implementation Use Cases for User Data Privacy in News Media Industry(link to publication).  Programming  Hacker Rank Profile. Code Signal Profile  Hobbies Love coding, traveling, anime, cycling, ping pong, motorcycle, and humor.Resume You can find my resume here.Contact You can contact me via email.","date":"January 1, 0001","link":"/about/","readTime":3,"title":"About Me"},{"content":"I am reachable on all major social media like facebook, twitter, linkedin etc. You can also contact me via email as well. Table of contents    Before contacting me via email My email address   Before contacting me via email I would appreciate if you do not send email for following reasons: Query regarding a question or answer on StockOverflow. You can read in details at Jon Skeet\u0026rsquo;s article regarding this issue. You want me to help you with problems which are not related to contents of this website. You want me to contribute to your project. You want your link to be posted in this website. You are offering freelancing jobs. Send spam emails to me. Want to approach me for marketing purpose. As I do not have any intension of earning money or promote things on my website, I can\u0026rsquo;t accept any such proposal.  My email address Please contact me at \u0026#64;ignoreme-\u0026#46;com var name = 'mX3SzFFYJSLLSOSUe'; var at = '@'; var domain = 'ruddra'; document.getElementById('name').textContent = name.replace('X3SzFFYJSLLSOSU', '') document.getElementById('domain').textContent = domain .","date":"January 1, 0001","link":"/contact/","readTime":1,"title":"Contact Me"},{"content":"Effective date: August 11, 2018 It is ruddra.com\u0026rsquo;s policy to respect your privacy regarding any information we may collect while operating our website. This Privacy Policy applies to https://ruddra.com (hereinafter, \u0026ldquo;us\u0026rdquo;, \u0026ldquo;we\u0026rdquo;, or \u0026ldquo;https://ruddra.com\u0026rdquo;). Table of contents      Effective date: August 11, 2018   Cookie Policy Email Policy User Information Policy Third Party Privacy Policy External Usage Children Policy Log Policy Contact Us   Cookie Policy We use cookies to analyze our traffic. We also share information about your use of our site with our analytics(Google Analytics) partner who may combine it with other information that youve provided to them or that theyve collected from your use of their services.Email Policy We use a third party service to send you emails. This information is not shared with third party advertisements. We collect it by fair and lawful means, with your knowledge and consent. We also let you know why were collecting it and how it will be used. We only retain it for as long as necessary to provide you with your requested service. What data we store, well protect within commercially acceptable means to prevent loss and theft, as well as unauthorized access, disclosure, copying, use or modification.We dont share email addresses publicly or with third-party advertisements. User can unsubscribe at any time.User Information Policy We only ask for personal information when we truly need it to provide a service to you. We collect it by fair and lawful means, with your knowledge and consent. We also let you know why were collecting it and how it will be used.We don\u0026rsquo;t retain user information(email) in our site, rather we store it in third party service. We dont share any personally identifying information publicly or with third-parties.Third Party Privacy Policy Please see here the privacy policies of third party services we use: Google Mailgun  External Usage Our website may link to external sites that are not operated by us. Please be aware that we have no control over the content and practices of these sites, and cannot accept responsibility or liability for their respective privacy policies.Children Policy We don\u0026rsquo;t store any age information, and our content is not age restricted.Log Policy We don\u0026rsquo;t generate any logs for system usage.Contact Us Your continued use of our website will be regarded as acceptance of our practices around privacy and personal information. If you have any questions about how we handle user data and personal information, feel free to contact us at via email.","date":"January 1, 0001","link":"/privacy/","readTime":2,"title":"Privacy Policy"}];const query=new URLSearchParams(window.location.search);const searchString=query.get('q').replace(/[^\w\s]/gi,'');document.querySelector('.search-content').innerHTML=searchString;const $target=document.querySelector('.search-items');const postsByTitle=posts.reduce((acc,curr)=>{acc[curr.title]=curr;return acc;},{});fetch('/json/search-index.json?modified=1590583129').then(function(res){return res.json();}).then(function(data){const index=lunr.Index.load(data);const matches=index.search(searchString);const matchPosts=[];matches.forEach((m)=>{matchPosts.push(postsByTitle[m.ref]);});if(matchPosts.length>0){$target.innerHTML=matchPosts.map(function(p){if(p!=undefined){return `<li>
          ${p.date} -
          <a href="${p.link}"> ${p.title}</a>
          </li>`;}}).join('');}else{$target.innerHTML=`<br /> <h2 style="text-align:center">No search results found</h2>`;}}).catch(function(error){$target.innerHTML=`<br /><h2 style="text-align:center">Error occurred</h2>`;console.log(error)});</script></div></div></div><label for=sidebar-checkbox class=sidebar-toggle></label><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-58095062-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script src="https://polyfill.io/v3/polyfill.min.js?flags=gated&features=Object.assign%2CIntersectionObserver%2CURLSearchParams" async></script><script src=https://ruddra.com/js/bundle.min.f498e44c76ce8c724d264f0e23974d1a7c96509cc31a1587c524fff6bc6aab72.js async></script></body></html>